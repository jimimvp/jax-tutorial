{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# JAX Basics"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import jax\n",
    "from jax import numpy as jnp\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# cool feature no. 1\n",
    "\n",
    "x = np.array([1.0, 2.0])[None].T\n",
    "y = jnp.array([1.0, 2.0])[None].T\n",
    "\n",
    "# numpy arrays are transformed to JAX tensors automatically at operation execution (no to_tensor mumbo-jumbo)\n",
    "print(\"Dot product:\", x.T@y)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Dot product: [[5.]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "\n",
    "M = np.random.uniform(-0.1, 0.1, (500, 500))\n",
    "\n",
    "def numpy_function(M):\n",
    "    return M @ M.T\n",
    "\n",
    "\n",
    "def function(M):\n",
    "    return M @ M.T\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def jit_function(M):\n",
    "    return  M @ M.T"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "print(\"Numpy function:\")\n",
    "%timeit -n 50 numpy_function(M)\n",
    "print(\"JAX function:\")\n",
    "%timeit -n 50 function(M)\n",
    "print(\"JAX jit function:\")\n",
    "%timeit -n 50 jit_function(M)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Numpy function:\n",
      "The slowest run took 5.10 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
      "6.11 ms ± 2.99 ms per loop (mean ± std. dev. of 7 runs, 50 loops each)\n",
      "JAX function:\n",
      "11.2 ms ± 1.08 ms per loop (mean ± std. dev. of 7 runs, 50 loops each)\n",
      "JAX jit function:\n",
      "4.06 ms ± 421 µs per loop (mean ± std. dev. of 7 runs, 50 loops each)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# so how do we calculate gradients?\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "source": [
    "# dataset for linear regression\n",
    "\n",
    "x = np.random.uniform(-1,1, (100, 10))\n",
    "theta = np.random.uniform(-1,1, (10,2))\n",
    "y = x @ theta\n",
    "theta_ = np.random.uniform(-1,1, (10,2))\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "source": [
    "y.shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(100, 2)"
      ]
     },
     "metadata": {},
     "execution_count": 37
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "source": [
    "@jax.jit\n",
    "def predict(x, theta):\n",
    "    return x@theta\n",
    "\n",
    "def mse(x, theta, y):\n",
    "    y_ = predict(x, theta)\n",
    "    return ((y-y_)**2).mean()\n",
    "\n",
    "\n",
    "\n",
    "grad_func = jax.grad(mse, argnums=[1]) # returns a function which takes the same arguments as the wrapped one\n",
    "\n",
    "\n",
    "# returns tuple of gradients with respect to arguments of function\n",
    "grad, = grad_func(x, theta, y)\n",
    "\n",
    "# for practicality, this is also available\n",
    "loss, (grad,) = jax.value_and_grad(mse, argnums=[1])(x,theta_, y)\n",
    "print(f\"MSE loss: {loss}\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "MSE loss: 1.5848617553710938\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "source": [
    "grad.shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(10, 2)"
      ]
     },
     "metadata": {},
     "execution_count": 59
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "source": [
    "# stupid loop to optimize our model\n",
    "for _ in range(100):\n",
    "    loss, (grad,) = jax.value_and_grad(mse, argnums=[1])(x,theta_, y)\n",
    "    theta_ -= 0.01*grad \n",
    "    print(f\"MSE: {loss}\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "MSE: 0.0006293614860624075\n",
      "MSE: 0.0006269514560699463\n",
      "MSE: 0.0006245508557185531\n",
      "MSE: 0.0006221593939699233\n",
      "MSE: 0.000619777652900666\n",
      "MSE: 0.000617404468357563\n",
      "MSE: 0.000615040073171258\n",
      "MSE: 0.0006126854568719864\n",
      "MSE: 0.0006103403284214437\n",
      "MSE: 0.0006080031162127852\n",
      "MSE: 0.0006056762067601085\n",
      "MSE: 0.000603357853833586\n",
      "MSE: 0.0006010483484715223\n",
      "MSE: 0.0005987479817122221\n",
      "MSE: 0.0005964564625173807\n",
      "MSE: 0.0005941731506027281\n",
      "MSE: 0.0005918987444601953\n",
      "MSE: 0.0005896340007893741\n",
      "MSE: 0.0005873775808140635\n",
      "MSE: 0.000585129193495959\n",
      "MSE: 0.000582889246288687\n",
      "MSE: 0.0005806598346680403\n",
      "MSE: 0.000578437524382025\n",
      "MSE: 0.0005762248183600605\n",
      "MSE: 0.0005740205524489284\n",
      "MSE: 0.0005718244356103241\n",
      "MSE: 0.0005696367588825524\n",
      "MSE: 0.0005674573476426303\n",
      "MSE: 0.0005652862600982189\n",
      "MSE: 0.0005631243111565709\n",
      "MSE: 0.0005609701620414853\n",
      "MSE: 0.0005588248022831976\n",
      "MSE: 0.0005566873005591333\n",
      "MSE: 0.0005545581225305796\n",
      "MSE: 0.0005524371517822146\n",
      "MSE: 0.000550324097275734\n",
      "MSE: 0.0005482195992954075\n",
      "MSE: 0.0005461231339722872\n",
      "MSE: 0.0005440342938527465\n",
      "MSE: 0.0005419538938440382\n",
      "MSE: 0.0005398823413997889\n",
      "MSE: 0.00053781719179824\n",
      "MSE: 0.0005357612390071154\n",
      "MSE: 0.0005337127950042486\n",
      "MSE: 0.0005316718597896397\n",
      "MSE: 0.0005296390154398978\n",
      "MSE: 0.0005276142037473619\n",
      "MSE: 0.0005255982396192849\n",
      "MSE: 0.0005235891439951956\n",
      "MSE: 0.0005215879646129906\n",
      "MSE: 0.000519594585057348\n",
      "MSE: 0.0005176078993827105\n",
      "MSE: 0.0005156295956112444\n",
      "MSE: 0.0005136590916663408\n",
      "MSE: 0.0005116960383020341\n",
      "MSE: 0.0005097408429719508\n",
      "MSE: 0.0005077927489764988\n",
      "MSE: 0.0005058518727310002\n",
      "MSE: 0.0005039190873503685\n",
      "MSE: 0.0005019937525503337\n",
      "MSE: 0.0005000753444619477\n",
      "MSE: 0.0004981650854460895\n",
      "MSE: 0.0004962609964422882\n",
      "MSE: 0.0004943651147186756\n",
      "MSE: 0.0004924766253679991\n",
      "MSE: 0.0004905948881059885\n",
      "MSE: 0.0004887203103862703\n",
      "MSE: 0.000486853183247149\n",
      "MSE: 0.0004849928373005241\n",
      "MSE: 0.00048314043669961393\n",
      "MSE: 0.0004812947299797088\n",
      "MSE: 0.0004794559790752828\n",
      "MSE: 0.00047762467875145376\n",
      "MSE: 0.00047580027603544295\n",
      "MSE: 0.00047398312017321587\n",
      "MSE: 0.00047217251267284155\n",
      "MSE: 0.00047036982141435146\n",
      "MSE: 0.0004685728927142918\n",
      "MSE: 0.0004667837347369641\n",
      "MSE: 0.0004650015034712851\n",
      "MSE: 0.0004632252675946802\n",
      "MSE: 0.0004614566278178245\n",
      "MSE: 0.00045969418715685606\n",
      "MSE: 0.00045793893514201045\n",
      "MSE: 0.0004561905807349831\n",
      "MSE: 0.0004544486873783171\n",
      "MSE: 0.00045271357521414757\n",
      "MSE: 0.0004509859427344054\n",
      "MSE: 0.0004492632288020104\n",
      "MSE: 0.000447547878138721\n",
      "MSE: 0.000445839308667928\n",
      "MSE: 0.0004441375203896314\n",
      "MSE: 0.0004424422513693571\n",
      "MSE: 0.0004407536762300879\n",
      "MSE: 0.0004390713875181973\n",
      "MSE: 0.0004373954434413463\n",
      "MSE: 0.0004357261350378394\n",
      "MSE: 0.00043406261829659343\n",
      "MSE: 0.0004324060573708266\n",
      "MSE: 0.0004307560157030821\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "source": [
    "# what if we want to get a Jacobian?\n",
    "\n",
    "J, = jax.jacobian(predict, argnums=[1])(x, theta)\n",
    "J.shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(100, 2, 10, 2)"
      ]
     },
     "metadata": {},
     "execution_count": 65
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "source": [
    "# what about the Hessian?\n",
    "H, = jax.jacfwd(jax.jacrev(predict), argnums=[1])(x, theta)\n",
    "H.shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(100, 2, 100, 10, 10, 2)"
      ]
     },
     "metadata": {},
     "execution_count": 68
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "source": [
    "# vmap usage\n",
    "# say we have a \"complicated function\" that we want to apply row-wise, ie. over axis=0\n",
    "\n",
    "M = np.random.uniform(1, 10, (200, 2))\n",
    "func = lambda x: x[0]**2 + jnp.exp(x[1])\n",
    "\n",
    "@jax.jit\n",
    "def naive(M):\n",
    "    return jnp.stack([func(x) for x in M])\n",
    "\n",
    "@jax.jit\n",
    "def with_vmap(M):\n",
    "    return jax.vmap(func)(M)\n",
    "\n",
    "print(\"Naive:\")\n",
    "%timeit -n 50 naive(M)\n",
    "print(\"With vmap:\")\n",
    "%timeit -n 50 with_vmap(M)\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Naive:\n",
      "The slowest run took 5783.31 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
      "12.3 ms ± 30 ms per loop (mean ± std. dev. of 7 runs, 50 loops each)\n",
      "With vmap:\n",
      "The slowest run took 235.43 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
      "310 µs ± 738 µs per loop (mean ± std. dev. of 7 runs, 50 loops each)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "source": [
    "# say we use an ensemble neural network (this cause a bit of pain for me and Sebastian to implement in PyTorch)\n",
    "from jax.tree_util import tree_flatten, tree_unflatten\n",
    "\n",
    "\n",
    "\n",
    "def tree_stack(trees):\n",
    "    \"\"\"Takes a list of trees and stacks every corresponding leaf.\n",
    "    For example, given two trees ((a, b), c) and ((a', b'), c'), returns\n",
    "    ((stack(a, a'), stack(b, b')), stack(c, c')).\n",
    "    Useful for turning a list of objects into something you can feed to a\n",
    "    vmapped function.\n",
    "    \"\"\"\n",
    "    leaves_list = []\n",
    "    treedef_list = []\n",
    "    for tree in trees:\n",
    "        leaves, treedef = tree_flatten(tree)\n",
    "        leaves_list.append(leaves)\n",
    "        treedef_list.append(treedef)\n",
    "\n",
    "    grouped_leaves = zip(*leaves_list)\n",
    "    result_leaves = [jnp.stack(l) for l in grouped_leaves]\n",
    "    return treedef_list[0].unflatten(result_leaves)\n",
    "\n",
    "\n",
    "def tree_unstack(tree):\n",
    "    \"\"\"Takes a tree and turns it into a list of trees. Inverse of tree_stack.\n",
    "    For example, given a tree ((a, b), c), where a, b, and c all have first\n",
    "    dimension k, will make k trees\n",
    "    [((a[0], b[0]), c[0]), ..., ((a[k], b[k]), c[k])]\n",
    "    Useful for turning the output of a vmapped function into normal objects.\n",
    "    \"\"\"\n",
    "    leaves, treedef = tree_flatten(tree)\n",
    "    n_trees = leaves[0].shape[0]\n",
    "    new_leaves = [[] for _ in range(n_trees)]\n",
    "    for leaf in leaves:\n",
    "        for i in range(n_trees):\n",
    "            new_leaves[i].append(leaf[i])\n",
    "    new_trees = [treedef.unflatten(l) for l in new_leaves]\n",
    "    return new_trees\n",
    "\n",
    "\n",
    "\n",
    "def get_nn_params():\n",
    "        return [\n",
    "            (np.random.uniform(-1,1, (10, 64)), np.random.uniform(-1,1, (64, 1))),\n",
    "            (np.random.uniform(-1,1, (64, 64)), np.random.uniform(-1,1, (64, 1))),\n",
    "            (np.random.uniform(-1,1, (64, 2)),  np.random.uniform(-1,1, (2,1)))\n",
    "        ]\n",
    "\n",
    "def forward(x, theta):\n",
    "    w, b =theta[0]\n",
    "    x = jax.nn.relu(x@w) + b.T\n",
    "    w, b =theta[0]\n",
    "    x = jax.nn.relu(x@w) + b.T\n",
    "    w, b =theta[0]\n",
    "    x = jax.nn.relu(x@w) + b.T\n",
    "    return x\n",
    "\n",
    "\n",
    "params = get_nn_params()\n",
    "\n",
    "\n",
    "out = forward(x, params)\n",
    "out.shape\n",
    "\n"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "TracerIntegerConversionError",
     "evalue": "The __index__() method was called on the JAX Tracer object Traced<ShapedArray(int32[], weak_type=True)>with<DynamicJaxprTrace(level=1/0)>\nSee https://jax.readthedocs.io/en/latest/errors.html#jax.errors.TracerIntegerConversionError",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTracerIntegerConversionError\u001b[0m              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_111846/1777776336.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_111846/1777776336.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(x, theta)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mlayer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m@\u001b[0m\u001b[0mtheta\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtheta\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfori_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtheta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "    \u001b[0;31m[... skipping hidden 15 frame]\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_111846/1777776336.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(i, x)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m     \u001b[0mlayer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m@\u001b[0m\u001b[0mtheta\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtheta\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfori_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtheta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/tutorial_jax/.venv/lib/python3.7/site-packages/jax/core.py\u001b[0m in \u001b[0;36m__index__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    479\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    480\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__index__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 481\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mTracerIntegerConversionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    482\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    483\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTracerIntegerConversionError\u001b[0m: The __index__() method was called on the JAX Tracer object Traced<ShapedArray(int32[], weak_type=True)>with<DynamicJaxprTrace(level=1/0)>\nSee https://jax.readthedocs.io/en/latest/errors.html#jax.errors.TracerIntegerConversionError"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "source": [
    "lots_of_params = [get_nn_params() for _ in range(100)]\n",
    "# how do we parallelize this?\n",
    "\n",
    "stacked_tree = tree_stack(lots_of_params)\n",
    "\n",
    "x = np.random.uniform(-1,1, (512,10))\n",
    "\n",
    "\n",
    "seq_ensemble_forward = lambda x,trees: [forward(x,tree) for tree in trees]\n",
    "seq_ensemble_forward = jax.jit(seq_ensemble_forward)\n",
    "\n",
    "vmap_ensemble_forward = jax.vmap(forward, in_axes=[None,[(0, 0), (0, 0), (0,0)]])\n",
    "vmap_ensemble_forward = jax.jit(vmap_ensemble_forward)\n",
    "\n",
    "\n",
    "seq_ensemble_forward = jax.jit(seq_ensemble_forward)\n",
    "\n",
    "%timeit -n 50 seq_ensemble_forward(x, lots_of_params)\n",
    "%timeit -n 50 vmap_ensemble_forward(x, stacked_tree)\n",
    "\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The slowest run took 13.71 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
      "46.5 ms ± 73.1 ms per loop (mean ± std. dev. of 7 runs, 50 loops each)\n",
      "21.1 ms ± 1.15 ms per loop (mean ± std. dev. of 7 runs, 50 loops each)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# JAX magic functions\n",
    "\n",
    "# dataset\n",
    "x = np.random.uniform(-1,1, (100, 10))\n",
    "theta = np.random.uniform(-1,1, (10,1))\n",
    "theta_ = np.random.uniform(-1,1, (10,1))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "y = x @ theta # ground truth\n",
    "y_ = x @ theta_ # estimates\n",
    "\n",
    "# mse error\n",
    "def mse(y_,y):\n",
    "    pass\n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "# numpy and JAX syntax is similar in most cases, except one crucial annoying one...\n",
    "\n",
    "# random seed initialization\n",
    "random_seed = 123\n",
    "key = jax.random.PRNGKey(random_seed) # returns "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# to generate random numbers, we need to iteratively "
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.7.6",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.6 64-bit ('.venv': poetry)"
  },
  "interpreter": {
   "hash": "ccf5e81f62d872eea62a78649221eaf09e80342389ba706c701efe27d4b9b8fc"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}