{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JAX Basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "from jax import numpy as jnp\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# cool feature no. 1\n",
    "\n",
    "x = np.array([1.0, 2.0])[None].T\n",
    "y = jnp.array([1.0, 2.0])[None].T\n",
    "\n",
    "# numpy arrays are transformed to JAX tensors automatically at operation execution (no to_tensor, from_numpy mumbo-jumbo)\n",
    "print(\"Dot product:\", x.T@y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# jax arrays are immutable\n",
    "\n",
    "#y[0] = 3.0\n",
    "y.at[0].set(3.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can do this, by the following syntax (first uglyness)\n",
    "\n",
    "y.at[0].set(1.0) # returns a new array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The JIT\n",
    "\n",
    "Stands for \"Just in Time Compilation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "M = np.random.uniform(-0.1, 0.1, (500, 500))\n",
    "M_jax = jnp.array(M)\n",
    "\n",
    "def numpy_function(M):\n",
    "    return M @ M.T\n",
    "\n",
    "def function(M):\n",
    "    return M @ M.T\n",
    "\n",
    "@jax.jit\n",
    "def jit_function(M):\n",
    "    return  M @ M.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Numpy function:\")\n",
    "%timeit  numpy_function(M)\n",
    "print(\"JAX function:\")\n",
    "%timeit  function(M_jax)\n",
    "print(\"JAX jit function:\")\n",
    "%timeit  jit_function(M_jax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating Gradients\n",
    "\n",
    "Using the `jax.grad` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "from matplotlib import pyplot as plt\n",
    "# n-th order polynomial\n",
    "\n",
    "n = 6\n",
    "roots = np.random.uniform(-3, 3, n).astype(np.float64)\n",
    "def nth_order_polynomial(x, roots):\n",
    "    y = 1\n",
    "    for r in roots:\n",
    "        y = y*(x-r)\n",
    "    return y\n",
    "\n",
    "\n",
    "poly = jax.jit(functools.partial(nth_order_polynomial, roots=roots))\n",
    "\n",
    "x = jnp.linspace(-2, 2, 100)\n",
    "\n",
    "y = poly(x)\n",
    "\n",
    "\n",
    "plt.plot(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can trace the computation of the polynomial\n",
    "jax.make_jaxpr(poly)(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n-th order derivatives of this function?\n",
    "grad_func = poly\n",
    "nth_order_grads = []\n",
    "for i in range(n):\n",
    "\n",
    "    grad_func = jax.grad(grad_func)\n",
    "    y = jax.vmap(grad_func)(x)\n",
    "    plt.plot(x, y, label=f'n={i+1}')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JAX Can Differentiate with Respect to (almost) Anything\n",
    "\n",
    "As long as we register the type. But standard Python containers are supported out of the box.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def square(data):\n",
    "    return data['x']**2 + data['y']**2\n",
    "\n",
    "g = jax.grad(square)({\"x\": 1.0, \"y\": 2.0})\n",
    "\n",
    "print(f\"Gradient 1: {g}\")\n",
    "\n",
    "\n",
    "def square(data):\n",
    "    return data[0]**2 + data[1] ** 2\n",
    "\n",
    "g = jax.grad(square)([1.0, 2.0])\n",
    "print(f\"Gradient 2: {g}\")\n",
    "\n",
    "\n",
    "from dataclasses import dataclass\n",
    "@dataclass\n",
    "class Point:\n",
    "    x: float\n",
    "    y: float\n",
    "\n",
    "from jax.tree_util import register_pytree_node\n",
    "\n",
    "register_pytree_node(\n",
    "    Point, \n",
    "    lambda x: ([x.x, x.y], None), #unpacking\n",
    "    lambda d, x: Point(*x)\n",
    ")\n",
    "\n",
    "def square(data):\n",
    "    return data.x**2 + data.y ** 2\n",
    "\n",
    "g = jax.grad(square)(Point(1.0, 2.0))\n",
    "print(f\"Gradient 3: {g}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset for linear regression\n",
    "\n",
    "\n",
    "def generate_data(d, N):\n",
    "    x = np.random.uniform(-1,1, (N, d))\n",
    "    theta = np.random.uniform(-1,1, (d,1))\n",
    "    y = x @ theta\n",
    "    theta_ = np.random.uniform(-1,1, (d,1))\n",
    "    return x, y, theta, theta_\n",
    "\n",
    "x, y, theta, theta_ = generate_data(20, 500)\n",
    "\n",
    "\n",
    "\n",
    "def vis_lines(theta, theta_):\n",
    "    x = np.linspace(-2, 2, 100)\n",
    "    y = theta[0, 0]*x + theta[1,0]\n",
    "    y_ = theta_[0,0]*x+ theta_[1,0]\n",
    "    plt.plot(x, y, label='gt')\n",
    "    plt.plot(x, y_, label='pred')\n",
    "    plt.legend()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def predict(x, theta):\n",
    "    return x@theta\n",
    "\n",
    "def mse(x, theta, y):\n",
    "    y_ = predict(x, theta)\n",
    "    return ((y-y_)**2).mean()\n",
    "\n",
    "grad_func = jax.grad(mse, argnums=[1]) # returns a function which takes the same arguments as the wrapped one\n",
    "\n",
    "\n",
    "# returns tuple of gradients with respect to arguments of function\n",
    "grad, = grad_func(x, theta, y)\n",
    "\n",
    "# for practicality, this is also available\n",
    "loss, (grad,) = jax.value_and_grad(mse, argnums=[1])(x,theta_, y)\n",
    "print(f\"MSE loss: {loss}\")\n",
    "\n",
    "\n",
    "\n",
    "vis_lines(theta, theta_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stupid loop to optimize our model\n",
    "for _ in range(50):\n",
    "    loss, (grad,) = jax.value_and_grad(mse, argnums=[1])(x,theta_, y)\n",
    "    theta_ -= 0.01*grad \n",
    "\n",
    "print(f\"MSE: {loss}\")\n",
    "vis_lines(theta, theta_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what if we want to get a Jacobian?\n",
    "J, = jax.jacobian(predict, argnums=[0])(x[0], theta)\n",
    "J.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what about the Hessian?\n",
    "H, = jax.jacfwd(jax.jacrev(predict), argnums=[0])(x[0], theta)\n",
    "H.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2nd Order Optimization!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "d = 10\n",
    "\n",
    "@jax.jit\n",
    "def square(x):\n",
    "    return (x**2).sum(-1)\n",
    "\n",
    "\n",
    "x = np.linspace(-10, 10, 100)[:, None]\n",
    "plt.plot(x, square(x))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = jnp.array([10.0]*d)\n",
    "# how does the hessian look like?\n",
    "jax.hessian(square)(x0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets try to minimize it via gradient descent\n",
    "\n",
    "f = square\n",
    "x_ = x0\n",
    "trajx = []\n",
    "trajy = []\n",
    "\n",
    "err = 1e-3\n",
    "for i in range(200):\n",
    "    g = jax.grad(f)(x_)\n",
    "    x_ -= 0.01*g\n",
    "    trajx.append(x_[0])\n",
    "    trajy.append(f(x_))\n",
    "    if trajy[-1] < err:\n",
    "        break\n",
    "print(f\"Converged in {i} steps, {err}, {x_}\")\n",
    "plt.plot(x, f(x))\n",
    "plt.plot(trajx, trajy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets try to minimize it via gradient descent\n",
    "\n",
    "f = square\n",
    "x_ = x0\n",
    "trajx = [x0[0]]\n",
    "trajy = [f(x0)]\n",
    "f = square\n",
    "\n",
    "\n",
    "\n",
    "err = 1e-6\n",
    "for i in range(100):\n",
    "    g = jax.grad(f)(x_)[:, None]\n",
    "    H = jax.hessian(f)(x_)\n",
    "    x_ -= (jnp.linalg.inv(H) @ g).flatten()\n",
    "    trajx.append(x_[0])\n",
    "    trajy.append(f(x_))\n",
    "    if trajy[-1] < err:\n",
    "        break\n",
    "print(f\"Converged in {i} steps, {err}, {x_}\")\n",
    "plt.plot(x, f(x))\n",
    "plt.plot(trajx, trajy)\n",
    "plt.scatter(trajx, trajy, color='red')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorization / Parallelization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vmap usage\n",
    "# say we have a \"complicated function\" that we want to apply row-wise, ie. over axis=0\n",
    "\n",
    "M = np.random.uniform(1, 10, (200, 2))\n",
    "func = lambda x: x[0]**2 + jnp.exp(x[1])\n",
    "\n",
    "@jax.jit\n",
    "def naive(M):\n",
    "    return jnp.stack([func(x) for x in M])\n",
    "\n",
    "@jax.jit\n",
    "def with_vmap(M):\n",
    "    return jax.vmap(func)(M)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def with_pmap(M): # this is just for the sake of example, it goes across devices\n",
    "    return jax.pmap(func)(M)\n",
    "\n",
    "\n",
    "print(\"Naive:\")\n",
    "%timeit -n 50 naive(M)\n",
    "print(\"With vmap:\")\n",
    "%timeit -n 50 with_vmap(M)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note on Gradient Calculation\n",
    "\n",
    "<p style=\"font-size:20px\">\n",
    "\n",
    "Some options:\n",
    "* finite differences $\\frac{df}{dx} = \\lim_{h \\mapsto 0} \\frac{f(x+h)-f(x)}{h}$\n",
    "* symbolic\n",
    "* automatic differentiation - most of deep learning\n",
    "<p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def square(x):\n",
    "    return x**2\n",
    "\n",
    "def finite_differences(f, h):\n",
    "    def func(x):\n",
    "        return  (f(x+h)-f(x))/h\n",
    "    return func\n",
    "\n",
    "\n",
    "finite_differences(square, 1e-5)(2.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autodiff Forward vs. Reverse Mode\n",
    "\n",
    "<p style=\"font-size:16px\">\n",
    "\n",
    "**Forward Mode**: augments the outputs of the forward pass with their derivatives in a (primal, tangent) tuple $(x, \\dot x)$. This is preferred in the case where the number of inputs is much smaller than the number of outputs, in practice we compute a Jacobian-vector product.\n",
    "\n",
    "\n",
    "**Reverse Mode**: comes in two stages. First we make a forward pass through our computation graph which is followed by computation of partial derivatives with respect to intermediate variables (adjoints). Backpropagation is a special case of reverse mode autodiff. Vector-Jacobian product.\n",
    "\n",
    "**Reverse on Forward**: hybrid, for example computing Hessian.\n",
    "\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# back to our Hessian example, let's time it with different ways of computing the gradient\n",
    "print(\"Only reverse mode autodiff:\")\n",
    "%timeit jax.jacrev(jax.jacrev(predict), argnums=[0])(x[0], theta)\n",
    "print(\"Hybrid autodiff:\")\n",
    "%timeit jax.jacrev(jax.jacfwd(predict), argnums=[0])(x[0], theta)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JAX Random Numbers\n",
    "\n",
    "Random numbers in JAX are annoying. There is no stateful random number generator, but we need to pass around a `key` that we split with `jax.random.split`. This is also where JAX syntax for distributions and numpy syntax differs considerably."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 123\n",
    "key = jax.random.PRNGKey(random_seed) \n",
    "\n",
    "rngs = jax.random.split(key, 10)\n",
    "\n",
    "\n",
    "print(\"These are 10 random numbers\")\n",
    "print(jnp.array([jax.random.normal(k) for k in rngs]))\n",
    "print(\"These are the same numbers\")\n",
    "print(jnp.array([jax.random.normal(k) for k in rngs]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Here come the neural networks..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# say we use an ensemble neural network (this cause a bit of pain for me and Sebastian to implement in PyTorch)\n",
    "from jax.tree_util import tree_flatten, tree_unflatten\n",
    "\n",
    "\n",
    "\n",
    "def tree_stack(trees):\n",
    "    \"\"\"Takes a list of trees and stacks every corresponding leaf.\n",
    "    For example, given two trees ((a, b), c) and ((a', b'), c'), returns\n",
    "    ((stack(a, a'), stack(b, b')), stack(c, c')).\n",
    "    Useful for turning a list of objects into something you can feed to a\n",
    "    vmapped function.\n",
    "    \"\"\"\n",
    "    leaves_list = []\n",
    "    treedef_list = []\n",
    "    for tree in trees:\n",
    "        leaves, treedef = tree_flatten(tree)\n",
    "        leaves_list.append(leaves)\n",
    "        treedef_list.append(treedef)\n",
    "\n",
    "    grouped_leaves = zip(*leaves_list)\n",
    "    result_leaves = [jnp.stack(l) for l in grouped_leaves]\n",
    "    return treedef_list[0].unflatten(result_leaves)\n",
    "\n",
    "\n",
    "def tree_unstack(tree):\n",
    "    \"\"\"Takes a tree and turns it into a list of trees. Inverse of tree_stack.\n",
    "    For example, given a tree ((a, b), c), where a, b, and c all have first\n",
    "    dimension k, will make k trees\n",
    "    [((a[0], b[0]), c[0]), ..., ((a[k], b[k]), c[k])]\n",
    "    Useful for turning the output of a vmapped function into normal objects.\n",
    "    \"\"\"\n",
    "    leaves, treedef = tree_flatten(tree)\n",
    "    n_trees = leaves[0].shape[0]\n",
    "    new_leaves = [[] for _ in range(n_trees)]\n",
    "    for leaf in leaves:\n",
    "        for i in range(n_trees):\n",
    "            new_leaves[i].append(leaf[i])\n",
    "    new_trees = [treedef.unflatten(l) for l in new_leaves]\n",
    "    return new_trees\n",
    "\n",
    "\n",
    "\n",
    "def get_nn_params():\n",
    "        return [\n",
    "            (np.random.uniform(-1,1, (10, 512)), np.random.uniform(-1,1, (512, 1))),\n",
    "            (np.random.uniform(-1,1, (512, 256)), np.random.uniform(-1,1, (256, 1))),\n",
    "            (np.random.uniform(-1,1, (256, 2)),  np.random.uniform(-1,1, (2,1)))\n",
    "        ]\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def forward(x, theta):\n",
    "    for w, b in theta:\n",
    "        x = jax.nn.relu(x@w) + b.T\n",
    "    return x\n",
    "\n",
    "\n",
    "\n",
    "params = get_nn_params()\n",
    "\n",
    "\n",
    "out = forward(x, params)\n",
    "out.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lots_of_params = [get_nn_params() for _ in range(20)]\n",
    "# how do we parallelize this?\n",
    "\n",
    "stacked_tree = tree_stack(lots_of_params)\n",
    "\n",
    "def seq_ensemble_forward(x, trees):\n",
    "    return jnp.stack([forward(x,tree) for tree in trees])\n",
    "\n",
    "\n",
    "seq_ensemble_forward = jax.jit(seq_ensemble_forward)\n",
    "\n",
    "vmap_ensemble_forward = jax.vmap(forward, in_axes=[None,[(0, 0)]*len(lots_of_params[0])])\n",
    "vmap_ensemble_forward = jax.jit(vmap_ensemble_forward)\n",
    "\n",
    "\n",
    "\n",
    "# seems to still not be better than linear speedup when stacking the matrices into tensors, but there is some improvement (possibly limited by hardware)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what about simple stacking?\n",
    "\n",
    "@jax.jit\n",
    "def stacked_forward(x, theta):\n",
    "    for w, b in theta:\n",
    "        x = jax.nn.relu(x@w) + b.transpose(0,2,1)\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x = np.random.uniform(-1,1, (1024,10))\n",
    "\n",
    "vmap_ensemble_forward(x, stacked_tree)\n",
    "stacked_forward(x, stacked_tree)\n",
    "seq_ensemble_forward(x, lots_of_params)\n",
    "\n",
    "%timeit stacked_forward(x, stacked_tree).block_until_ready() \n",
    "\n",
    "\n",
    "%timeit vmap_ensemble_forward(x, stacked_tree).block_until_ready() \n",
    "\n",
    "\n",
    "%timeit seq_ensemble_forward(x, lots_of_params).block_until_ready() \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# JAX magic functions\n",
    "\n",
    "# dataset\n",
    "x = np.random.uniform(-1,1, (100, 10))\n",
    "theta = np.random.uniform(-1,1, (10,1))\n",
    "theta_ = np.random.uniform(-1,1, (10,1))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "y = x @ theta # ground truth\n",
    "y_ = x @ theta_ # estimates\n",
    "\n",
    "# mse error\n",
    "def mse(y_,y):\n",
    "    pass\n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numpy and JAX syntax is similar in most cases, except one crucial annoying one...\n",
    "\n",
    "# random seed initialization\n",
    "random_seed = 123\n",
    "key = jax.random.PRNGKey(random_seed) # returns "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flax: Making Things More Simple\n",
    "\n",
    "Alternatives: Haiku, Stax, Objax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import flax\n",
    "import jax\n",
    "from jax import numpy as jnp\n",
    "import numpy as np\n",
    "from typing import Optional\n",
    "# for details about this, read about python >=3.7 dataclasses\n",
    "\n",
    "class MLP(flax.linen.Module):\n",
    "    # here, ordering is preserved\n",
    "    num_hidden: int\n",
    "    hidden_size: int\n",
    "    outputs: int\n",
    "    act_function: Optional[str] = 'relu'\n",
    "\n",
    "    def setup(self):\n",
    "        self.layers = [flax.linen.Dense(features=64) for _ in range(self.num_hidden)]\n",
    "        self.last_layer = flax.linen.Dense(self.outputs)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        act_function = getattr(flax.linen, self.act_function)\n",
    "        for layer in self.layers[:-1]:\n",
    "            x = act_function(layer(x))\n",
    "        # don't apply act in last layer\n",
    "        x = self.layers[-1](x)\n",
    "        return x\n",
    "\n",
    "\n",
    "key = jax.random.PRNGKey(0)\n",
    "\n",
    "X = np.random.randn(128, 10)\n",
    "\n",
    "model = MLP(2, 64, 2)\n",
    "\n",
    "# we need to call the init function, takes a batch and key\n",
    "key, _ = jax.random.split(key)\n",
    "params = model.init(key, X)\n",
    "\n",
    "\n",
    "# we need to call the apply function, which also takes the model parameters\n",
    "y_ = model.apply(params, X)\n",
    "y_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import flax\n",
    "from typing import Optional\n",
    "# for details about this, read about python 3.7 datamodule class\n",
    "\n",
    "class MLPCompact(flax.linen.Module):\n",
    "    # here, ordering is preserved\n",
    "    num_hidden: int\n",
    "    hidden_size: int\n",
    "    outputs: int\n",
    "    act_function: Optional[str] = 'relu'\n",
    "    \n",
    "    @flax.linen.compact\n",
    "    def __call__(self, x):\n",
    "        act_function = getattr(flax.linen, self.act_function)\n",
    "        for _ in range(self.num_hidden):\n",
    "            x = flax.linen.Dense(self.hidden_size)(x)\n",
    "            x = act_function(x)\n",
    "        # don't apply act in last layer\n",
    "        x = flax.linen.Dense(self.outputs)(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "key = jax.random.PRNGKey(0)\n",
    "X = np.random.randn(128, 10)\n",
    "\n",
    "model = MLPCompact(2, 64, 2)\n",
    "\n",
    "# we need to call the init function, takes a batch and key\n",
    "params = model.init(key, X)\n",
    "\n",
    "\n",
    "# we need to call the apply function, which also takes the model parameters\n",
    "y_ = model.apply(params, X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VAE Example\n",
    "\n",
    "This is our loss functions (maximizing ELBO)\n",
    "$$\n",
    "\\mathcal{L}(\\theta, \\phi) = -\\mathbb{E}[p_\\phi(x | z)] +  \\mathbb{KL}[q_\\theta(z | x) || p(z)]  $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss functions\n",
    "\n",
    "@jax.vmap\n",
    "def kl_divergence(mean, logvar):\n",
    "  return -0.5 * jnp.sum(1 + logvar - jnp.square(mean) - jnp.exp(logvar))\n",
    "\n",
    "\n",
    "@jax.vmap\n",
    "def binary_cross_entropy_with_logits(logits, labels):\n",
    "  logits = flax.linen.log_sigmoid(logits)\n",
    "  return -jnp.sum(labels * logits + (1. - labels) * jnp.log(-jnp.expm1(logits)))\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def loss(logits, mean, logvar):\n",
    "  reconstruction_loss = binary_cross_entropy_with_logits(logits, image)\n",
    "  kl_div = kl_divergence(mean, logvar)\n",
    "  return jnp.mean(reconstruction_loss + kl_div)\n",
    "\n",
    "from typing import Sequence\n",
    "class Sequential(flax.linen.Module):\n",
    "  layers: Sequence[flax.linen.Module]\n",
    "\n",
    "  def __call__(self, x):\n",
    "    for layer in self.layers:\n",
    "      x = layer(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "class MLPCompact(flax.linen.Module):\n",
    "    # here, ordering is preserved\n",
    "    num_hidden: int\n",
    "    hidden_size: int\n",
    "    outputs: int\n",
    "    act_function: Optional[str] = 'relu'\n",
    "    \n",
    "\n",
    "    def forward(self, x):\n",
    "      act_function = getattr(flax.linen, self.act_function)\n",
    "      for _ in range(self.num_hidden):\n",
    "          x = flax.linen.Dense(self.hidden_size)(x)\n",
    "          x = act_function(x)\n",
    "      # don't apply act in last layer\n",
    "      x = flax.linen.Dense(self.outputs)(x)\n",
    "      return x\n",
    "\n",
    "    @flax.linen.compact\n",
    "    def __call__(self, x):\n",
    "      return self.forward(x)\n",
    "\n",
    "class Decoder(MLPCompact):\n",
    "\n",
    "    def setup(self):\n",
    "        return\n",
    "\n",
    "    @flax.linen.compact\n",
    "    def __call__(self, x):\n",
    "      x = super().forward(x)\n",
    "      return x\n",
    "\n",
    "from numpyro.distributions import Normal\n",
    "\n",
    "class VAE(flax.linen.Module):\n",
    "  latents: int\n",
    "  outputs: int\n",
    "  \n",
    "  def setup(self):\n",
    "    self.encoder = MLPCompact(3, 128, self.latents*2)\n",
    "    self.decoder = Decoder(3, 256, outputs=self.outputs)\n",
    "\n",
    "  def __call__(self, key, x, deterministic=False):\n",
    "\n",
    "    gauss_params = self.encoder(x)\n",
    "    mu, logvar = jnp.split(gauss_params, 2, -1)\n",
    "    sigma = jnp.sqrt(jnp.exp(logvar))\n",
    "\n",
    "    if not deterministic:\n",
    "      gauss_dist = Normal(mu, sigma)\n",
    "      z = gauss_dist.rsample(key)\n",
    "    else:\n",
    "      z = mu      \n",
    "    return self.decoder(z), z, mu, logvar\n",
    "\n",
    "  def generate(self, key, samples):\n",
    "    # sample from prior distribution \n",
    "    mu = jnp.zeros((samples, self.latents))\n",
    "    sigma = jnp.ones((samples, self.latents))\n",
    "    gauss_dist = Normal(mu, sigma)\n",
    "    z = gauss_dist.sample(key)\n",
    "    logits =  self.decoder(z)\n",
    "    return jnp.round(flax.linen.log_sigmoid(logits)).reshape(-1, 28, 28)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#vae = VAE(20, 28*28)\n",
    "#params = vae.init(rk1, rk2, image)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "plt.imshow(imgs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Construct a tf.data.Dataset\n",
    "ds = tfds.as_numpy(tfds.load('binarized_mnist', split='train', batch_size=64, shuffle_files=True))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "from flax.training import train_state\n",
    "import optax\n",
    "\n",
    "# Build your input pipeline\n",
    "#params = None\n",
    "\n",
    "# create optimizer\n",
    "optimizer = optax.adam(1e-3)\n",
    "\n",
    "# this returns a function!\n",
    "def construct_training_step(model, optimizer):\n",
    "  # model forward\n",
    "  def model_loss(params, key, image):\n",
    "    logits, z, mean, logvar = model.apply(params, key, image)\n",
    "    loss = jnp.mean(binary_cross_entropy_with_logits(logits, image) + kl_divergence(mean, logvar))\n",
    "    return loss\n",
    "  \n",
    "  grad_func = jax.value_and_grad(model_loss, argnums=0)\n",
    "  # this is the function that we call in the end\n",
    "  def update_func(params, opt_state, key,  image):\n",
    "    loss, grads = grad_func(params, key, image)\n",
    "    updates, opt_state = optimizer.update(grads, opt_state)\n",
    "    return loss, updates, opt_state\n",
    "\n",
    "  return jax.jit(update_func)\n",
    "\n",
    "for e in range(20):\n",
    "  tqdm_iter = tqdm(enumerate(ds))\n",
    "  for i, batch in tqdm_iter:\n",
    "    image  = batch[\"image\"]\n",
    "    \n",
    "    image = image.reshape(64, -1)\n",
    "    if params is None:\n",
    "      # initialize params, optimizer\n",
    "      rk1, rk2 = jax.random.split(key)\n",
    "      params = vae.init(rk1, rk2, image)\n",
    "      opt_state = optimizer.init(params)\n",
    "      training_step = construct_training_step(vae, optimizer)\n",
    "\n",
    "    key, _ = jax.random.split(key)\n",
    "    logits, z, mean, logvar = vae.apply(params, key, image)\n",
    "\n",
    "    # this is equivalent to  .backward and optimizer.step in PyTorch\n",
    "    loss, updates, opt_state = training_step(params, opt_state, key, image)\n",
    "    params = optax.apply_updates(params, updates)\n",
    "    \n",
    "    tqdm_iter.set_description(f\"Epoch {e:5d}, batch {i:5d}, loss={loss:10.5f}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vae' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_13114/41434178.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# now we sample from p(z)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mimgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'vae' is not defined"
     ]
    }
   ],
   "source": [
    "# now we sample from p(z)\n",
    "key, _ = jax.random.split(key)\n",
    "imgs = vae.apply(params, key, 2, method=vae.generate)\n",
    "plt.imshow(imgs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Meta Learning: MAML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad(g)(x0) = 4.0\n",
      "x0 - grad(g)(x0) = -2.0\n",
      "maml_objective(x,y)=5.0\n",
      "x0 - maml_objective(x,y) = -2.0\n"
     ]
    }
   ],
   "source": [
    "# gradients of gradients test for MAML\n",
    "# check numerics\n",
    "g = lambda x, y : jnp.square(x) + y\n",
    "x0 = 2.\n",
    "y0 = 1.\n",
    "print('grad(g)(x0) = {}'.format(jax.grad(g)(x0, y0))) # 2x = 4\n",
    "print('x0 - grad(g)(x0) = {}'.format(x0 - jax.grad(g)(x0, y0))) # x - 2x = -2\n",
    "def maml_objective(x, y):\n",
    "    return g(x - jax.grad(g)(x, y), y)\n",
    "print('maml_objective(x,y)={}'.format(maml_objective(x0, y0))) # x**2 + 1 = 5\n",
    "print('x0 - maml_objective(x,y) = {}'.format(x0 - jax.grad(maml_objective)(x0, y0))) # x - (2x) = -2.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLPCompact(2, 64, 1)\n",
    "params  = mlp.init(key, (-1,1))\n",
    "optimizer = optax.adam(1e-3)\n",
    "opt_state = optimizer.init(params)\n",
    "training_step = construct_training_step(mlp, optimizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "\n",
    "\n",
    "\n",
    "def mse(params, inputs, targets):\n",
    "    # Computes average loss for the batch\n",
    "    print(\"shape:\", inputs.shape)\n",
    "    predictions = mlp.apply(params, inputs)\n",
    "    return jnp.mean((targets - predictions)**2)\n",
    "\n",
    "def inner_update(p, x1, y1, alpha=.1):\n",
    "    grads = jax.grad(mse)(p, x1, y1)\n",
    "    inner_sgd_fn = lambda g, state: (state - alpha*g)\n",
    "    return tree_multimap(inner_sgd_fn, grads, p)\n",
    "\n",
    "def maml_loss(p, x1, y1, x2, y2):\n",
    "    p2 = inner_update(p, x1, y1)\n",
    "    return loss(p2, x2, y2)\n",
    "\n",
    "# vmapped version of maml loss.\n",
    "# returns scalar for all tasks.\n",
    "def batch_maml_loss(p, x1_b, y1_b, x2_b, y2_b):\n",
    "    task_losses = jax.vmap(functools.partial(maml_loss, p))(x1_b, y1_b, x2_b, y2_b)\n",
    "    return jnp.mean(task_losses)\n",
    "\n",
    "@jax.jit\n",
    "def step(i, opt_state, x1, y1, x2, y2):\n",
    "\n",
    "    g = jax.grad(batch_maml_loss)(params, x1, y1, x2, y2)\n",
    "    l = batch_maml_loss(p, x1, y1, x2, y2)\n",
    "    return opt_update(i, g, opt_state), l\n",
    "\n",
    "\n",
    "# this returns a function!\n",
    "def construct_training_step(model, optimizer):\n",
    "  # model forward\n",
    "  grad_func = jax.value_and_grad(batch_maml_loss)\n",
    "  # this is the function that we call in the end\n",
    "  def update_func(params, opt_state, x1_b, y1_b, x2_b, y2_b):\n",
    "    loss, grads = grad_func(params, x1_b, y1_b, x2_b, y2_b)\n",
    "    updates, opt_state = optimizer.update(grads, opt_state)\n",
    "    return loss, updates, opt_state\n",
    "  return jax.jit(update_func)\n",
    "\n",
    "import numpy as np\n",
    "def sample_tasks(outer_batch_size, inner_batch_size):\n",
    "    # Select amplitude and phase for the task\n",
    "    As = []\n",
    "    phases = []\n",
    "    for _ in range(outer_batch_size):        \n",
    "        As.append(np.random.uniform(low=0.1, high=.5))\n",
    "        phases.append(np.random.uniform(low=0., high=np.pi))\n",
    "    def get_batch():\n",
    "        xs, ys = [], []\n",
    "        for A, phase in zip(As, phases):\n",
    "            x = np.random.uniform(low=-5., high=5., size=(inner_batch_size, 1))\n",
    "            y = A * np.sin(x + phase)\n",
    "            xs.append(x)\n",
    "            ys.append(y)\n",
    "        return np.stack(xs), np.stack(ys)\n",
    "    x1, y1 = get_batch()\n",
    "    x2, y2 = get_batch()\n",
    "    return x1, y1, x2, y2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([[ 1.217549  ,  0.96401066, -0.33445993,  1.1671789 ,\n",
       "               0.17691591, -0.13342527,  0.48303285, -0.6133985 ,\n",
       "               1.0132715 , -0.4752681 , -0.77856994, -0.14450656,\n",
       "              -0.05202928, -0.7119731 ,  0.3950571 , -0.6889251 ,\n",
       "               0.5499303 , -1.2038087 ,  0.63696206, -0.84807223,\n",
       "              -0.21197855,  0.0369479 ,  0.8223873 ,  1.0972558 ,\n",
       "              -0.23932524, -0.11999835,  0.16028355,  0.58580595,\n",
       "               1.2142421 ,  0.4585943 ,  0.78676397, -0.06160979,\n",
       "               0.6225825 , -0.8867102 , -1.3376327 , -0.8195154 ,\n",
       "               1.0616896 ,  0.35472408, -0.31525803, -0.41200545,\n",
       "              -0.0381908 ,  0.22006422, -0.00762545, -0.07454741,\n",
       "              -0.36376864, -0.51508933, -0.43315154, -1.1841109 ,\n",
       "              -0.39475057, -0.71836525,  0.19761282, -0.27437112,\n",
       "              -0.16490982, -0.6765223 , -0.68541926,  0.5837301 ,\n",
       "               0.2580216 ,  0.4211686 , -0.96356535, -0.65198255,\n",
       "               0.02731689, -0.12161921, -0.17070362, -0.57317805],\n",
       "             [ 0.3327177 ,  1.0943112 , -0.9874902 ,  0.5750975 ,\n",
       "              -0.8423987 , -0.24639688,  1.0338149 , -0.7582868 ,\n",
       "               1.2779737 , -0.10245758, -0.5801031 , -0.5714204 ,\n",
       "              -0.62953633, -0.3985731 , -0.35413253,  1.1921264 ,\n",
       "              -0.3663379 , -0.73398554, -0.8511014 ,  0.17162675,\n",
       "              -0.4539279 ,  0.17498134,  0.5815921 , -0.74478805,\n",
       "               0.50764257,  0.9084648 , -0.8359228 ,  1.1095431 ,\n",
       "              -1.4565622 , -0.07832   ,  0.3916607 ,  0.9513266 ,\n",
       "               0.12893972, -0.05834335,  0.7156991 , -0.50049645,\n",
       "              -0.18810165, -1.4220675 ,  0.4851352 , -0.34151712,\n",
       "               0.77741057,  1.4347417 , -0.20878805,  0.233984  ,\n",
       "              -1.3361984 ,  0.20293775,  0.44398063, -0.3207938 ,\n",
       "               0.7028059 , -0.68657   ,  0.3193759 , -0.2415642 ,\n",
       "              -0.19765794, -0.19537003, -0.26805344, -0.12819356,\n",
       "               0.12729917, -0.30044976,  0.30616665,  1.1779265 ,\n",
       "              -0.7830591 , -0.29834673,  1.2458665 ,  0.71270573]],            dtype=float32)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params['params']['Dense_0']['kernel']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (20, 1)\n"
     ]
    },
    {
     "ename": "ScopeParamShapeError",
     "evalue": "Inconsistent shapes between value and initializer for parameter \"kernel\" in \"/Dense_0\": (2, 64), (1, 64). (https://flax.readthedocs.io/en/latest/flax.errors.html#flax.errors.ScopeParamShapeError)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mScopeParamShapeError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_13114/4036223199.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mx1_b\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my1_b\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2_b\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my2_b\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_tasks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx1_b\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my1_b\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2_b\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my2_b\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mnp_batched_maml_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_updates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "    \u001b[0;31m[... skipping hidden 13 frame]\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_13114/1687923837.py\u001b[0m in \u001b[0;36mupdate_func\u001b[0;34m(params, opt_state, x1_b, y1_b, x2_b, y2_b)\u001b[0m\n\u001b[1;32m     37\u001b[0m   \u001b[0;31m# this is the function that we call in the end\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mupdate_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx1_b\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my1_b\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2_b\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my2_b\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx1_b\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my1_b\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2_b\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my2_b\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m     \u001b[0mupdates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "    \u001b[0;31m[... skipping hidden 7 frame]\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_13114/1687923837.py\u001b[0m in \u001b[0;36mbatch_maml_loss\u001b[0;34m(p, x1_b, y1_b, x2_b, y2_b)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m# returns scalar for all tasks.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mbatch_maml_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx1_b\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my1_b\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2_b\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my2_b\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mtask_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunctools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaml_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1_b\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my1_b\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2_b\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my2_b\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mjnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask_losses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "    \u001b[0;31m[... skipping hidden 3 frame]\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_13114/597303877.py\u001b[0m in \u001b[0;36mmaml_loss\u001b[0;34m(p, x1, y1, x2, y2)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmaml_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mp2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minner_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_13114/597303877.py\u001b[0m in \u001b[0;36minner_update\u001b[0;34m(p, x1, y1, alpha)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0minner_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0minner_sgd_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtree_multimap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minner_sgd_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "    \u001b[0;31m[... skipping hidden 9 frame]\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_13114/597303877.py\u001b[0m in \u001b[0;36mmse\u001b[0;34m(params, inputs, targets)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m# Computes average loss for the batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"shape:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mjnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtargets\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "    \u001b[0;31m[... skipping hidden 7 frame]\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_13114/3812918578.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mact_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_hidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mact_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;31m# don't apply act in last layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "    \u001b[0;31m[... skipping hidden 3 frame]\u001b[0m\n",
      "\u001b[0;32m~/Projects/tutorial_jax/.venv/lib/python3.7/site-packages/flax/linen/linear.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    171\u001b[0m     kernel = self.param('kernel',\n\u001b[1;32m    172\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel_init\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m                         (inputs.shape[-1], self.features))\n\u001b[0m\u001b[1;32m    174\u001b[0m     \u001b[0mkernel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m     y = lax.dot_general(inputs, kernel,\n",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "\u001b[0;32m~/Projects/tutorial_jax/.venv/lib/python3.7/site-packages/flax/core/scope.py\u001b[0m in \u001b[0;36mparam\u001b[0;34m(self, name, init_fn, *init_args)\u001b[0m\n\u001b[1;32m    622\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mjnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mjnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mabs_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    623\u001b[0m           raise errors.ScopeParamShapeError(name, self.path_text, \n\u001b[0;32m--> 624\u001b[0;31m               jnp.shape(val), jnp.shape(abs_val))\n\u001b[0m\u001b[1;32m    625\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    626\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_mutable_collection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'params'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mScopeParamShapeError\u001b[0m: Inconsistent shapes between value and initializer for parameter \"kernel\" in \"/Dense_0\": (2, 64), (1, 64). (https://flax.readthedocs.io/en/latest/flax.errors.html#flax.errors.ScopeParamShapeError)"
     ]
    }
   ],
   "source": [
    "np_batched_maml_loss = []\n",
    "K=20\n",
    "for i in range(20000):\n",
    "    x1_b, y1_b, x2_b, y2_b = sample_tasks(4, K)\n",
    "    loss, updates, opt_state = training_step(params, opt_state, x1_b, y1_b, x2_b, y2_b)\n",
    "    np_batched_maml_loss.append(l)\n",
    "    params = optax.apply_updates(params, updates)\n",
    "    if i % 1000 == 0:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoregressive Flows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_checkerboard, make_moons\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "X_train, X_test = make_moons(5000, noise=0.07)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "_ =  ax.hist2d(*X_train.T, bins=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpyro.distributions.flows import InverseAutoregressiveTransform\n",
    "from numpyro.distributions import Normal\n",
    "import numpyro\n",
    "from typing import Sequence\n",
    "\n",
    "\n",
    "\n",
    "def get_masks(input_dim, hidden_dim=64, num_hidden=1):\n",
    "    masks = []\n",
    "    input_degrees = jnp.arange(input_dim)\n",
    "    degrees = [input_degrees]\n",
    "\n",
    "    for n_h in range(num_hidden + 1):\n",
    "        degrees += [jnp.arange(hidden_dim) % (input_dim - 1)]\n",
    "    degrees += [input_degrees % input_dim - 1]\n",
    "\n",
    "    for (d0, d1) in zip(degrees[:-1], degrees[1:]):\n",
    "        masks += [jnp.transpose(jnp.expand_dims(d1, -1) >= jnp.expand_dims(d0, 0)).astype(jnp.float32)]\n",
    "    return masks\n",
    "\n",
    "def masked_transform(rng, input_dim):\n",
    "    masks = get_masks(input_dim, hidden_dim=64, num_hidden=1)\n",
    "    act = stax.Relu\n",
    "    init_fun, apply_fun = stax.serial(\n",
    "        flows.MaskedDense(masks[0]),\n",
    "        act,\n",
    "        flows.MaskedDense(masks[1]),\n",
    "        act,\n",
    "        flows.MaskedDense(masks[2].tile(2)),\n",
    "    )\n",
    "    _, params = init_fun(rng, (input_dim,))\n",
    "    return params, apply_fun\n",
    "\n",
    "init_fun = flows.Flow(\n",
    "    flows.Serial(*(flows.MADE(masked_transform), flows.Reverse()) * 5),\n",
    "    flows.Normal(),\n",
    ")\n",
    "\n",
    "params, log_pdf, sample = init_fun(flow_rng, input_dim)\n",
    "\n",
    "\n",
    "\n",
    "class Flow(flax.linen.Module):\n",
    "    base_dist: numpyro.distributions.Distribution\n",
    "    transforms: Sequence[flax.nn.Module]\n",
    "    \n",
    "    def __call__(self, key, num_samples):\n",
    "        return self.forward(key, num_samples)\n",
    "\n",
    "    def forward(self, key, num_samples):\n",
    "        logdet = jnp.zeros(num_samples)\n",
    "        x = self.base_dist.sample(key, (1, num_samples,1)).reshape(-1,2)\n",
    "        for transform in self.transforms:\n",
    "            x, t_logdet = transform(x)\n",
    "            logdet+=t_logdet\n",
    "        return x, self.base_dist.expand(x.shape).log_prob(x).sum(-1)+t_logdet[:]\n",
    "\n",
    "    def inverse(self, x):\n",
    "        logdet = jnp.zeros(x.shape[0])\n",
    "        for transform in reversed(self.transforms):\n",
    "            x, t_logdet = transform.inverse(x)\n",
    "            logdet+=t_logdet\n",
    "        return x, self.base_dist.expand(x.shape).log_prob(x).sum(-1) + t_logdet[:, None]\n",
    "    \n",
    "\n",
    "class MADE(flax.linen.Module):\n",
    "    arnn: flax.nn.Module\n",
    "    \n",
    "    def inverse(self, x):\n",
    "        log_weight, bias = self.arnn(x).split(2, axis=1)\n",
    "        outputs = (x - bias) * jnp.exp(-log_weight)\n",
    "        log_det_jacobian = -log_weight.sum(-1)\n",
    "        return outputs, log_det_jacobian\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = jnp.zeros_like(x)\n",
    "        for i_col in range(x.shape[1]):\n",
    "            log_weight, bias = self.arnn(outputs).split(2, axis=1)\n",
    "            outputs = jax.ops.index_update(\n",
    "                outputs, jax.ops.index[:, i_col], x[:, i_col] * jnp.exp(log_weight[:, i_col]) + bias[:, i_col]\n",
    "            )\n",
    "        log_det_jacobian = -log_weight.sum(-1)\n",
    "        return outputs, log_det_jacobian\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return self.forward(x)\n",
    "\n",
    "arnn = MLPCompact(2, 64, 2)\n",
    "made = MADE(arnn)\n",
    "flow = Flow(Normal(jnp.zeros(2), jnp.ones(2)), [made])\n",
    "\n",
    "x = jnp.zeros((10, 2))\n",
    "key, _ = jax.random.split(key)\n",
    "params = flow.init(key, key, 10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import optax\n",
    "\n",
    "def arr_iterator(arr, batch_size):\n",
    "    iters = int(arr.shape[0]/batch_size)\n",
    "    for i in range(0, iters, batch_size):\n",
    "        yield arr[i*batch_size:(i+1)*batch_size]\n",
    "\n",
    "optimizer = optax.adam(1e-3)\n",
    "def construct_training_step(model, optimizer):\n",
    "  # model forward\n",
    "  def model_loss(params, x):\n",
    "    _, logprob = model.apply(params, x, method=model.inverse)\n",
    "    return jnp.mean(-logprob) # NLL\n",
    "  \n",
    "  grad_func = jax.value_and_grad(model_loss, argnums=0)\n",
    "  # this is the function that we call in the end\n",
    "  def update_func(params, opt_state,  x):\n",
    "    loss, grads = grad_func(params, x)\n",
    "    updates, opt_state = optimizer.update(grads, opt_state)\n",
    "    return loss, updates, opt_state\n",
    "\n",
    "  return jax.jit(update_func)\n",
    "\n",
    "\n",
    "#params = None\n",
    "for e in range(100):\n",
    "  #tqdm_iter = tqdm(arr_iterator(X_train, 128))\n",
    "  for i, batch in enumerate(arr_iterator(X_train, 128)):\n",
    "\n",
    "    if params is None:\n",
    "      # initialize params, optimizer\n",
    "      rk1, rk2 = jax.random.split(key)\n",
    "      params = flow.init(rk1, rk2, 10)\n",
    "      opt_state = optimizer.init(params)\n",
    "      training_step = construct_training_step(flow, optimizer)\n",
    "\n",
    "    key, _ = jax.random.split(key)\n",
    "    loss, updates, opt_state = training_step(params, opt_state, x)\n",
    "    params = optax.apply_updates(params, updates)\n",
    "  print(f\"Epoch {e:5d}, batch {i:5d}, loss={loss:10.5f}\")\n",
    "    #tqdm_iter.set_description(f\"Epoch {e:5d}, batch {i:5d}, loss={loss:10.5f}\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, logdet  = flow.apply(params, key, 200)\n",
    "_ = plt.hist2d(*x.T, bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dist = Normal(jnp.zeros(2),jnp.ones(2))\n",
    "base_dist.expand(x.shape).log_prob(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shapea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits, z, mean, logvar = vae.apply(params, key, image)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stein Variational Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SVGD(flax.linen.Module):\n",
    "  base_dist: numpyro.distributions.Distribution\n",
    "  kernel: callable\n",
    "\n",
    "  def __init__(self, P, K, optimizer):\n",
    "    self.P = P\n",
    "    self.K = K\n",
    "    self.optim = optimizer\n",
    "\n",
    "  def phi(self, X):\n",
    "    X = X.detach().requires_grad_(True)\n",
    "\n",
    "    log_prob = self.base_dist.log_prob(X)\n",
    "    score_func = autograd.grad(log_prob.sum(), X)[0]\n",
    "\n",
    "    K_XX = self.K(X, X.detach())\n",
    "    grad_K = -autograd.grad(K_XX.sum(), X)[0]\n",
    "\n",
    "    phi = (K_XX.detach().matmul(score_func) + grad_K) / X.size(0)\n",
    "\n",
    "    return phi\n",
    "\n",
    "  def step(self, X):\n",
    "    self.optim.zero_grad()\n",
    "    X.grad = -self.phi(X)\n",
    "    self.optim.step()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "be97daffee0cb79bf284fb5c73b937ebf75cd7f53ef12b5a2937ebadc10a9be5"
  },
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('.venv': poetry)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
