{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JAX Basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "from jax import numpy as jnp\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# cool feature no. 1\n",
    "\n",
    "x = np.array([1.0, 2.0])[None].T\n",
    "y = jnp.array([1.0, 2.0])[None].T\n",
    "\n",
    "# numpy arrays are transformed to JAX tensors automatically at operation execution (no to_tensor, from_numpy mumbo-jumbo)\n",
    "print(\"Dot product:\", x.T@y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# jax arrays are immutable\n",
    "\n",
    "#y[0] = 3.0\n",
    "y.at[0].set(3.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can do this, by the following syntax (first uglyness)\n",
    "\n",
    "y.at[0].set(1.0) # returns a new array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The JIT\n",
    "\n",
    "Stands for \"Just in Time Compilation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "M = np.random.uniform(-0.1, 0.1, (500, 500))\n",
    "M_jax = jnp.array(M)\n",
    "\n",
    "def numpy_function(M):\n",
    "    return M @ M.T\n",
    "\n",
    "def function(M):\n",
    "    return M @ M.T\n",
    "\n",
    "@jax.jit\n",
    "def jit_function(M):\n",
    "    return  M @ M.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Numpy function:\")\n",
    "%timeit  numpy_function(M)\n",
    "print(\"JAX function:\")\n",
    "%timeit  function(M_jax)\n",
    "print(\"JAX jit function:\")\n",
    "%timeit  jit_function(M_jax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating Gradients\n",
    "\n",
    "Using the `jax.grad` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "from matplotlib import pyplot as plt\n",
    "# n-th order polynomial\n",
    "\n",
    "n = 6\n",
    "roots = np.random.uniform(-3, 3, n).astype(np.float64)\n",
    "def nth_order_polynomial(x, roots):\n",
    "    y = 1\n",
    "    for r in roots:\n",
    "        y = y*(x-r)\n",
    "    return y\n",
    "\n",
    "\n",
    "poly = jax.jit(functools.partial(nth_order_polynomial, roots=roots))\n",
    "\n",
    "x = jnp.linspace(-2, 2, 100)\n",
    "\n",
    "y = poly(x)\n",
    "\n",
    "\n",
    "plt.plot(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can trace the computation of the polynomial\n",
    "jax.make_jaxpr(poly)(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n-th order derivatives of this function?\n",
    "grad_func = poly\n",
    "nth_order_grads = []\n",
    "for i in range(n):\n",
    "\n",
    "    grad_func = jax.grad(grad_func)\n",
    "    y = jax.vmap(grad_func)(x)\n",
    "    plt.plot(x, y, label=f'n={i+1}')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JAX Can Differentiate with Respect to (almost) Anything\n",
    "\n",
    "As long as we register the type. But standard Python containers are supported out of the box.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def square(data):\n",
    "    return data['x']**2 + data['y']**2\n",
    "\n",
    "g = jax.grad(square)({\"x\": 1.0, \"y\": 2.0})\n",
    "\n",
    "print(f\"Gradient 1: {g}\")\n",
    "\n",
    "\n",
    "def square(data):\n",
    "    return data[0]**2 + data[1] ** 2\n",
    "\n",
    "g = jax.grad(square)([1.0, 2.0])\n",
    "print(f\"Gradient 2: {g}\")\n",
    "\n",
    "\n",
    "from dataclasses import dataclass\n",
    "@dataclass\n",
    "class Point:\n",
    "    x: float\n",
    "    y: float\n",
    "\n",
    "from jax.tree_util import register_pytree_node\n",
    "\n",
    "register_pytree_node(\n",
    "    Point, \n",
    "    lambda x: ([x.x, x.y], None), #unpacking\n",
    "    lambda d, x: Point(*x)\n",
    ")\n",
    "\n",
    "def square(data):\n",
    "    return data.x**2 + data.y ** 2\n",
    "\n",
    "g = jax.grad(square)(Point(1.0, 2.0))\n",
    "print(f\"Gradient 3: {g}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset for linear regression\n",
    "\n",
    "\n",
    "def generate_data(d, N):\n",
    "    x = np.random.uniform(-1,1, (N, d))\n",
    "    theta = np.random.uniform(-1,1, (d,1))\n",
    "    y = x @ theta\n",
    "    theta_ = np.random.uniform(-1,1, (d,1))\n",
    "    return x, y, theta, theta_\n",
    "\n",
    "x, y, theta, theta_ = generate_data(20, 500)\n",
    "\n",
    "\n",
    "\n",
    "def vis_lines(theta, theta_):\n",
    "    x = np.linspace(-2, 2, 100)\n",
    "    y = theta[0, 0]*x + theta[1,0]\n",
    "    y_ = theta_[0,0]*x+ theta_[1,0]\n",
    "    plt.plot(x, y, label='gt')\n",
    "    plt.plot(x, y_, label='pred')\n",
    "    plt.legend()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def predict(x, theta):\n",
    "    return x@theta\n",
    "\n",
    "def mse(x, theta, y):\n",
    "    y_ = predict(x, theta)\n",
    "    return ((y-y_)**2).mean()\n",
    "\n",
    "grad_func = jax.grad(mse, argnums=[1]) # returns a function which takes the same arguments as the wrapped one\n",
    "\n",
    "\n",
    "# returns tuple of gradients with respect to arguments of function\n",
    "grad, = grad_func(x, theta, y)\n",
    "\n",
    "# for practicality, this is also available\n",
    "loss, (grad,) = jax.value_and_grad(mse, argnums=[1])(x,theta_, y)\n",
    "print(f\"MSE loss: {loss}\")\n",
    "\n",
    "\n",
    "\n",
    "vis_lines(theta, theta_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stupid loop to optimize our model\n",
    "for _ in range(50):\n",
    "    loss, (grad,) = jax.value_and_grad(mse, argnums=[1])(x,theta_, y)\n",
    "    theta_ -= 0.01*grad \n",
    "\n",
    "print(f\"MSE: {loss}\")\n",
    "vis_lines(theta, theta_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what if we want to get a Jacobian?\n",
    "J, = jax.jacobian(predict, argnums=[0])(x[0], theta)\n",
    "J.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what about the Hessian?\n",
    "H, = jax.jacfwd(jax.jacrev(predict), argnums=[0])(x[0], theta)\n",
    "H.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2nd Order Optimization!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "d = 10\n",
    "\n",
    "@jax.jit\n",
    "def square(x):\n",
    "    return (x**2).sum(-1)\n",
    "\n",
    "\n",
    "x = np.linspace(-10, 10, 100)[:, None]\n",
    "plt.plot(x, square(x))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = jnp.array([10.0]*d)\n",
    "# how does the hessian look like?\n",
    "jax.hessian(square)(x0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets try to minimize it via gradient descent\n",
    "\n",
    "f = square\n",
    "x_ = x0\n",
    "trajx = []\n",
    "trajy = []\n",
    "\n",
    "err = 1e-3\n",
    "for i in range(200):\n",
    "    g = jax.grad(f)(x_)\n",
    "    x_ -= 0.01*g\n",
    "    trajx.append(x_[0])\n",
    "    trajy.append(f(x_))\n",
    "    if trajy[-1] < err:\n",
    "        break\n",
    "print(f\"Converged in {i} steps, {err}, {x_}\")\n",
    "plt.plot(x, f(x))\n",
    "plt.plot(trajx, trajy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets try to minimize it via gradient descent\n",
    "\n",
    "f = square\n",
    "x_ = x0\n",
    "trajx = [x0[0]]\n",
    "trajy = [f(x0)]\n",
    "f = square\n",
    "\n",
    "\n",
    "\n",
    "err = 1e-6\n",
    "for i in range(100):\n",
    "    g = jax.grad(f)(x_)[:, None]\n",
    "    H = jax.hessian(f)(x_)\n",
    "    x_ -= (jnp.linalg.inv(H) @ g).flatten()\n",
    "    trajx.append(x_[0])\n",
    "    trajy.append(f(x_))\n",
    "    if trajy[-1] < err:\n",
    "        break\n",
    "print(f\"Converged in {i} steps, {err}, {x_}\")\n",
    "plt.plot(x, f(x))\n",
    "plt.plot(trajx, trajy)\n",
    "plt.scatter(trajx, trajy, color='red')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorization / Parallelization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vmap usage\n",
    "# say we have a \"complicated function\" that we want to apply row-wise, ie. over axis=0\n",
    "\n",
    "M = np.random.uniform(1, 10, (200, 2))\n",
    "func = lambda x: x[0]**2 + jnp.exp(x[1])\n",
    "\n",
    "@jax.jit\n",
    "def naive(M):\n",
    "    return jnp.stack([func(x) for x in M])\n",
    "\n",
    "@jax.jit\n",
    "def with_vmap(M):\n",
    "    return jax.vmap(func)(M)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def with_pmap(M): # this is just for the sake of example, it goes across devices\n",
    "    return jax.pmap(func)(M)\n",
    "\n",
    "\n",
    "print(\"Naive:\")\n",
    "%timeit -n 50 naive(M)\n",
    "print(\"With vmap:\")\n",
    "%timeit -n 50 with_vmap(M)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note on Gradient Calculation\n",
    "\n",
    "<p style=\"font-size:20px\">\n",
    "\n",
    "Some options:\n",
    "* finite differences $\\frac{df}{dx} = \\lim_{h \\mapsto 0} \\frac{f(x+h)-f(x)}{h}$\n",
    "* symbolic\n",
    "* automatic differentiation - most of deep learning\n",
    "<p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def square(x):\n",
    "    return x**2\n",
    "\n",
    "def finite_differences(f, h):\n",
    "    def func(x):\n",
    "        return  (f(x+h)-f(x))/h\n",
    "    return func\n",
    "\n",
    "\n",
    "finite_differences(square, 1e-5)(2.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autodiff Forward vs. Reverse Mode\n",
    "\n",
    "<p style=\"font-size:16px\">\n",
    "\n",
    "**Forward Mode**: augments the outputs of the forward pass with their derivatives in a (primal, tangent) tuple $(x, \\dot x)$. This is preferred in the case where the number of inputs is much smaller than the number of outputs, in practice we compute a Jacobian-vector product.\n",
    "\n",
    "\n",
    "**Reverse Mode**: comes in two stages. First we make a forward pass through our computation graph which is followed by computation of partial derivatives with respect to intermediate variables (adjoints). Backpropagation is a special case of reverse mode autodiff. Vector-Jacobian product.\n",
    "\n",
    "**Reverse on Forward**: hybrid, for example computing Hessian.\n",
    "\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# back to our Hessian example, let's time it with different ways of computing the gradient\n",
    "print(\"Only reverse mode autodiff:\")\n",
    "%timeit jax.jacrev(jax.jacrev(predict), argnums=[0])(x[0], theta)\n",
    "print(\"Hybrid autodiff:\")\n",
    "%timeit jax.jacrev(jax.jacfwd(predict), argnums=[0])(x[0], theta)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JAX Random Numbers\n",
    "\n",
    "Random numbers in JAX are annoying. There is no stateful random number generator, but we need to pass around a `key` that we split with `jax.random.split`. This is also where JAX syntax for distributions and numpy syntax differs considerably."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 123\n",
    "key = jax.random.PRNGKey(random_seed) \n",
    "\n",
    "rngs = jax.random.split(key, 10)\n",
    "\n",
    "\n",
    "print(\"These are 10 random numbers\")\n",
    "print(jnp.array([jax.random.normal(k) for k in rngs]))\n",
    "print(\"These are the same numbers\")\n",
    "print(jnp.array([jax.random.normal(k) for k in rngs]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Here come the neural networks..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# say we use an ensemble neural network (this cause a bit of pain for me and Sebastian to implement in PyTorch)\n",
    "from jax.tree_util import tree_flatten, tree_unflatten\n",
    "\n",
    "\n",
    "\n",
    "def tree_stack(trees):\n",
    "    \"\"\"Takes a list of trees and stacks every corresponding leaf.\n",
    "    For example, given two trees ((a, b), c) and ((a', b'), c'), returns\n",
    "    ((stack(a, a'), stack(b, b')), stack(c, c')).\n",
    "    Useful for turning a list of objects into something you can feed to a\n",
    "    vmapped function.\n",
    "    \"\"\"\n",
    "    leaves_list = []\n",
    "    treedef_list = []\n",
    "    for tree in trees:\n",
    "        leaves, treedef = tree_flatten(tree)\n",
    "        leaves_list.append(leaves)\n",
    "        treedef_list.append(treedef)\n",
    "\n",
    "    grouped_leaves = zip(*leaves_list)\n",
    "    result_leaves = [jnp.stack(l) for l in grouped_leaves]\n",
    "    return treedef_list[0].unflatten(result_leaves)\n",
    "\n",
    "\n",
    "def tree_unstack(tree):\n",
    "    \"\"\"Takes a tree and turns it into a list of trees. Inverse of tree_stack.\n",
    "    For example, given a tree ((a, b), c), where a, b, and c all have first\n",
    "    dimension k, will make k trees\n",
    "    [((a[0], b[0]), c[0]), ..., ((a[k], b[k]), c[k])]\n",
    "    Useful for turning the output of a vmapped function into normal objects.\n",
    "    \"\"\"\n",
    "    leaves, treedef = tree_flatten(tree)\n",
    "    n_trees = leaves[0].shape[0]\n",
    "    new_leaves = [[] for _ in range(n_trees)]\n",
    "    for leaf in leaves:\n",
    "        for i in range(n_trees):\n",
    "            new_leaves[i].append(leaf[i])\n",
    "    new_trees = [treedef.unflatten(l) for l in new_leaves]\n",
    "    return new_trees\n",
    "\n",
    "\n",
    "\n",
    "def get_nn_params():\n",
    "        return [\n",
    "            (np.random.uniform(-1,1, (10, 512)), np.random.uniform(-1,1, (512, 1))),\n",
    "            (np.random.uniform(-1,1, (512, 256)), np.random.uniform(-1,1, (256, 1))),\n",
    "            (np.random.uniform(-1,1, (256, 2)),  np.random.uniform(-1,1, (2,1)))\n",
    "        ]\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def forward(x, theta):\n",
    "    for w, b in theta:\n",
    "        x = jax.nn.relu(x@w) + b.T\n",
    "    return x\n",
    "\n",
    "\n",
    "\n",
    "params = get_nn_params()\n",
    "\n",
    "\n",
    "out = forward(x, params)\n",
    "out.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lots_of_params = [get_nn_params() for _ in range(20)]\n",
    "# how do we parallelize this?\n",
    "\n",
    "stacked_tree = tree_stack(lots_of_params)\n",
    "\n",
    "def seq_ensemble_forward(x, trees):\n",
    "    return jnp.stack([forward(x,tree) for tree in trees])\n",
    "\n",
    "\n",
    "seq_ensemble_forward = jax.jit(seq_ensemble_forward)\n",
    "\n",
    "vmap_ensemble_forward = jax.vmap(forward, in_axes=[None,[(0, 0)]*len(lots_of_params[0])])\n",
    "vmap_ensemble_forward = jax.jit(vmap_ensemble_forward)\n",
    "\n",
    "\n",
    "\n",
    "# seems to still not be better than linear speedup when stacking the matrices into tensors, but there is some improvement (possibly limited by hardware)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what about simple stacking?\n",
    "\n",
    "@jax.jit\n",
    "def stacked_forward(x, theta):\n",
    "    for w, b in theta:\n",
    "        x = jax.nn.relu(x@w) + b.transpose(0,2,1)\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x = np.random.uniform(-1,1, (1024,10))\n",
    "\n",
    "vmap_ensemble_forward(x, stacked_tree)\n",
    "stacked_forward(x, stacked_tree)\n",
    "seq_ensemble_forward(x, lots_of_params)\n",
    "\n",
    "%timeit stacked_forward(x, stacked_tree).block_until_ready() \n",
    "\n",
    "\n",
    "%timeit vmap_ensemble_forward(x, stacked_tree).block_until_ready() \n",
    "\n",
    "\n",
    "%timeit seq_ensemble_forward(x, lots_of_params).block_until_ready() \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# JAX magic functions\n",
    "\n",
    "# dataset\n",
    "x = np.random.uniform(-1,1, (100, 10))\n",
    "theta = np.random.uniform(-1,1, (10,1))\n",
    "theta_ = np.random.uniform(-1,1, (10,1))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "y = x @ theta # ground truth\n",
    "y_ = x @ theta_ # estimates\n",
    "\n",
    "# mse error\n",
    "def mse(y_,y):\n",
    "    pass\n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numpy and JAX syntax is similar in most cases, except one crucial annoying one...\n",
    "\n",
    "# random seed initialization\n",
    "random_seed = 123\n",
    "key = jax.random.PRNGKey(random_seed) # returns "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flax: Making Things More Simple\n",
    "\n",
    "Alternatives: Haiku, Stax, Objax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import flax\n",
    "import jax\n",
    "from jax import numpy as jnp\n",
    "import numpy as np\n",
    "from typing import Optional\n",
    "# for details about this, read about python >=3.7 dataclasses\n",
    "\n",
    "class MLP(flax.linen.Module):\n",
    "    # here, ordering is preserved\n",
    "    num_hidden: int\n",
    "    hidden_size: int\n",
    "    outputs: int\n",
    "    act_function: Optional[str] = 'relu'\n",
    "\n",
    "    def setup(self):\n",
    "        self.layers = [flax.linen.Dense(features=64) for _ in range(self.num_hidden)]\n",
    "        self.last_layer = flax.linen.Dense(self.outputs)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        act_function = getattr(flax.linen, self.act_function)\n",
    "        for layer in self.layers[:-1]:\n",
    "            x = act_function(layer(x))\n",
    "        # don't apply act in last layer\n",
    "        x = self.layers[-1](x)\n",
    "        return x\n",
    "\n",
    "\n",
    "key = jax.random.PRNGKey(0)\n",
    "\n",
    "X = np.random.randn(128, 10)\n",
    "\n",
    "model = MLP(2, 64, 2)\n",
    "\n",
    "# we need to call the init function, takes a batch and key\n",
    "key, _ = jax.random.split(key)\n",
    "params = model.init(key, X)\n",
    "\n",
    "\n",
    "# we need to call the apply function for the forward pass, which also takes the model parameters\n",
    "y_ = model.apply(params, X)\n",
    "y_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import flax\n",
    "from typing import Optional\n",
    "# for details about this, read about python 3.7 datamodule class\n",
    "\n",
    "class MLPCompact(flax.linen.Module):\n",
    "    # here, ordering is preserved\n",
    "    num_hidden: int\n",
    "    hidden_size: int\n",
    "    outputs: int\n",
    "    act_function: Optional[str] = 'relu'\n",
    "    \n",
    "    @flax.linen.compact\n",
    "    def __call__(self, x):\n",
    "        act_function = getattr(flax.linen, self.act_function)\n",
    "        for _ in range(self.num_hidden):\n",
    "            x = flax.linen.Dense(self.hidden_size)(x)\n",
    "            x = act_function(x)\n",
    "        # don't apply act in last layer\n",
    "        x = flax.linen.Dense(self.outputs)(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "key = jax.random.PRNGKey(0)\n",
    "X = np.random.randn(128, 10)\n",
    "\n",
    "model = MLPCompact(2, 64, 2)\n",
    "\n",
    "# we need to call the init function, takes a batch and key\n",
    "params = model.init(key, X)\n",
    "\n",
    "\n",
    "# we need to call the apply function, which also takes the model parameters\n",
    "y_ = model.apply(params, X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VAE Example\n",
    "\n",
    "This is our loss functions (maximizing ELBO)\n",
    "$$\n",
    "\\mathcal{L}(\\theta, \\phi) = -\\mathbb{E}[p_\\phi(x | z)] +  \\mathbb{KL}[q_\\theta(z | x) || p(z)]  $$\n",
    "\n",
    "\n",
    "First term of the loss we call reconstruction loss, second term you can see as some kind of complexity/regularization term.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss functions\n",
    "\n",
    "@jax.vmap\n",
    "def kl_divergence(mean, logvar):\n",
    "  return -0.5 * jnp.sum(1 + logvar - jnp.square(mean) - jnp.exp(logvar))\n",
    "\n",
    "\n",
    "@jax.vmap\n",
    "def binary_cross_entropy_with_logits(logits, labels):\n",
    "  logits = flax.linen.log_sigmoid(logits)\n",
    "  return -jnp.sum(labels * logits + (1. - labels) * jnp.log(-jnp.expm1(logits)))\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def loss(logits, mean, logvar):\n",
    "  reconstruction_loss = binary_cross_entropy_with_logits(logits, image)\n",
    "  kl_div = kl_divergence(mean, logvar)\n",
    "  return jnp.mean(reconstruction_loss + kl_div)\n",
    "\n",
    "from typing import Sequence\n",
    "class Sequential(flax.linen.Module):\n",
    "  layers: Sequence[flax.linen.Module]\n",
    "\n",
    "  def __call__(self, x):\n",
    "    for layer in self.layers:\n",
    "      x = layer(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "class MLPCompact(flax.linen.Module):\n",
    "    # here, ordering is preserved\n",
    "    num_hidden: int\n",
    "    hidden_size: int\n",
    "    outputs: int\n",
    "    act_function: Optional[str] = 'relu'\n",
    "    \n",
    "\n",
    "    def forward(self, x):\n",
    "      act_function = getattr(flax.linen, self.act_function)\n",
    "      for _ in range(self.num_hidden):\n",
    "          x = flax.linen.Dense(self.hidden_size)(x)\n",
    "          x = act_function(x)\n",
    "      # don't apply act in last layer\n",
    "      x = flax.linen.Dense(self.outputs)(x)\n",
    "      return x\n",
    "\n",
    "    @flax.linen.compact\n",
    "    def __call__(self, x):\n",
    "      return self.forward(x)\n",
    "\n",
    "class Decoder(MLPCompact):\n",
    "\n",
    "    def setup(self):\n",
    "        return\n",
    "\n",
    "    @flax.linen.compact\n",
    "    def __call__(self, x):\n",
    "      x = super().forward(x)\n",
    "      return x\n",
    "\n",
    "from numpyro.distributions import Normal\n",
    "\n",
    "class VAE(flax.linen.Module):\n",
    "  latents: int\n",
    "  outputs: int\n",
    "  \n",
    "  def setup(self):\n",
    "    self.encoder = MLPCompact(3, 128, self.latents*2)\n",
    "    self.decoder = Decoder(3, 256, outputs=self.outputs)\n",
    "\n",
    "  def __call__(self, key, x, deterministic=False):\n",
    "\n",
    "    gauss_params = self.encoder(x)\n",
    "    mu, logvar = jnp.split(gauss_params, 2, -1)\n",
    "    sigma = jnp.sqrt(jnp.exp(logvar))\n",
    "\n",
    "    if not deterministic:\n",
    "      gauss_dist = Normal(mu, sigma)\n",
    "      z = gauss_dist.rsample(key)\n",
    "    else:\n",
    "      z = mu      \n",
    "    return self.decoder(z), z, mu, logvar\n",
    "\n",
    "  def generate(self, key, samples):\n",
    "    # sample from prior distribution \n",
    "    mu = jnp.zeros((samples, self.latents))\n",
    "    sigma = jnp.ones((samples, self.latents))\n",
    "    gauss_dist = Normal(mu, sigma)\n",
    "    z = gauss_dist.sample(key)\n",
    "    logits =  self.decoder(z)\n",
    "    return jnp.round(flax.linen.log_sigmoid(logits)).reshape(-1, 28, 28)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#vae = VAE(20, 28*28)\n",
    "#params = vae.init(rk1, rk2, image)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "plt.imshow(imgs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Construct a tf.data.Dataset\n",
    "ds = tfds.as_numpy(tfds.load('binarized_mnist', split='train', batch_size=64, shuffle_files=True))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "from flax.training import train_state\n",
    "import optax\n",
    "\n",
    "# Build your input pipeline\n",
    "#params = None\n",
    "\n",
    "# create optimizer\n",
    "optimizer = optax.adam(1e-3)\n",
    "\n",
    "# this returns a function!\n",
    "def construct_training_step(model, optimizer):\n",
    "  # model forward\n",
    "  def model_loss(params, key, image):\n",
    "    logits, z, mean, logvar = model.apply(params, key, image)\n",
    "    loss = jnp.mean(binary_cross_entropy_with_logits(logits, image) + kl_divergence(mean, logvar))\n",
    "    return loss\n",
    "  \n",
    "  grad_func = jax.value_and_grad(model_loss, argnums=0)\n",
    "  # this is the function that we call in the end\n",
    "  def update_func(params, opt_state, key,  image):\n",
    "    loss, grads = grad_func(params, key, image)\n",
    "    updates, opt_state = optimizer.update(grads, opt_state)\n",
    "    return loss, updates, opt_state\n",
    "\n",
    "  return jax.jit(update_func)\n",
    "\n",
    "for e in range(20):\n",
    "  tqdm_iter = tqdm(enumerate(ds))\n",
    "  for i, batch in tqdm_iter:\n",
    "    image  = batch[\"image\"]\n",
    "    \n",
    "    image = image.reshape(64, -1)\n",
    "    if params is None:\n",
    "      # initialize params, optimizer\n",
    "      rk1, rk2 = jax.random.split(key)\n",
    "      params = vae.init(rk1, rk2, image)\n",
    "      opt_state = optimizer.init(params)\n",
    "      training_step = construct_training_step(vae, optimizer)\n",
    "\n",
    "    key, _ = jax.random.split(key)\n",
    "    logits, z, mean, logvar = vae.apply(params, key, image)\n",
    "\n",
    "    # this is equivalent to  .backward and optimizer.step in PyTorch\n",
    "    loss, updates, opt_state = training_step(params, opt_state, key, image)\n",
    "    params = optax.apply_updates(params, updates)\n",
    "    \n",
    "    tqdm_iter.set_description(f\"Epoch {e:5d}, batch {i:5d}, loss={loss:10.5f}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vae' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_13114/41434178.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# now we sample from p(z)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mimgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'vae' is not defined"
     ]
    }
   ],
   "source": [
    "# now we sample from p(z)\n",
    "key, _ = jax.random.split(key)\n",
    "imgs = vae.apply(params, key, 2, method=vae.generate)\n",
    "plt.imshow(imgs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Meta Learning: MAML\n",
    "\n",
    "In MAML, we concern ourselves with the multi-task setting. So the following objective has two parts to it. The inner loss is the loss for instances from task 1 and the outer loss is calculated on the shifter paramters in task 2.\n",
    "\n",
    "$$\n",
    "    \\mathcal{L}(\\theta - \\nabla \\mathcal{L}(\\theta, x_1, y_1), x_2, y_2)\n",
    "$$\n",
    "\n",
    "It is clear that here we have an optimization step within the loss calculation. Lucky for us, JAX can help us out here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLPCompact(2, 64, 1)\n",
    "params  = mlp.init(key, jnp.ones((64,1)))\n",
    "optimizer = optax.adam(1e-2)\n",
    "opt_state = optimizer.init(params)\n",
    "training_step = construct_training_step(mlp, optimizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "from jax.tree_util import tree_multimap\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "def mse(params, inputs, targets):\n",
    "    # Computes average loss for the batch\n",
    "    predictions = mlp.apply(params, inputs)\n",
    "    return jnp.mean((targets - predictions)**2)\n",
    "\n",
    "def inner_update(p, x1, y1, alpha=.1):\n",
    "    \"\"\"\n",
    "        This is the expression with which we obtain \\theta - grad(inner_loss)\n",
    "    \"\"\"\n",
    "    grads = jax.grad(mse)(p, x1, y1)\n",
    "    inner_sgd_fn = lambda g, state: (state - alpha*g)\n",
    "    return tree_multimap(inner_sgd_fn, grads, p)\n",
    "\n",
    "def maml_loss(p, x1, y1, x2, y2):\n",
    "    \"\"\"\n",
    "        This is the outer update\n",
    "    \"\"\"\n",
    "    p2 = inner_update(p, x1, y1)\n",
    "    return mse(p2, x2, y2)\n",
    "\n",
    "# returns scalar for all tasks.\n",
    "def batch_maml_loss(p, x1_b, y1_b, x2_b, y2_b):\n",
    "    task_losses = jax.vmap(functools.partial(maml_loss, p))(x1_b, y1_b, x2_b, y2_b)\n",
    "    return jnp.mean(task_losses)\n",
    "\n",
    "\n",
    "# this returns a function!\n",
    "def construct_training_step(model, optimizer):\n",
    "  # model forward\n",
    "  grad_func = jax.value_and_grad(batch_maml_loss)\n",
    "  # this is the function that we call in the end\n",
    "  def update_func(params, opt_state, x1_b, y1_b, x2_b, y2_b):\n",
    "    l, grads = grad_func(params, x1_b, y1_b, x2_b, y2_b)\n",
    "    updates, opt_state = optimizer.update(grads, opt_state)\n",
    "    return l, updates, opt_state\n",
    "  return jax.jit(update_func)\n",
    "\n",
    "def sample_tasks(outer_batch_size, inner_batch_size):\n",
    "    # Sample random sinusoid functions\n",
    "    As = []\n",
    "    phases = []\n",
    "    for _ in range(outer_batch_size):        \n",
    "        As.append(np.random.uniform(low=0.1, high=.5))\n",
    "        phases.append(np.random.uniform(low=0., high=np.pi))\n",
    "    def get_batch():\n",
    "        xs, ys = [], []\n",
    "        for A, phase in zip(As, phases):\n",
    "            x = np.random.uniform(low=-5., high=5., size=(inner_batch_size, 1))\n",
    "            y = A * np.sin(x + phase)\n",
    "            xs.append(x)\n",
    "            ys.append(y)\n",
    "        return np.stack(xs), np.stack(ys)\n",
    "    x1, y1 = get_batch()\n",
    "    x2, y2 = get_batch()\n",
    "    return x1, y1, x2, y2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1415.9001\n",
      "1000 0.04489129\n",
      "2000 0.02302628\n",
      "3000 0.037248895\n",
      "4000 0.04379121\n",
      "5000 0.0729846\n",
      "6000 0.10690337\n",
      "7000 0.0094840955\n",
      "8000 0.02084308\n",
      "9000 0.020276764\n",
      "10000 0.021204028\n",
      "11000 0.007675347\n",
      "12000 0.004466174\n",
      "13000 0.019061018\n",
      "14000 0.016666032\n",
      "15000 0.0064548296\n",
      "16000 0.023171611\n",
      "17000 0.005731207\n",
      "18000 0.007888082\n",
      "19000 0.0088955425\n"
     ]
    }
   ],
   "source": [
    "np_batched_maml_loss = []\n",
    "K=20\n",
    "for i in range(20000):\n",
    "    x1_b, y1_b, x2_b, y2_b = sample_tasks(4, K)\n",
    "    l, updates, opt_state = training_step(params, opt_state, x1_b, y1_b, x2_b, y2_b)\n",
    "    np_batched_maml_loss.append(l)\n",
    "    params = optax.apply_updates(params, updates)\n",
    "    if i % 1000 == 0:\n",
    "        print(i, l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f2787f5bd90>"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAD4CAYAAADhNOGaAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAACK4UlEQVR4nOydd3gUVffHP3d303svpBcIpBAg9N6LFEFBsQBiw95791Vff4K+KqLYCwoioIiC9N4JkIRAAoEkpJLe65b5/TEh0glk08h8nmef7M7cuXMmyc6Ze+653yMkSUJBQUFBof2iamkDFBQUFBRaFsURKCgoKLRzFEegoKCg0M5RHIGCgoJCO0dxBAoKCgrtHE1LG3A9ODs7S35+fi1thoKCgkKb4uDBg/mSJLlcuL1NOgI/Pz+io6Nb2gwFBQWFNoUQ4vSltiuhIQUFBYV2juIIFBQUFNo5iiNQUFBQaOcojkBBQUGhnaM4AgUFBYV2jlEcgRDiOyFErhAi/jL7hRDiUyHESSFEnBCi+zn7ZgohkupeM41hj4KCgoJCwzHWiOAHYMwV9o8FguteDwBfAAghHIE3gN5AL+ANIYSDkWxSUFBQUGgARllHIEnSdiGE3xWaTAJ+kmTN671CCHshhAcwBNggSVIhgBBiA7JDWWIMuxqMXgu5CVCSAaWZoKsGj67g2Q3MbJrVFAWFZqc8F84cgdxjoDYFx0BwCgR7X1Ap0eP2QHMtKOsApJ/zOaNu2+W2X4QQ4gHk0QQ+Pj7GsaqyEA5+D/u/hrLsS5xUBe4RMOhZCBkPQhjnvAoKLY1eC7G/wo4PoSjl0m0c/KDf4xB5J5iYN6t5Cs1Lm1lZLEnSV8BXAFFRUY2rpmPQw7b/g12fgq4KAobCyP+AUwDYeoFKA1mHITMa4lfA0rvAqyeMfBt8+xnjchQUWgZJgiPLYMu7UJQKnt2h94PgFgauXcCgg8JTkJcIh3+G1U/L35UhL0KPe5SHoRuU5nIEmYD3OZ+96rZlIoeHzt2+tUktKcuBFfdC6g4IuxUGPgNuXS5uFzxCfg18FmIXw5b34PuxMPQVGPSc8oVQaHvoamHNM3DoJ3mkO30pdBx98f+yjZv8wNPjHkjZDts+gL+fgrR9MP5/YGrZMvYrNBnNFQBcBcyoyx7qA5RIkpQNrANGCSEc6iaJR9VtaxpSd8KXAyEjGm7+Am799tJO4FzUGug+Ax47BBG3y09SKx+Wv1QKCm2Finz4aZLsBAY+Aw9sg05jrvxAIwQEDIaZf8kPQHFL4bvRUHRJuRqFNoxRRgRCiCXIT/bOQogM5EwgEwBJkhYCa4BxwEmgErinbl+hEOI/wIG6rt4+O3FsdCRJjoea2cDdf4Bb6LUdb2oJkxeCYwBsfQ9K0mH6EmUyWaH1U5Ipj2bLc2DKNxAx9dqOV6lg8PNyAsWK++HbUXDvenDwbRp7FZod0RaL10dFRUnXpT5aUQAa08bfvON+gz/mQOAwmP6rPGpQUGiNVJfAd2OhOA1mrASvqMb1l5sA340BK2eYvU7+qdBmEEIclCTpon+C9pUbZuVknCf4iGlw04dwcgOse6nx/SkoNAW6Wlh6N+Qfh9t+arwTAHDtDHcslVOtF0+D2orG96nQ4rQvR2BMou6Bfo/B/q9g78KWtkZB4XwkCf56HFK2wcT58ujVWPj0gVu/lzPrls0Cg8F4fSu0CIojaAwj3pbXF6x7CZK3trQ1Cgr/cvB7iF0CQ16CyDuM33/IOBj7ASSth72fG79/hWZFcQSNQaWCKV+BUxCsfESOxyootDQFp2DdK/L6mEHPN915et4nPwhtektemazQZlEcQWMxtYKbF0JZFqx9uaWtUWjv6HXw+wOyVMTNnzetRIQQMOFTsHCEFfeBtqrpzqXQpCiOwBh49YABT0PMz3D8n5a2RqE9s/MjeUX8+I/A1rPpz2flJDucvETY8HrTn0+hSVAcgbEY/IK8TH/V43KaqoJCc3PmiCwHET4Vwm5pvvMGDYc+D8uJE+n7m++8CkZDcQTGQmMqLzirKoTN/2lpaxTaG5IE/7wAZrbyJG5zM/QVsPGENc/JWl4KbQrFERgT93DoeT8c+hHOXLJGj4JC03BsJZzeBcNfA0vH5j+/mTWM+g9kx8hidQptCsURGJvBz4O5Hax7WX5KU1BoarRVsP41OTTZvQWL/IXdAj795CyiqqKWs0PhmlEcgbGxdJSHySnblIljheZh16ey9tWY90Glbjk7hIBxH8hOYMt/W84OhWtGcQRNQY97wCUE1r+iqJQqNC0lmbDzf9BlEvgPbGlr5PBo1Gw48DXkJ7W0NQoNpF05gvitGzm87m8qS5t44ZdaA6PfhcJkiP62ac+l0L7ZPlcuJjOyFSUoDH4RNOZyHQOFNkG7cgQnD+xl83cL+XLODH5//00Sdm5FW13dNCcLGgF+A+WnNWWhjUJTUJwmT8x2n3FFSejSgiqSonNIjcunILOc2ipd09pl7QK9HpAroeUdb9pzKRiF9iVDDeSlpZKwcyuJO7dRVpCHiZk5QT370HnAEHzCI1FrjCgpnboTfrhJTufr/aDx+lVQAPjrCYhZDI8fBjuv+s1lhdVknigi80QxmceLKM0rwKDPQQhzhMoWhBVeIY70GO2LV2cHRFNU26sogE8iIHgUTP3e+P0rXBeXk6Fud47gLJLBQGbiMRJ2buXE3p1UV5RjYWtHp74D6DxgCB7BIcb5gnx/k1wD9vEYpQC4gvEoToNPu0GPWZT3e6/uxl9ExvFCSs6kYdBlI0lZ6A0ZUFt+3qEqtQkm5p2Q1L1w8/cmaqwf/pHOxncIm/4jF4N6aNe1F4JSaBKa1BEIIcYAnwBq4BtJkt6/YP//gKF1Hy0BV0mS7Ov26YGzilVpkiRNvNr5jOEIzkWn1ZIac5CEXdtIjt6HTluLnasbIf2H0HnAEJy8vK/eyeVI3gY/TYRx86DX/UazWaH9YtAbOPXNXDKOl5BuOoTivDQMuiwM+iwMumyEJC/oqjTTUeCoQ9XBAXs/b1LzT1GWm4tDmSnBWTaoDAJz2wgkqSeu/h3oNcEf3zAn4zmEykL4pCsEDIHbFhmnT4VG0WSOQAihBk4AI4EM5LKT0yVJOnaZ9o8B3SRJml33uVySJOtrOaexHcG51FRWcvLAHhJ2biXtSCySZMDVL5DOAwbTqf8gbByvsSKTJMllAovT5CG8xqxJ7FZoP/y9cA0nd21Hr88AfTEABiSK7XQYPG1wCPAlqEsPwgN6EmAfgFqlRqqtBbWa9IpM1qWuY9nhX/A6ZqBzmi1q1FjYRqGXeuAR5Ea/KYF4BNkbx9gt/4Vt78NDu5VRQSugKR1BX+BNSZJG131+CUCSpEsmEgshdgNvSJK0oe5zq3IE51JRXMTx3dtJ2LmVM6eSQAi8u4TTecAQgnv3w9yqgWaf2gKLbobx/5NT6xQUrpO9cYfZ/X/foNenUd5BjbW/F74hEURGDCLQORh13ToCfUkJlQcPURkdTeWBA1QfOwZ6PcLcHJWVFRYDB7BvuAc/ZP2FV6yOoExrNGYWmFn1R28IIyDSlb6TA3Fwt2qcwZWF8L9QCJ0si9MptChN6QhuBcZIknRf3ee7gd6SJD16iba+wF7AS5Lk8asQQgfEADrgfUmSVl7mPA8ADwD4+Pj0OH36dKPsvlYKszJJ3LWVxF3bKMrOQq3R4N+tJ50HDiGgW080pqaXP1iS4JvhUFUMj0Y3rTSwwg3LtvRtbH9/J5b5e+gcZsO415bU79MVFFAZfbD+xl9z/DhIEsLEBPOuEVh2644wM8NQVYk+P5/SdeuRamqwHD6M6DF+/JC5mo6xKjwLLDC3c0Ooh4DwImxQB3qO98PC+gr/31djzXMQ/T08eQRsPRr/i1C4blqLI3gB2Qk8ds62DpIkZQohAoDNwHBJkk5d6ZzNNSK4FJIkkXMqSc482r2dypJiTC0sCe7djy4Dh+LVJQzVpVZ3HlkOK+6F6Uuh05jmN1yhTbM2ZS0/Lf2DHrFlWKpLuefZp6nJqKLywAEqo6OpPSV/ZYS5ORbdIrHs2RPLqCgsunZFZXZxOFJXWEjhokUU/bIYQ2kpFiOHsXWUO38eX0dkvDXWVRrsPMKpqeqDqaUDUeP8iBjihdrkOh5iCpNhfg/o/ySMeKORvwmFxtAqQkNCiMPAI5Ik7b5MXz8Af0uStPxK52xJR3AuBr2etKNxJO7cStL+3dRWVWHt4EinfoPoPGAIrv6B/0686bXyxJlTEMxc1bKGK7QpYvNieXjV49yycyhSZTRjHSsRW7IBUFlbY9G92783/tBQxJVGpxegLyuj8IcfKfzhBwyVlZiNG8WKgRriDu4j/KQtarUJzh2GU1Ycgp2LFX0nBxHY3eXaJ5SX3i3Lrjx1TBaoU2gRmtIRaJAni4cDmciTxXdIknT0gnYhwFrAX6o7qRDCAaiUJKlGCOEM7AEmXW6i+SytxRGci7a2huSDB0jYuYWUwwcx6HU4enrRecAQQgYMwd7NHXZ+DBvfgDm7wD2spU1WaANkl2czffV0+sWNwunkPvwtSuhyuAiLyG64Pv8c5iEhCHXj9YV0RUUUfPMNRYvqlEOn3sSX/hlI+zLwzrXE1NEJG9ubqChxxiPQjv63BuPmb9vwE6QfgG9HKGtqWpimTh8dB3yMnD76nSRJ7woh3gaiJUlaVdfmTcBckqQXzzmuH/AlYEBe5fyxJElX1WRojY7gXKrKyzixZycJO7eSmSj7Q4/gTnTu3ZtOh57BsutEuHlBC1up0Nqp1FYy458ZlOdoGb3NH3TJjM1PRRSbEfDXKkw8jB9v12ZlkffpfEr+/BO1nR3Fd47mC+kAfof0WFdpcAyMRF/dn5pKM4J7utF3ciA2jg1cH/PtKCg7I2fPtaQ4XjtGWVDWQpTm55K4S848yk9LRQjwsyqm8/TnCBowGhNzZZGZwsUYJANPb32aLelbeDz5BYoTFhMu8vGOKcHjvfewnzK5Sc9fnZBAzv99QOXevZj4+3FkandWJ++h4ykLVBoNPqHjKcgIRAgVkcO96T7GF1Pzq6zKP7YKfrsbbvsFOo9vUvsVLo3iCFoBeWmpJG74g4TNqynTmcvyFr360qVO3kJlhCG+wo3B5zGf80XsFzzl9irFizajkXIZdiQRu34D8Vr4ZdPIQlyAJEmUb9lK7gcfUJuaimnvKP4aZE/6oWN45lugcXHAx/92spIssLA1pfcEfzr380ClvsyEsl4ny064dIK7/2hy+xUuRnEErQhp0S1knjxFgudsju/bRU1FBVb2DoT0H0S3MROxc3VraRMVWpCNpzfy1NanmBQwiaC/O1OQuoSuZTl4Z9cQsHYjJm6uzWqPpNVStORX8hYswFBWhnTTMH7yzMMyphirag0OYeHYaMaSe1qHo6cVfScHXn6F8tb/g63vwWOHwCmwWa9D4fKOQElobwFEz9l4iRRGDg5kzpc/M/GZl/EIDuHw2tWsnPsf2qJzVjAOJ4pO8PLOl4lwjuA2cT+F6RuwMLXAI7kc53tnNLsTABAmJjjOuJugdWtxvPsuxD9buWdREiM6hZHW0UDB0ThSEubhGZmMrlbH6gVx/PlxDLmnSy/urPsMEGo4+EOzX4fC5VEcQUsQPFou9B39PRoTE4J79WPSs68wdNYD5Kelknc6paUtVGgBymvLeWrLU1ibWDOv/4fs/vUfJH0+oVmpmNmb4PDA4y1qn9reHreXXiLw77+w7NcXh59W8+jOavx7BlJoXUPylpWcKZlPx4E6CjLLWfbfaNZ/E09JXuW/ndh6QMg4WT5b20QS8ArXjOIIWgK1BrrfDSc3QtG/K6Q79umPSq0mcde2FjROoSWQJInXd79OZnkm8wbPI31nGeX527CzsMElsxzXB6ahuob1AU2JqZ8f3p99hs+PP6J2sCf06w3MzrPGpJsr1SXFxK36lCr7pYQOticlLp/Fb+5j+9ITVJbWVeuLuheqCuHYny17IQr1tCtHkHmiiKykInS1+pY2BbrdLdd4PfRT/SZLWzv8unYncdd2JIOhBY1TaG4WJy5mw+kNPNH9CUIswti/ciVIZXRJSsLKXcLm7uda2sSLsOrdC/9ly/B49104k8vIH/Yw2cqTiiBzKuNT2Lv6NSzC99Cptyvx2zL5+bU9HFidQq1Hf3AMgOjvWvoSFOowYhWW1s+Bv1PIPFGMSi1w8bHBI9AOj0B73APtsLRt5qcte28IGgmHF8GQF0FtAkBI/8EkHzpA5vFjeHVWFp21B47kHWFe9DyGeA1hZuhMtvwUS035XlytHHAoOInrC6MRJq1TtVao1djfMgXbMaMp+O57Cr77jqk6HQVj+rGhOoO8jbtIs9tD5PjpaLLD2P9XCvHbMunZ+Rk6pz6OOueookraCmhXWUPV5Vqyk0s4c6qYrKQSctNKMejk67dztcAjyL7OOdhh72bZ9Cl6x/+BJbfDtEXQRS7DUFtdxRcP3EXooGGMuO+Rpj2/QotTqa3kllW3YJAM/DbhNwxFGn56YS666oMMPJmOl2sRnou3gYNfS5vaILQ5ueTN/5SS3/9AWFlxYkRXYs9kYlmpotLPkmHjHifngAXZJ0uwV2fRp3seAbOfbJZ0WAUlffSS6LR68k6XkX2qhOxTJZw5VUJ1hRYAc2uT+hGDR5AdLj42qDVGjqTV51WHwN2/129e/elcUmMPMefLn1BrTIx7ToVWxTt73+G347/x3ejviHKPYtXH20na8yE+ZlaEHzpM4MMBmDy8uqXNvGaqT5wg78OPKN+2Ddzc2Nvbh9yMfIQkMER5MbrvUySuOEphhQNufjb0nRJEh44OLW32Dc/lHEG7Cg1diMZELY8C6opwSJJEcU4l2SdLyD5VTPbJElJi8wFQm6hw9bWpHzW4B9hhbtXIm7RaA11vlwvcl2bXS/SG9B9M4q5tpMYeJrBHr8adQ6HVsjtrN0uPL2VGlxlEuUeRn1FO8sHVCJUg8HAczp1LMRkwq6XNvC7MO3bE+8uFVOzfT+7cefRZdQBtx0C2+lmi3Z/J33FP4dDXmwHJecTmP8LKjw7jG+ZE38mBOHVQROmam3Y9ImgIlaW1slM4VUL2yRLy08owGOTfmaOnVX0oySPIHhsn82sf4uafhM96wIi3YMCTAOh1WhY+OAPfiG6Mf+J5I1+RQmugrLaMyX9OxtLEkt/G/4a5xrxuNDAPP4OaiLQUAscXoHrhOJhatrS5jUKSJMrWriX3fx+jTUujqFsXdlnrUJXqKbSvJjzUnKCO8zm8Lp3aah0hfdzpNSGg4RpGCg1GGRFcJ5a2pgR2cyWwm7yQR1urJzeltN45JB3I4eiOLLmtnakcSgq0wyPIDmcv68svtz+LcxB49YLYJdD/CRACtcaEwKjenDqwF8lgQCiFbG44Poz+kPyqfBYNWYS5xlweDRxahxDgn5iEa+8yVF0nt3knACCEwHbsWGyGD6fot2WoP/+cmwoLOd2vK0fKDWTskjiY8zhjZj2OeZIfcVszOHEgh7CBHegx1q/5EznaIYojuEZMTNV06ORAh05yPNNgkCjMKq8LJ8khpVOHcgHQmKlx97fFvW7U4O5vh6nFJX7lkdPh76cg6zB06A6Ad5dwjm7dSH5GGi4+fs11eQrNQHpZOitPrmR6yHTCXcIB2PNHLPqaeLyq9Tj4uGHbIR0i72xhS42LMDXF8a47sbt5EoXffYfq+x/ooNdztKMlnHJk3wcfUdLTkdufeIHivSYc2ZbJsV1Z9J0cRPiQDsqEchOiOIJGolIJnL1scPayIXyIFwBlhdWcOVVC9slispNLOLgmFUmSlw04eVmfN2qwdjCH0Cnwz4vyqKDOEXh1llPqMhLiFUdwg/FD/A+ohIp7wu4BoCCznFMH1gMGAlPScLnZGeEUCN435vyQ2toal8cfx/7228n/bAGRy34jyKqCgxHBmO0pYcWRZ1GPCOGO554iaU0pO5aeoCCznEG3dzR+woYCoDiCJsHG0RwbR3OCe8ricbVVOnLOCScl7MnmyNYMAKwdzWTHYP0UHgdX4zSiGmFqjq2LG9ZOzmQmHKXbaEWy90YhrzKPP07+wc1BN+NqKYcb96w8ir42Do9qPc6dO2ElNkDka/KTww2MiasrHm+/hWMPK3I//ZxBu4+R6+1OjKkt+pXJfLHnPtzHDaCP+20c3ZBF0ZkKxj4YjoWNEioyNoojaAZMLTR4d3HEu4sjAAa9gfyM8voJ6MwTRSSV9AZ64/jWDkY90hcnT2u8QkLJOHYESZKUYfENwo9Hf0Qv6etHA4XZFZzcvwEkLUGnM3GZMwRxRsjZZO0Es1EP4B03j0qnUVhuL8M5OpbTwT6Qb4nhh2h+DdvP9NveI+H3Epa9H81ND0comUVGxijjLCHEGCHEcSHESSHEi5fYP0sIkSeEiKl73XfOvplCiKS610xj2NPaUalVuPra0nWYN2MeCGPW+/25+62eDHVdRFVpDcv/G83RHZl0CAmlvKiQkpwzLW2yghEori7mtxO/MdZ/LN423gAc+PsE+poYXKsNuEVEYlW7HfwHgp1XC1vbjFjYQ8dRWFZuxfeXRfjO/5TOkgnD41Lw1ELwETXL/34Wz9tq0OsMrJh7kNQj+S1t9Q1Fox2BEEINLADGAl2A6UKILpdoulSSpMi61zd1xzoCbwC9gV7AG3V1jNsVQghs3Wzo0t+T2xyfwMPfiq2/HCcnVX7qyUg8epUeFNoCixMXU6Wr4t6wewEoyaskcfdGkKoJTM/CZfooKEyG8GktbGkLED4VynMQqduxGTEC/5Ur8X/vPXoVVROcW4xfhgU7f55Lzcg47FwsWPN5HDEb0xTJdiNhjBFBL+CkJEnJkiTVAr8Ckxp47GhggyRJhZIkFQEbgDFGsKltEn4rViKfCYOO0am3O6didZhb25CREN/Slik0EoNkYNmJZQz2GkywQzAA0WuS0VcfxKFGokPXbliKeFCbQucJLWxtCxA8GszsIG4ZIGsY2U2cSMAfvxMZ3IVuqWdwLzIjZ9GfHO/8O75dndi1/CRbfzmOXq8INDYWYziCDkD6OZ8z6rZdyC1CiDghxHIhhPc1HosQ4gEhRLQQIjovL88IZrdCPCLBKQgRv4zgXm5IenDsEEymMiJo8xwrOEZ+VT6j/UYDcmbZse2bkQzlBGVm4/LwwxC/AoJHyaGS9oaJOXSZAAl/gbaqfrPazg7vL74gbPpd9D2RiUOlGtPlcWwy+YjwUR4c25nFX5/G1EvDKFwfzZWL9RfgJ0lSBPJT/4/X2oEkSV9JkhQlSVKUi4uL0Q1sFQghD5FTd+LpVo1KI9CYe1N8JpvyosKWtk6hEWzL2IZKqBjQYQAAh9anoq08gI0WfLpEYOlUAeU58t+/vRI+FWrL4MTa8zYLtRrXJ58k9P0PGJiai2OVAc/1+fx28kV63O5B9qkSlr8fTdGZihYyvO1jDEeQCXif89mrbls9kiQVSJJUU/fxG6BHQ49td4TdCkiYJK3EI9COqnJnAGVU0MbZlr6Nri5dcTB3QK81cGzbDiRDEUGZZ3B55GE5JGJqAx1Ht7SpLYffQLB2rw8PXYjt6FF0/mkRA0tqcS+rwv+gniVrn6LrLHtqq3Ws+OAg6QnKA9P1YAxHcAAIFkL4CyFMgduBVec2EEJ4nPNxIpBQ934dMEoI4VA3STyqblv7xTlIDhEdWYZ3Z0dKC2zQmJmTkaA4grZKTkUOCYUJDPIaBMDpowVUlRzAzKAmILgzVt0iIGGVLEVuYtHC1rYgKjWE3QJJ66Gq6JJNzDt3JnjZMvrbuROQU4RPioaV37+A123VWNmb8df8WOK3ZTSz4W2fRjsCSZJ0wKPIN/AE4DdJko4KId4WQkysa/a4EOKoECIWeByYVXdsIfAfZGdyAHi7blv7JvxWyDqMt0clQqiwdw8gU5kwbrPsyNwBwBCvIQDEbz+GpM/CNycPl0cflW98NaXy3729EzEVDNorlrHUODnh++MP9O4/hIi0XNzyTNi9YB76QfH4hDqybckJtv96AoMyidxgjDJHIEnSGkmSOkqSFChJ0rt1216XJGlV3fuXJEkKlSSpqyRJQyVJSjzn2O8kSQqqe31vDHvaPKFTAIFzwV+YWWrQmHqRl36a6vLylrZM4TrYlr6NDtYdCLQPpLZax+mYXQAEevpg2bsXxC8HKxfwG9TClrYCPCLBMRCO/nHFZipTUzzeeYceDz1Kn+RsHMrVpH+9nCT3FXQd7sWRrRmsXhBHTZWueexu47Qr4Q7tmTPoCgpafz1guw7g2x/VseV4hThQWeYCkkTm8WMtbZnCNVKtq2Zv9l4GeQ1CCEFKbB7aqmPYVoHHlCmI2go4sR663CzXp2jvCAGhkyFlO1RcedGYEALHGTPo+vF8BmYW4lBpQPfbQTbkzWXAHYFkJBax8qNDraNGeSunXTmCM2++RVL/ARzvGsnJ4SNIvfMuMp9+mpz/+4CCH36gdO1aKg8dRpuVhaRt4XS0sMmQfwLvDjXUVDuiUquVCeM2yP4z+6nWVzPYazAAcZsPIhmK8S0swGbkSEhaB7oq+eanIBM6GSSDPG/SAKwH9Cf0lyUMqVbjWlaF7bZslmx+hgGzfMlPL2fPH6ea2OC2T7t6BHGcNROr/v3R5eagPZOD7swZqo4eRbd5C1J19fmNhUDt7ISJqxsad3dM3FzRuLqhcXfDxN0djasbJm6uqKysmsbYzhNhzXN467ciRHdsXPyUhWVtkO0Z27HQWNDTvSfV5VqyEvciUBEYEorGwQHW/yFnyvj0aWlTWw9uoeAUDPG/Q9TsBh1i5u9P8NJfMX/6GQ4mJwD2LCl6jL69XiFuSwa+YU74hDo1rd1tmHblCKz69MGqz8VfOEmSMJSUoM3JRZdzRg4h5eSizTmD7kwO2rQ0KqOjMZSUXHSsysYGjZsrJm7uspNwc0Pj5l63TXYianv7axeNs3YF3/7Ypi3F1qU/uqoO5CTvQltTjYmZUrmpLSBJEtsyttHXoy+malOORKehqzmOc7kWp9tugpoySNoA3WfKGTMKMmfDQzvmQVkO2Lg16DC1rS2+C7/AYt6HWK/6nXjJmYOV7xDo8SKbfkzg9td7YWGtKJdeinblCC6HEAK1vT1qe3vo1PGy7QxVVehy6kYTOWfqHEeO7DBycqlJSkKXlwcX6J8IU1M0bmedhNt5DsPEzRWNuzsaZ2eE5oI/R+jNsPoZvH3hWLQLBr2e7KQT+IRFGP+XoGB0koqTOFNxhoe6PgRA3OZdIFXJYaERw+HEOtBVy39nhfMJmwLbP5DDQ73ub/BhQqPB7cUX6B0cjOm89zkkXEl3/QGHyrvZ+vNxxs4Jb0Kj2y6KI7gGVBYWmPr5Yernd9k2klaLLj//X4eRm4M2Jwdd9hm0uTlUxcWh25CDVFt7QecqNM7O2E+bhsujj8jbzoaHTGOIl/xACDIS4hVH0EbYkSGnjQ7oMABdrZ68lAOoJQ3+XXugtrWVM2Os3cFbCQtdhGtncAmBoyuvyRGcxf6WKfR0daXgjRc5DZj3SSI5RuJMcgnuAXZGN7etozgCIyNMTDDx8MDEw4PLLQ2SJAl9cXGds5BHE7qcM1RGHyR/wQJsRo7AvFOn+vBQh6IlqFRvYmXvqUwYtyG2Z2yns2NnXC1dSU8oQF+bikuFhP3d4/4NC0XdA0pN6ksTOhm2vg9lZ8DG/ZoPtx44gL5jJ1K4YwN5B1Zi5/ooB9ee5qaHlQepC1H+A1sAIQQaBwfMQ0KwGTIEh9um4fL443jN/xSVjQ25H330b+PQmzEvisXFU4PKxIuspET0OiU3urVTUlNCbF5svbbQyYMJQC1u5cVYDx0Gx9eCvkbJFroSXW4GpCsuLrsank8+RTcTS0y0UFKxlJTYPPIzlPU4F6I4glaE2s4Op/vvo2LbdioPHJA3dp4IQoW3bSo1la7oamrITVHS4Vo7e7L3oJf09bIS6UflkZxPlxDU1lZyWMjGE7xuzLrERsE1BFw6w7GGpZFeCmFiQtePPyMovwR1aQGIdA6tTTWejTcIiiNoZTjedRcaV1dyP/xILrpRFx7yrvobofYEUNJI2wA7MnZgZ2ZHuHM4Br2Boswk1JIGt4GDoaYcTm6U6w4oYaEr03kCpO2G8uuXnjf19qb7vfdjptVRW72JkwdzKc6pNKKRbR/lv7CVobKwwPmRR6iKiaF882Z5Y5dJuFdtwsTcGjNrF6ViWSvHIBnYmbmT/p79UavU5KWXo9NmYVMtpzCTtF4OC3WZePXO2jtdJsqLyxL/blQ3nrfdiVdlFVJVEQZDFofXnzaSgTcG7Wqy+K09bxGTG4OlxhILjcW/LxOL8z9f4WWpsaxvb642R90E+d/2t0yh8PvvyV/wOTbDh0PnCajXPEcH5yJOV8sTxpLBgFCeJlslR/OPUlhdyECvgQCkxKSAoRRXnQFTf39Y/rasLeTTt4UtbQO4hYGDv5xGGnXPdXcjTEyIvP0u0v/6HW3tJhL3dqDneH+sHZQ1OdDOHEEH6w4UVhVSpauiSldFcU0x1fpqqrRV9dt00rVNxJqqTBvkSBrqfDytPTHTmGE/dSq5c+eizczEpEMH8O6Fd+YekrW+aCtjyU8/jYuvfxP9phQaw47MHQgEAzzlieLU2CMA+AX4I3Q1srZQxFRlEVlDEEIeFexZIEtTW1x/SXOfO2bSYckiUjR56E2yidmQzoBpwUY0tu3SrhzBfeH3XbWNVq+lUldZ7xgu+dKe//nC9tW6aoqqi8jSZZ23vUZfc9Xzu1m68e6Ad4kcOoTcuXMp27oVxzvvhM4T8UpeiErzDiDPEyiOoHWyI2MHES4R2JvbIxkk8k4fR0gqPPsPhFObQVvRPusSXy+dJ8GuT+RMq8jp192NytSU0Cm3kLF+LTrdVo7u9KTHWF8sbJTVxu3KETQEE7UJdmo77MyMv+hEb9DLI5A6Z3KhAymrLeObI99w//r7mdllBpN8fCjfctYRjMdx3StYW1tgqLUnI+Eo3cYoN5PWRkFVAfEF8Twa+SgARWcq0VZnYqVVY9O3Dxx+H8ztFMnpa6FDd7DtIIeHGuEIAIJnzSF+xXJSNVnUqnOI25JB74kBRjK07WIURyCEGAN8AqiBbyRJev+C/U8D9wE6IA+YLUnS6bp9euBIXdM0SZJu2Bk0tUqNlcoKKxMrLrfabKTvSD6M/pAfjv2Ie6AbPXbuQ19egdrBD+ERgZcukWPFHcg4Fo8kSdeuYaTQpOzKkmsNDPCSw0Jpx84g6fNw0YGJlyf8ugY6jQON8hTaYISQR1DR38sL8cxsrrsrlZkZwePHkrF9BwbDduK2eNBtpA+mFu37mbjRs41CCDWwABgLdAGmCyG6XNDsMBBVV7x+OfDBOfuqJEmKrHvdsE6goViaWPJa39d4vNvj/OWRjaTVUrFbvrnQZSLeuo1IeFJZWkxRdvsu79wa2ZmxE0dzRzo7dgYg+fARQMLbxxuRugOqS5Sw0PXQeaKcaZW0vtFdhd3/FJ6l5ejKTlNdnkP8duV7ZIy0k17ASUmSkiVJqgV+BSad20CSpC2SJJ1N3N2LXKRe4QpM6zSNZB8zai1MKN+6Td7YeSLeprGoNPKvT1lP0LrQG/Tszt7NgA4DUAkVkiSRfeIYSGAW3pP8A8vRayw5qOnGgdRCDqQWsj+lkH3JBexNLmDPqQJ2n8pn98l8dp3MZ2dSPgdPF5JXViOvKWnP+PSRM60S/mp0VyoLC7yHD0RtMCBJO4nZmNbui9cYYzzUAUg/53MG0PsK7e8F/jnns7kQIho5bPS+JEkrL3WQEOIB4AEAHx+fxtjbJrAzs2Nk0FgOBayi39atcrqoSycs3dxxKa/lTJU1GcfiiRg+pqVNVajjSP4RSmpKsJHCeOWPI8SfKGBIZQbmehPmHBX86biafwzhPPrN4Wvu29pMQxcPW2b192N0qDtqVTsLCarUckgtfgVoq8GkcWmfPR95mbQpY0lXnaJC5JKwO5vwIe33+bRZA2NCiLuAKGDwOZt9JUnKFEIEAJuFEEckSbpIQ0GSpK+ArwCioqLaxePR1I5TWRi4kj5HC6mOi8MiMhI6T8AnZTdnhCfpx44o8wStiMVx60ESLPxHg7VJFqMtLDHosrHXGvhwsjMu20sIGDidn/x6IQQI5L+b/B6o23b2sxCCihodpwsqSC2oZNuJPB7+5RABLlY8PCSIKd06oGpPDqHzBDj0I6Rsg46jG9WV2toatwHdyTySBNIeDq13p8tAT9Tq9rk2xxiOIBPwPuezV9228xBCjABeAQZLklSfRylJUmbdz2QhxFagG6CI6QBdXbpS2j0I/d8nKNuypc4RjMd742McUE+hvPAEpXk52LleuzKjgnGQJIndpwr4fOtJDuu2YKr25ZNp/Rgf4cma+es5gRZ/by/66vaCyoQug6bIWUPXgd4g8U98Ngu2nOLZZbH8Fp3OB7dE4OfcRFXyWhv+g8DURl5l3EhHAND3ibfInjqBTFUipSKXpP05hPT1MIKhbQ9juL8DQLAQwl8IYQrcDpynEiWE6AZ8CUyUJCn3nO0OQgizuvfOQH9AqdBehxCCiZHTSfSC/I1r5Y0ekXg4lWJiKt/8MxIUuYmWoLJWx8rDmYyfv5M7v9lHYm42aosM7o8ax6TIDqhVgsy6ORzffgMg4W8IGHzdTgBArRKyg3l8AB/cGkFCdiljPtnONzuS0RvawSBZYwbBIyFxDRgaH9M3tbPHsWcIwiAhpP0cXHsaQ3v4PV6CRjsCSZJ0wKPAOiAB+E2SpKNCiLeFEGezgOYC1sAyIUSMEOKso+gMRAshYoEtyHMEiiM4h/EB44nraIY4lYY2MxOEQNNlNF7m2QiVhTJh3IwYDBKrYrN4cFE03f+zgSeXxlCl1fP+lHBeuVX+Kg3xltcHlBfVUFWejolejWu4NxSlQMhNRrFDCMG0KG82PDWY/oHOvLM6gakLd3Mytx3IK3ceD5X5kL7PKN31f/pdPErKqCk7QlF2Dikx1y9u15YxyhyBJElrgDUXbHv9nPcjLnPcbkCpHXcFrE2tsR02HDauIX/TOjxmzIaQ8fhs+ZpTak/S4o9cvROFRlNUUcuTS2PYdiIPVxszpkV5MybMnT7+TqhUgue3fyanjTrJaaOZSYUYdJm4SGpMivYBAjoZxxGcxd3OnG9mRvFnTBZv/nWUcZ/u4MkRwTwwMADNjRrrDhoJalN5hOXbr9HdWTm5YhPug0gvAsMBDq71JKCbS7ubd7tB/1tuLMYOnk2WA6T987u8wbcf3janUGm8KM07Q1lhfssaeIMTk17M+Pk72XOqgHduDmPvS8N5e1IY/QKdUamEnDaa9W/aKEDywRMgVeHj6S7HtL16NrgI+7UghODmbh1Y/9QghnVy5YO1x5n8+W4Sz5Qa/VytAnNb8B8s/06NlFLb77l3cS8uo6YsltzUM6QdKzRKv20JxRG0AUKdQkkNd8YyLhldeTmoTXAMjcDKzBZQ5gmakj9jMpm2cA8Ayx/qy119fC/K1IkviKekpoT+nv3rt2XExwHgF9kFsmPlkEYT4mpjzsK7e7Dgju5kFVcxYf5OPtmYhFZvaNLztgidx0PxacgxTljU0d0X8xAXWe7acJBDa9ufRLXiCNoInqMnotFLHPtnMQCi8034WZwGYUr6USU81BR8tzOFJ36NoZuPPX8/NoAIL/tLttuZuROVUNHPUw5VVJbWUl6ajtqgwsOrLkEupGkdwVluivBgw9ODGRvmwf82nmDiZ7uIzyxplnM3G53GAUIODxmJPs/+B/eScmrLDpF5PJusk8VG67stoDiCNsKgsQ9QYQ6n166QNwQNx8cyAZXak7QjiiMwJpIkMXddIm//fYzRoW78OLsXDlaX1wbambGTMOcw7M3tAcg+WYyky8QBU0yyt8jlFp0Cm8l6cLQy5dPp3fjq7h7kl9cwacEu5q07To3uBlk9a+0K3r0hcbXRuvT06YLG3wZJ0iPpD7W7UYHiCNoIVpZ2FET64hKTRnFVEZha4d3JHpVJB0pyM6ksKW5pE28Yfj2QzoItp5jey5vP7+yBucnl6wYUVhdytOBofZF6gJTDyUiGErzdneH0bqNlC10ro0Ld2fjUYG6O7MBnW04yYf5OYtKLW8QWoxNyE+QcgSLj3bB7PPM6rqUVaCuiSY3LJC+9zGh9t3YUR9CG8B0zBbsK2LR2IQAWEcNxNJPj1UoaqXGo1ur5dFMS3X3seW9y+FWlHHZl7kJCYmCHgfXb0mJjAPD1swFJDyHjmtLkK2JnacKH07ry/T09KavWMeXzXfx3TQLV2jY+OjjrXI//c+V210BQcC9EB1N5VKCLaVejAsURtCGCx96GQQXZ6/6SRcg6jiHAKg3QcFoJD10zmcVV/B2XdZ6g26/708guqeaZUZ0alEK4M1NWG+3iJAvuVldoKS1OR0gCH4c0sPEAj25Ndg0NZWgnV9Y9NYjbenrz5fZkxn26g4Oni1rarOvHKRBcQhpdy/hCQp98DpfSCrSV+0k6mE7RmQqj9t9aURxBG0JtZ0d1aABBR4s4cOYAWDnj56NFpfEkNS6upc1rU1TV6pn13X4eXXyYz7fKiibVWj0Ltp6it78j/QKdrtrH2bTRfp796tNGs08WY9BlYi/MMMneIU9stpLa0rbmJvx3SgSL7u1FjdbArQt38+7qY213dNBpnBx6qzReume38JFILhKSpMNQG8vh9WlG67s10zr+QxUajNeYm/HNg793fQeAe/cINCbulOamU1XefmKajeXNVUc5mVdOL39H5q47zpL9afy89zR5ZTU8PbJjg0YDRwuOUlxTfN78wOmYdCR9Pl4O5nJJyhYMC12OgcEurHtqEHf08uHrHSmM+2QH0altMHc+5CY59Ja0wajdBjz6CM6llegr95O4J52ywmqj9t8aURxBG8Nh+EgAarbtIrcyF3XoONwt5H/UTGU9QYP4MyaTpdHpPDQ4kF/u682QTi688scRPtmYxIAgZ3oHXH00APL8gEDUp40CpB6SJaZ9XWtlgTS/gZc7vEWxNtPw7uRwfrmvN7V6A1O/3MObq45SWatradMajmd3sHY3eniof89b0NvXYJBq0VXHErPhxh8VKI6gjWHq54fw86Z7kp4VSSvAKZAg13JAzalDMS1tXqsnNb+CV/6IJ8rXgadHdsREreLzO7vT1dueshodT43s2OC+dmbuJNw5HAdzBwBqqnQUF6WDBH6mMRA8QhZKa8X0D3Jm3ZODmNnXjx92pzLyo+1sOZ579QNbAyoVdBoLJzfJNQqM1a1Q4TZnFo7lVeir93F0RxqVpbVG6781ojiCNojjyNGEpsHfcb+hM+jwj/RCaDxIi4tpadNaNSdyypj+9V7UKsEn07vV6/FYmmpYdG9vfn+4Hz18HRrUV1F1EUfyj9C/w7+rieX5gSxshRlmuhyjaws1FVZmGt6cGMqyOX2xMFVzz/cHeGzJYfLKaq5+cEsTMl4OwaVsN2q3owbMQm9VgcFQTU1FHHGb069+UBtGcQRtEOuhw1AbJDrE57ItfRt2UUOxMHGgND+Tmsr2keVwrexNLuCWL3ajM0gsvr83Hewtzttvbaahu0/DnADA7qzdSEjnzQ+kx2Yg6c/QwcoAKo0smdyG6OnnyOrHB/DUiI6siz/D8A+3smjv6dYtce0/8N8aBUbERG2C9T23Yl9RhVSzl7gtp6mp1Br1HA1FkiQqS2s5k1zC8X1nmqSsZrNWKFMwDhZdI1A7OjIwpYpfj//K8BFf4mnzKyerJNKPHSUoqldLm9iq+DMmk+eWxeHtaMGPs3vh5WDZ6D53Zu7E3syeUKfQ+m3JB2MAA37WeeA3ACzsG32e5sZMo+aJEcHcFOHBayvjeW1lPMsPZvDuzWGEdbj+WgpNhsYMgobDibVgMBg1Q2vCiIdY8s1v6PUWVJUeYddyD4bN6Gy0/s9FrzVQWlBFaX41pflVlORXUZr372dtzb83f2evXjh1sDbq+RVH0AYRajXWQ4bQdd1qPszYQ0rZaUK6OHEyV8WJ3QcUR1CH3iDxwdpEvtyeTC8/R76a0QN7y8tLRTQUg2SoTxtVq+RVx7XVOooLM0ECf+sT0OmeRp+nJQlytWbx/b35MyaLd1YnMPGznczo68fTozpia27S0uadT8hNcGwlZB4E755G69bKxApx52jsvt9DpWY3x3aF0qGTA516X3tFQEmSqCrTUppfVf8qya+uu9lXUV5cA+cMvNQmKswdVAg7LVJQIZX6VEqr0qgpyaJc5YsTiiNQAGyGDaXk998JzTTht+O/8eSAnqh2pJARf+2F0W9EiitreWzJYXYk5XNXHx9eHx+KqcY4T4sJBQkUVheeFxY6k1yCQZeFldBgYaqTJzHbOGclroeGuPLh+uP8uCeV1UeyefWmzkzs6tl6NPuDR4JQw/HVRnUEAJPGPsHKn9ai1ZnjaHWUrYtNcPW1wcH94vKg1/JUD2Buq8HEXsLgUY3BJ5cS3WmKK1IxFGRiVlyJTbIJploTBGpUgD2AJFF6PAE8jKtdZRRHIIQYA3wCqIFvJEl6/4L9ZsBPQA+gALhNkqTUun0vAfcCeuBxSZLWGcOmGx2rfv0QZmZMzvbko5N/8tiUB7ExX01JySlqq6swNbe4eic3KHEZxTz8yyFyS2v4v1vCua2nj1H735G5A+C8tNG0g+kYdFl0MK0E9wiw977c4W0OOwsT3p4Uxq09vHh1ZTxP/BrDsugM3p4USoCLcZ9MrwsLB/DrL8tNjHjTqF07WzhTOa0/rj/FkMcGrFzdWfd1PN1G+lBaUC2/rvBUb+GgRthqEZ0rqNbkUFhzioriVKT8bGwLJKwzNGj0JoAGC0D+1qox01pgVaPFSluFjbkF9s4uOHp4YR8QiFNE4wvyXEijHYEQQg0sAEYCGcABIcSqC0pO3gsUSZIUJIS4Hfg/4DYhRBfkGsehgCewUQjRUZKkNrrUsflQWVpi1bcvnROOUtarlH/SNuHtbkXJKYnUw3F07Nu7pU1sdiRJYtHe07zzdwIuNmb8Nqcvkd72Rj/PrsxdhDqF4mTx73qDlENHAC2+FqchpG2HhS5HhJc9fzzcn8X7TvPBuuOM+XgHcwYH8PDQoCsK8zULnW6CtS9AwSmjK71OnvAMC/dOokO6GdX5S9DWPsDGH+SkDHNbDab2IHlWIwJLKNRkUVBxktq8ZEzzC7FPM8Gy5t+bvaMQOAJgjalWj1VtLVbosbWyxN7JBUcvbxwDO2IV4I+Jjw8aFxdEM6xMN8aIoBdwUpKkZAAhxK/AJM4vQj8JeLPu/XLgMyGPKycBv0qSVAOkCCFO1vW3xwh23fBYDxtK+dat9K/249fjv/JhvynEn4ojcfOmducIiitrefmPI6w5cobhIa58OK2rUeYDLqSkpoS4/DjuD7+/fpuuVk9RUTYA/s6FN0RY6HKoVYK7+/oxOsyd91Yn8Onmk6yMyeKtSaEM7eTacoZ1Gis7guNroN9jRu3a28abx/67hnUv3E9BrhZd+XxO9nLEkHMauyId9tkmWNSaoNKbYKHS4CUEIAAnNDo9Vlod1mo1dja22Lu44ejtg3OnEKwCgjD16oDK6uIwU3NjDEfQATg3yTYDuPAuVN9GkiSdEKIEcKrbvveCYztc6iRCiAeABwB8fIw71G+rWA8ZAsBteQE8brGdwiEdUKtdyDp1omUNa2Z2JOXx7LJYCspreXFsCA8MDLioipix2JO1B4NkuGh+QK/LxBwVNi5ucmjoBsfVxpyPb+/GtChvXv0znnu+P8CoLm48MjSIrk0wCrsqDr7gFgaJxncEAO5W7syYv4q/H7ybEyXFBO0uoS5qj1pvwEpvwMbUDDtbBxzc3HDw9sM5pAs2HTti4ubWLE/1jaHNTBZLkvQV8BVAVFRUK05sbj5MXF0x7xqBT+wZrAKtWJr2D51srCkoTm0X8wTVWj3v/5PID7tTCXK15tuZPZs8xXFn5k5sTW0Jdw6v35Z2IAWDLhNPVbH8ZNpaJlGbgX5BzvzzxEC+3p7Ml9uSWX8sh15+jtzT34+BHV2wNmvGW0yncbBjHlQUgFXDZEKuBSEEN332HR4fz0Ov1eHo549jSGfsQzqjboan+tyyalYezuS+AcZ/0DHGXykTOHdmzKtu26XaZAghNIAd8qRxQ45VuAI2Q4eR9/HHTL13Cr+k/sOQwKkUHEzm+MaNhI+f0NLmNRkx6cU8/VsMyXkVzOrnx4tjQ5o8Tm2QDOzK2nVe2ihA6uETIFXjZ5dbV0axfWGmUfPosGBm9vNj6YF0vt+VykO/HEKtEnT1smNgsAu39vDC27Hx6zeuSMg42P6BvKag251NcgqVqSlRz7/cJH1fjuS8cr7ekcKKQxlo9QZ6+zsZfdRljPHKASBYCOEvhDBFnvxddUGbVcDMuve3ApslWQR+FXC7EMJMCOEPBAP7jWBTu8Fm+DAAJpzxQGvQkhZhCghO7NzVsoY1ETU6PR+tP84tX+ymqlbPz/f25s2Joc0yWZlUlER+Vf55shI6rZ78oiwAfN118kKydoqNuQn3DQxg23NDWHxfb+YMDsAgwfzNSQyeu4X7foxm18n8pjPAIxJsO8jzBDcAh9OKeOjngwz/aBsrDmVwS3cvNj8zpElCb40eEdTF/B8F1iGnj34nSdJRIcTbQLQkSauAb4FFdZPBhcjOgrp2vyFPLOuAR5SMoWvDNCgIEx8fTPfE0fOWniwvW89NGjdyss60tGlG53BaES+siONETjlTunfgjQmh2Fk03+KmYwVy/kOkS2T9tpzkEvS6bEwlcAgbBOpWttiqBdCoVfQLcqZfkDPPjYas4ioW70tjyf40Nibk8NbEUGb28zP+iYWQQ3Mxi0FbBSZtLzQqSRJbj+excNsp9qUUYmuu4aHBgczq74erjXmTndcoATxJktYAay7Y9vo576uBqZc59l3gXWPY0R4RQmAzdChFixdz+5P/4ZkDr2Lm2Iny3FSqc7Mwd/VsaRMbTWWtjo/Wn+C7XSm42Zrz3awohoW4Nbsdx4uOY6GxwNvm32jm6T0nMWgz8FCXIjrfmGmjjcXT3oJnR3fiseFBPPLLId7++xiBLtYMCHY2/sk6jYMD30Dy1jaVvVWrM/BXbBZfbU/meE4ZHnbmvHpTZ27v5dMs8yyteypboUFYDx+GpNXSM80EVwtX0jpoAQPxf/7e0qY1mk0JOYz8aDvf7Ezh9l4+rH9qUIs4AYDEwkSCHYLPmx9Ii0sGqRw/+0IIalsic82NmUbNx7d3I8jFmod/OUhKfhMIJPoNBDNbSFxt/L6bgPIaHd/sSGbw3C08sywWgI+mdWX780O5b2BAs022K47gBsCye3fUdnZUbdnGrR1vZYODPM1yMi6phS27frJLqpiz6CD3/hiNpamaZXP68t7kcGxaSOdGkiROFJ4gxCGkfpteZyCvKAcA37AgMLdtEdvaEtZmGr6ZGYVaJbj3xwOUVBlZ0VNjCkEj6kToWm+UObesmg/WJtL3v5t4Z3UCvk6WfD+rJ2ufHMiU7l6YqJv31qw4ghsAodFgPWQw5Vu3MSVgEloLA5jak1dUJsdK2xA6vYFvd6Yw4sNtbDmey3OjO7H68YH09HNsUbuyKrIo05bRybFT/bac5CL0umw0Ejj3bBu1B1oD3o6WLLyrB+mFlTy25DA6vcG4Jwi5CSryIOOAcfs1Asl55bz0exwD3t/CF9tOMSDImZWP9OfXB/oyNMS1xfSb2sw6AmOwZH8aVbV6buvpjVVz5jc3A9bDhlPy5yqsEzIY5jOMAvtcnHILKD64Gfs+beMmdTitiFf+iOdYdilDOrnw9sQwfJyaOOWwgSQWJgIQ4vjviCBt5wl5/YCmHBHSNn7HrYXeAU78Z1IYL/5+hPfWJPL6hC7G6zx4JKhM5PCQTx/j9dsIDqUV8eW2U6w/loOJWsXUKC/uGxiAv3PLryqGduYIdibls/pINp9sSmJGX19m9vPD2bp1lxJsKNYD+iNMTSnfvInb77mdeS6v4pRrQvymPQxo5Y6gpFLLB+sSWbw/DVcbM764sztjwtxbj7olcLzwOCqhItghuH5bWnwakqEIP18TsG37k/LNze29fDieU8Z3u1Lo6GbN7b2MpBhgbien8R5fA6P+Y5w+rwODQWLL8Vy+3JbM/lQ5A+iRIUHM7OeHi03ruu+0K0ew4M7uzD5dxFfbT/HZlpN8tT2ZqVFe3D8wAF+n1uGZrxeVlRWWfftQtmkzPV54AX1HcziqJyW1iAEGPahaWBTsEkiSxB+HM3lvTQJFlVpm9/fnqZEdm3c1agNJLEzE19YXC42ckmjQG8gtygPAJ6pxuk5arZaMjAyqq41Xd7etMDVQMMy9AzW6fGKOlGKmMdL/acTLUFUE8XHNntIrSRJVWj1l1ToMeomHIs15trcPVmZqVMJAfkYyTbiaAgBzc3O8vLwwMWnYtbe+b1wT08PXgS/vjuJkbjnf7EjmtwMZLN6XxthwD+YMCiTcqxVWYWogNsOGc2bbdmqTTnJL5O2krv6DgiodhrT9qPz6trR553Eyt5xXVx5hb3Ihkd72/Dg7jFDP1vu7P154nAiXfzWEck8VotPnoALc+k9rVN8ZGRnY2Njg5+fXqkZBzYXOYOBUbgV6g0SgqxWmxnAGulrIPQo2bvKrGdAbDBRWaMkvr8FEb8DTRI2LjRl2FiaomvHvKkkSBQUFZGRk4O/v36Bj2u1kcZCrNe/fEsHOF4bywKBAtp/IY8JnO7nj671sP5GHvPC5bWE9dAgA5Zs3MSFgAoWOoNfmcGa3cQt7N4aqWj1z1yUy9pPtHMsq5d3JYfz+UL9W7QRKa0vJqsg6b6I4Y3ciBl0mLqY61J7hVzj66lRXV+Pk5NQunQCARqXCz8kSCYnUgkrj1EjWmMoLyqqLG9/XVdDqDWSXVJF4pozskirMNCr8na0IdrXGwdK0WZ0AyGuLnJycrmmE2W4dwVlcbc15cWwIu18cxsvjQjiVV86M7/Yz7tOd/BmTafyMhibkrAhd2abNWJta4xjhA+iIOZQOrcCxbU7MYeT/trFgyykmRHiy+dkh3Nnbt8mUQo3F8cLjwPkTxRlHM5D0efgFuRtFZK69OoGzmJmo8XG0pEZrIL2w0jgPYub2oK0EfdMUna/W6skoqiTxTBn5ZTVYm2kIcrUmwMUaG3OTFv2bXuu5270jOIuNuQkPDApkx/PD+ODWCLR6A0/8GsPguVv5flcKlbW6ljaxQdgMG051fDzanBwmjJSFt04Wm0N+y0lTn10TMPuHaMxN1Cy5vw8f3RbZZibqL+UIcvPzAQmvPsNbyKobDxtzEzzszSmt1nKm1AjzJeZ1o8zqksb3VYckSZRXa0nJr+BEThnFlVocLU3o6GaDr5MVlqZtM9quOIILMNWomBblzfonB/HNjCg87Mx5669j9Ht/Mx9tOEFBeU1Lm3hFzorQlW/eTJhPN3TmluiqqqiKbX4hLp3ewDc7khnx4Ta2npDXBKx5fCB9A40vEdyUJBYm4mTuhLOFLIlQUVJNlSSvinXrM74lTbvhcLIyxcnKlEBPFworai/brri4mM8///zKnWnMQW1qlPCQwSBRWFFLUm45yfkVVNXqqSnK4fbR/engYMmR2MM8/vjjV+zjvffeO+9zv37GLzl5vSiO4DKoVIIRXdxY/lA/ls/pS08/Rz7dlET//9vM63/Gk15Y2dImXhLTwEBMfH0o27QZAMegQCRdFksPHm1WOw6eLmL8/J28szqB3gFObHhqMI8MDTJaAfnm5HjR8fPnB3YexaDLwVKjwcLOoQUta170+qZfqSuEwMPeAgFkFldRXnPpkXiDHIEQcniopvyaVhnrdP+eU6s3cKa0msQzZWQUyd95LwcLQtxtzksBjYqK4tNPP71ivxc6gt27dzfYpqambY5jmpkoP0ei/Bw5mVvGl9uSWbI/jZ/3nuamCE8eHBTQ5MVQrgUhBDbDhlP488/oy8vpN3Q0a+OPkJDjye6kv+gX3LQ1CooqavlgXSJL9qfjYWfOwru6Mzq0da0JuBa0ei0ni09yd5e767dl7TmCpM/B3dfD6Od766+jHMsqNWqfXTxteWNC6GX3p6amMmbMGHr06MGhQ4cIDQ3lp59+wtLSEj8/P2677TY2bNjA888/j6OjI2+88QY1NTUEBgby/fffY219fgH7rVu3Mm/ePP7++28AHn30UaKiopg1axZ+fn5MmzaNf/75BwsLCxYvXkxQUBApKSnccccdlJeXM2nSJIQAU7WKhLQcnn/wbkqKi9FqtbzzzjtMmjSJF198kVOnThEZGcnIkSOZO3cuc+fO5bfffqOmpobJkyfz1ltvyeGhilyoKQULB6ytrbn//vtZv3497u7u/Prrr7i4uDBkyBAiIyPZuXMn06dPp0//ATzzzLOUlpVh7+DExwu+okuQD4nxsUy9914ARo0adclrLi8v57HHHiM6OhohBG+88QYHDhygqqqKyMhIQkND+eWXX7C2tqa8vBxJknj++ef5559/EELw6quvctttt7F161befPNNnJ2diY+Pp0ePHvz8888IIXjxxRdZtWoVGo2GUaNGMW/evEb9j7S9x7MWJMjVhrlTu7Lj+WHcNzCALYm5jJ+/k7u/3cfOpPxWk2lkM3wYaLVU7NiBf0Q3QOBWqOKFXe+SXJzcJOfU6g18vyuFIfO28lt0BvcP9Gfj04MZE+bRZp0AQHJJMjqD7jyNoTPZ1UiGEjx7DWpBy4zL8ePHefjhh0lISMDW1va8p20nJycOHTrEiBEjeOedd9i4cSOHDh0iKiqKjz766JrPZWdnx5EjR3j00Ud58sknAXjiiSd46KGHOHLkCB4esoP1c7LEzMycuV/+xL4D0WzZsoVnnnkGSZJ4//33CQwMJCYmhrlz57J+/XqSkpLYv38/MTExHDx4kO3bt4OpFag0UFUMQEVFBVFRURw9epTBgwfLzqKOmpoaNu3Yw03TZ/PIo4/xwRc/sHH7Hh5+4D4WfvQu1uYmzJ49m/nz5xMbG3vZ6/vPf/5Tf41xcXEMGzaM999/HwsLC2JiYvjll1/Oa//7778TExNDbGwsGzdu5LnnniM7W66BffjwYT7++GOOHTtGcnIyu3btoqCggD/++IOjR48SFxfHq6++es1/gwtRRgTXgbudOS+P68yjw4JYvC+N73amcNe3+wj1tOXBwYGMC3NH08yiUedi0a0bagcHyjZtpsPYsbj4dSI/PRXvwmAe3vQwP439CVdL4xQaP6uf/s7qY5zKq2BAkDOvje9CJ3cbo/Tf0sTmyV/4Lk6yBIJBq6PQIP9t3YM7G/18V3pyb0q8vb3p318uuHPXXXfx6aef8uyzzwJw2223AbB3716OHTtW3662tpa+fa99fcr06dPrfz711FMA7Nq1ixUrVgBw991388ILL2BmosbX0ZI5j77Eof27MTfVkJmZSU5OzkV9rl+/nvXr19OtWzcAysvLSUpKYtCgQfKooKoIJAMqlar+eu666y6mTJmC3mBAqzfQd+QEThdUkJp0nFMnEnlsxi0I5JCYh4cHxcXFFBcXy33W2fnPP/9cZMvGjRv59ddf6z87OFw5fHh2FKJWq3Fzc2Pw4MEcOHAAW1tbevXqhZeXFwCRkZGkpqbSp08fzM3Nuffeexk/fjzjxzd+nkpxBI3A1tyEOYMDuae/HysPZ/Ll9mQeX3KYuY4W3D8wgKk9vLEwbf4VvUKtxnrIEMo2bULSaukysD/bFn3L+NQIPnZNZNpf05g7eC493Xs26jxHs0p4b00Cu04W4O9sxbczoxjWgsJZTUF0TjQuFi742voCkLNtCzpDIQBu/kEtaZpRufBvdu5nq7p6vJIkMXLkSJYsWXJe23379vHggw8C8Pbbb+Po6IjB8G/a9YX57Of2fbn3Z/lj+VJqy4tYsmYrdpbmDO8Vdsn8eEmSeOmll+rtOA9zO6gskOcKzqFGq0dnkEjILqNWZ8DW2hpfJ0sotCQsNJQ9e/ac1764uPjivpsYM7N/5yHUajU6nQ6NRsP+/fvZtGkTy5cv57PPPmPz5s2NOk+jHluFEI5CiA1CiKS6nxe5PiFEpBBijxDiqBAiTghx2zn7fhBCpAghYupekY2xp6Uw06i5racPG58azJd398DZ2ozX/zxKv/c38c2OZGp1zb8WwWb4MAylpVRGRxPcS35qKyyzYpHfTGxMbbhv/X18e+Tb6w5nfbIxifHzd3Isq5Q3J3Rh/VODGN7Z7YZyApIkEX0mmii3qPrrytgajUF/Bhtbe8wviI23ZdLS0upvfIsXL2bAgItLbvbp04ddu3Zx8uRJQA6znDhxgt69exMTE0NMTAwTJ07E19eXY8eOUVNTQ3FxMZs2bTqvn6VLl9b/PDui6N+/f/1T9Lmhk5KSEjw93AlwtWXL1i2cPn2a0mot1tbWlJWV1bcbPXo03333HeXl8s0+MzOT3NxceaepDQgVVBdjMBj4efGvpOZXsOCbH4no0Rs7CxMsTNV4OVpiZ2FKSEgIeXl59b8PrVbL0aNHsbe3x97enp07d15k57mMHDmSBQsW1H8uKioCwMTEBK324jUNAwcOZOnSpej1evLy8ti+fTu9evW6ZN8gj3ZKSkoYN24c//vf/64YpmoojY1fvAhskiQpGNhU9/lCKoEZkiSFAmOAj4UQ9ufsf06SpMi6V0wj7WlRVCrB6FB3fn+oH8vm9CWsgx3vrE5gzCfb2Xo8t1ltserXD2FmRtmmzdi5umPn6k1tbRqmMRn8Ov5XRvqO5ONDH/P4lscprb22yck9pwr438YTjI/wZOtzQ5nV37/Z9dObg7SyNPKq8ohyj5I3SBJnMkHSncGjc8uEcJqKTp06sWDBAjp37kxRUREPPfTQRW1cXFz44YcfmD59OhEREfTt25fExMSL2nl7ezNt2jTCwsKYNm1afbjmLEVFRURERPDJJ5/wv//9D4BPPvmEBQsWEB4eTmZmZn3bO++8k+joaAb27sHWv5YTENSRzKIqSgzmRPXuQ5fQUJ58+hkGDR3ObbdPp2/fvoSHh3PrrbfWOwpJCAymthiqSrC0tGLTjj2MHNCTmH07mfff/+DtaHne6l9TU1OWL1/OCy+8QNeuXYmMjKzP8Pn+++955JFHiIyMvOxD1KuvvkpRURFhYWF07dqVLVu2APDAAw8QERHBnXfeeV77yZMnExERQdeuXRk2bBgffPAB7u7ul/1blZWVMX78eCIiIhgwYMB1zdNciGjMBKcQ4jgwRJKkbCGEB7BVkqROVzkmFrhVkqQkIcQPwN+SJC2/lvNGRUVJ0dHR1213cyFJsvrgf/5OICW/gtfHd2H2gIZpfxiD9Icepvp4IkGbNrHrt1/Y9/tSQt0jGDP3FSQTCxYnLmbegXm4W7nz0ZCP6Ox09Zh3RY2O0R9vR6MSrHliYJtdQNMQlp9Yzlt73uLPm/8kwC4AchP59vk9FFetYNBds+k5YYpRzpOQkEDnzsafb2goqampjB8/nvj4+CY/l5+fH9HR0Tg7X1+ZSkmS8/lzy2rQXmLVvwDUKhVqlUCjEkhAjU6PjaEcH1UuVsEDSM8twt7CpNWvaG8sl/q/EkIclCQp6sK2jX2Mc5MkKbvu/RngiupOQohegClw6pzN79aFjP4nhLjsUlMhxANCiGghRHReXl4jzW4ehBAMC3Fj3ZOD6OXvyLc7UzAYQ0elgdgMH4YuK5uaxMS68JDEqTIH9Cc2I4Tgzs538v2Y76k11HLXmrtYkrjkqqGi99YkkFlcxbypXW9oJwDy/ICTuRP+trLzrty3igq1fPNxD7hx5gfaEkIInKzNCHG3IcTdlmBXa/ydrfBxtMTT3gIXG3PsLDSYm6hAyEsJ7CxMsLRzREIgBDhamd7wTuBauaojEEJsFELEX+I16dx2knwHuexdpG7EsAi4R5Kks678JSAE6Ak4Ai9c7nhJkr6SJClKkqQoFxeXq19ZK8JUo+LuPr5kFlex61RTC9D+i/WQISAEZZs24+oXgKWtI7W16ZzcGlffJtI1kmUTltHToyfv7XuPBzc8yJmKM5fsb0dSHr/sS+O+Af5EtXDFsKZGkiQOnDlAT/ee9fMD2bsOI+nkjBXXG2ii2M/Pr1lGAyCPPq53NHAuQghMNSosTDXYmJtgb2mKs7UZ7nbmdHCwxNfJikAXawJdrPFysMTZxgJhZkP5qf2tQnertXFVRyBJ0ghJksIu8foTyKm7wZ+90V8yEC6EsAVWA69IkrT3nL6zJZka4Hvg8jMkbZxRoW7YW5rw64H0ZjunxtkZi8hIyjZvQghBx779kHSpbI3vQmF6cX07R3NHvhj+Ba/1eY2YvBim/DmFP0/+ed7ooLRaywvL4wh0seKZUVeM/t0QZJRlkFuZS5Rb3Si6MIWSXAMGfQ62ji6YWbaOymkK14CFPehr21z51uagsaGhVcDMuvczgT8vbCCEMAX+AH66cC7gHCcigJuB5nksaQHMNGomd+vAhqM5V9RQMTY2w4dRcywBbVYWwb36Ikl6DLWJrFlwkJrKfzMYhBBM6zSNFRNXEOwQzKu7XuXRzY+SWyn79nf/TuBMaTUfTovE3KT1FbkxNgdy5Hq39Sm2CX9RonPFoMvBo+ON7whvSMzOitAVt6gZrZHGOoL3gZFCiCRgRN1nhBBRQohv6tpMAwYBsy6RJvqLEOIIcARwBt5ppD2tmtt6elOrN/DH4cyrNzYSNqNHg0pF0ZIleHUJwyO4E9qqHZQUlrL+22MXzVl423jz/ZjveaHnC+zP3s/Nf97M/+1cxNLoNB4cHEikt32z2d6SHDhzAEdzR/zt6ib3E1ZRhCtI5Xh0DLnywQqtE7UGTK0VR3AJGuUIJEkqkCRpuCRJwXUhpMK67dGSJN1X9/5nSZJMzkkRrU8TlSRpmCRJ4XWhprskSSq/wunaPCHutnT1tmfpgbRmk6Mw9fbGduxYCn9ZjKGklHGPPgtChWXtT5yOz2ffqoslJ1RCxV1d7mL5xOX42Qby86kPcA78mTv7tR5NpaZEkiSic85ZP1CaBRkHKMUUAGcfv5Y1UOH6sbAHXY0SHrqAGy/5u5Vze09vTuSUE3NOjL6pcZ7zIFJVFYU//YS9uwdDxg2gsEqLi9NhDq09zcmDl17j4G3tg3Ppk9TmjsdgnsS01bew6tSqVqOp1FRklGdwpuLMOWEhWTytsi7Hwc6leUofNhcNUvI0AitXruTYsWNNfp4r0gQ1Cm4EFEfQzEzo6omlqZrF+9Ka7ZxmwcHYjB5N0aKf0ZeWEj71EQJtishO2YaDexWbfkqgIPP8wVitzsCTS2NYFXuGR7vP5o9JvxNkH8QrO1/hkU2PkFNxsd5La6NKV0VGWQYxuTHkVzU8W2t18moAernX5S4krEJn3xmtkOdUbIyQ9dKauFZHIEnSeRISDaVVOAK1KZhY1YvQKcjc2IngrRBrMw2Tu3Vg2cEMXhwbglMzVelyfmgOZWvXUrhoES6PPMKogT78uL6I2rI1mJhOZc0XcUx9qSfmViZU1up46OdDbDuRxwtjQpgzOAAhBN+P/p4liUv45NAn3PznzTzX8zkmB01udbISldpKnt72NLsyd9VvszOzY/mE5bhbXX7FJkB6aTrfHPmGkb4jCbAPgIp8OL2LYq/HMUjlmJpaodaYNJ3x/7wIZ44Yt0/3cBj7/mV3nyvpPHToUOLi4igqKjpP9jk1NZXRo0fTu3dvDh48yJo1a/jpp5/4+eefcXFxwdvbmx49evDss89y6tQpHnnkEfLy8rC0tOTrr7+msLCQVatWsW3bNt555x1WrFhBYGCgca+zoVjYyeE+XQ1o2kaVvKZGcQQtwD39/fhlXxqL96Xx2PDgZjmneadOWI8YTuGPP+E4cyaW3SYzKu4FVmZo6DwokdRjnfjlo4NkRdiw5WQeqfkV/HdKONN7+dT3oVapuavLXQz2Gszru1/njd1vsC51HW/2fRMPa+Nr818PZbVlPLzxYY7kH+G+8PvwsfHBwsSCN3a9wfPbn+e70d+hUV36316SJN7d/y4alYYXetYtaUn8GyQDRVIIGLZh7XDjrZ94//33iY+PJyYmBp1OR2VlJba2tuTn59OnTx8mTpwIQFJSEj/++CN9+vThwIEDrFixgtjYWLRaLd27d6dHjx6ALKWwcOFCgoOD2bdvHw8//DCbN29m4sSJjB8/nltvvbUlL1cuVlOaJSuS2lz5waC9oDiCFiDI1YZBHV1YtPc0Dw4ObLaqXeaz76d84yYOPfA4+++YwwO2Zdg4mXFs+98cdDZhQKY/ufllOIVY8tLdUYzsculYuLetN9+O/pbfjv/GRwc/YvKqyTzd42mmdpzaoqODkpoSHtzwIMeLjjN38FxG+o6s36c36Hlxx4t8EfsFj3V77JLHrzu9jl2Zu3ix14u4WdVd+9E/wDGQkmJTJEMZdu5NnDp6hSf35kCSJF5++WW2b9+OSqU6T/bZ19eXPn36ALJs9KRJkzA3N8fc3JwJE+SCR+Xl5ezevZupU6fW91lT08rKu2rMwMRSzh5SHAGgOIIW457+ftzz/QHWHMnm5m4djNZvSZWW1PwKUgsqSM2v5HRBBSkFFZwuqKSwopabIibz6KE/0ORUsndQVya6xrGoth+9arfhNag77CriDnd3+l7GCZxFJVTcHnI7AzoM4M09b/Kfvf9hfep63uz3Jl42Xka7noaSU5HDQ5seIrUklY+HfMxg78Hn7b8p4Cb2Ze/j67iv6enekz4efc7bX1JTwgf7P6CzY2du73S7vLEiH1K2w4CnKV1XhmQow8nXu7kuqUX45ZdfyMvL4+DBg5iYmODn51cv+3xWjvpKGAwG7O3tiYmJaWJLG4mFvRIeOgfFEbQQg4NdCHCx4rtdKUyK9Gzwk7TeIJFXVkN6USXphZWkFVZyuqCy7sZfQVHl+TK3Hnbm+DpZMjrUDT8nK/yce6Da6M2gLz7FLisSN/do7pw5kV8//xF1wXq6DJjIoXWnsXYwI3zI1W/oXjZefD3ya5YnLefD6A+ZsmoKz0Y9y7RO067r93I9nCo+xZyNcyirLWPB8AX09bx0sZQXe71IbF4sj2x8hBG+I5gSPAVvG29+Pf4rK06soFxbzvxh81Gr6hbMJfwFkgFCb6Zo6TbAgL37jZUxBGBjY1Ov1FlSUoKrqysmJiZs2SLLPl+K/v378+CDD/LSSy+h0+n4+++/eeCBB7C1tcXf359ly5YxdepUJEkiLi6Orl27nneeFkcJD52H4ghaCJVKcE9/f15bGc/WE3l42VtQWFFLUaWWospa+X1FLYWV8s+CilpySqvJL69Ff84iMCHA084CXydLxoR54O9siZ+TFb5OVvg6WV56FXDoQ+SpDeR/9hki2BGP3vvpP+0udv76E0NndcEvwpftS09gYWNKUI+rVzITQjC141QGeP47OijXljM7bLYxf2WX5GDOQR7b/BhmajN+GPMDIY6XX+xlaWLJwhEL+S7+O1anrGZNyhpAHt0M9xnOrNBZhDqfIy999A9wCgK3MMqq1wNg62ycym6tCScnJ/r3709YWBg9e/YkMTGR8PBwoqKiCAm59O+zZ8+eTJw4kYiICNzc3AgPD8fOTk7N/OWXX3jooYd455130Gq13H777XTt2pXbb7+d+++/n08//ZTly5e33GQxKOGhC2iUDHVL0VZkqK9GZa2OPu9torRad8n9lqZqHCxNcbSSX262ZrjamONmZ463gwXejpZ0sLe4LskHSZLI+/BDCr75FsdwCZfFR/jzo/dIjT3Mra/9lwN/V5JzupQJj3bFK6ThE6R6g56Xdr7EPyn/8FzUc8wInXHNtjWUf1L+4ZWdr+Bl48UXI76gg3XDQ2zVumo2pW0iqzyL8QHjL57sLs+DDzvCwGeQhr7CwrvnUqndzqwPP8fJy+fSnV4nLS1Dfb2Ul5djbW1NZWUlgwYN4quvvqJ79+4tbVbDKc+RRwWuXW7I8NC1yFArI4IWxNJUwzcze5J4prT+hm9vaYKTlRn2liZNqukjhMDlmWcwZMRTuHYfqvdfZcxTr/Lzi0+wdsFcpr42j3++TGLNwiNMfro7Lj4Nq0GsVql5b8B76Aw65kbPRaPScEfnO4xquyRJfBv/LZ8c+oQebj34ZOgn2Jld26pnc405NwXcdPkGCavqwkKT0ebmU6uSnfWNOCK4Xh544AGOHTtGdXU1M2fObFtOAJTw0DkojqCF6eXvSC//lklJFELg9t9PMSRFkv/LKnDy5aYnXmDpGy+w+btPGP/Ii/z+4SH++iyWW57rjp1LwxQ3NSoN/zfo/9Bv1fPf/f/F1syW8QGNL7ANoDVoeXfvu6xIWsFY/7G80/8dTNWmRun7PI7+AU7B4NqF4h2HkKQyTEzMMTE3N/652iiLFy9uaRMahxIeqkdZWdzOERa2eNzVD7tgifxP56NesZKhM+8nJeYgcZt+Z+LjkUh6iVWfxFBZ2nDVVBOVCXMHz6Wne09e2/Uau7N2N9rW4upiHtzwICuSVnBf+H28P/D9pnECZTlweheETgYhKE4+g2QoxdLOyfjnUmhZLBxk3SFddUtb0qIojkABET4Fj+7ZOEwcRuEPP+CybRehg4axd8USCjOPctOjEVSW1vLX/Bhqqy49n3EpTNWmfDL0EwLsAnhqy1McK7h+eYFTxaeYvno6MbkxvDfgPZ7o/gQq0UT/vvVhoZsBKMksqltD0DoWzSkYEXN7+Wc7l5xQHIECdByNMLXEbaAJzo88QunvfxCScApXvwD++exDzCwqGPNgOIWZFaz5Ig6dVt/grm1MbfhixBfYmdnx8MaHSS+7tsI8tfpavj3yLdNXT6dKV8X3Y75nQuCEa73Ca+PIMnkC0U3OICrNrUQylOLkZbz1HgqtBI2pLE1dVdiuK5cpjkABTK2g0zjEsT9xefhB3F5+maqNm+mRWYAQglXz3sUz0IphMzuTeaKYDd9dXMfgSrhaurJwxEJ0ko45G+ZQWF3YoON2ZOxgyqopfHzoY3p79ObX8b/S1aXr9V5lwyhKhfR9EP6vDEJJWRWgw95NmSi+IbFwaPfS1I1yBEIIRyHEBiFEUt1Ph8u0059TlGbVOdv9hRD7hBAnhRBL66qZKbQEEdPkp6KTm3CccTeec+ciYuLoXlRFfkYa6778lI693BgwNZjkw3lsW3L8muSoA+wD+GzYZ+RU5vDIxkeo1FZetm1iYSIPbniQhzc9DMDnwz9n/rD5VxWMMwrxK+SfYf86grKaOtVRlxvXEcyePRtXV1fCwsKu+Vg/Pz/y8xuu7rp161Z27278nFFDmDVrFsuXy4UR77vvvkurn5rbA4Kt61efZ9fChQv56aefmsXOlqaxI4IXgU2SJAUDm+o+X4qqc4rSTDxn+/8B/5MkKQgoAu5tpD0K10vgMLBwlMMigN2E8XgvXIjD6Qw6l2s5vns7h9asoutwb3qM8eXYjqxLFrW5EpGukcwdNJdjhcd4etvTlNScrwmfXZ7NKztfYdpf04jPj+e5qOf4feLvDPQaaLTLvCpHloN3H3DwBcBQU0OlkCWXb+TU0VmzZrF27dpmOVdjHYFO1/B5qnP55ptv6NKly8U71Bows2Hr1i3s3vWvYu2cOXOYMaPp1sG0JhqbPjoJGFL3/kdgK/BCQw6sq1M8DDibZP4j8CbwRSNtUrge1CZylkzsEqgpBzNrrAf0x/fHHxD3P0CRyoZti77F1T+A3pPCqCqr5eA/p7GwNqXr8Ibr7wz1GcrrfV7nrT1vMWbFGGaEzuDmwJtZcnwJvxz7BYBZYbO4N+zea14b0GhyjkLuMRg3r35TTXomtWoJtGDbDCOC/9v/fyQWJhq1zxDHEF7odeWv5aBBg0hNTb1im4qKCqZNm0ZGRgZ6vZ7XXnuN2267DYD58+fz119/odVqWbZsGSEhIRQWFjJ79mySk5OxtLTkq6++wtbWloULF6JWq/n555+ZP38+Awf+6+jffPNNTp06xcmTJ8nPz+f555/n/vvvZ+vWrbz22ms4ODiQmJhIQkICL774Ilu3bqWmpoZHHnmEBx98EEmSeOyxx9iwYQPe3t6Ymv4bZBgyZAjz5s0jKiqKtWvX8vLLL6PX63F2dubbzz5k4U+/oTYx5edffmH+/Pls2rQJa2trnn32WWJiYpgzZw6VlZUEBgby3Xff4eDgwJAhQ+jduzdbtmyhuLiYb7/9loEDB3L06FHuueceamtrMRgMrFixguDg5lEavh4a6wjcJEnKrnt/BricEIu5ECIa0AHvS5K0EnACiiVJOuveM4DLzsYJIR4AHgDw8THuyk6FOsKnQvS3kLgauspfcIvwcPwWL0Z//71sV9fy19x3uHveAgbf0YnqCh07lyVhbm1Cp94ND9vc0vEWwl3CWXB4AZ/HfM7nMZ8jEEwInMCjkY+2nKR13G8g1LJDrKPkWDKSVI5abYq5lXXL2NVKWLt2LZ6enqxeLRfuKSn5d0Tn7OzMoUOH+Pzzz5k3bx7ffPMNb7zxBt26dWPlypVs3ryZGTNm1N9Qz95gL0VcXBx79+6loqKCbt26cdNN8sK/Q4cOER8fj7+/P1999RV2dnYcOHCAmpoa+vfvz6hRozh8+DDHjx/n2LFj5OTk0KVLF2bPPl/qJC8vj/vvv5/t27fj7+9PYWEhjvZ2zLn7VqztnXn2Nbl0+qZNm+qPmTFjBvPnz2fw4MG8/vrrvPXWW3z88ceAPELZv38/a9as4a233mLjxo0sXLiQJ554gjvvvJPa2lr0+oYnWLQEV3UEQoiNwKW+5a+c+0GSJEkIcbmgsa8kSZlCiABgc13B+muqFSdJ0lfAVyBLTFzLsQoNxLs32PnI4aE6RwBgFuBP8OIl1N5/H1vLyvjj9ee54+MvGXlvF/7+LJbNPyZgbmWCb1jD8+w7OnTkk2GfcDT/KJvSNjHabzSdHJtY4vlKGAzy/EDgMLD6twJZ/r6jSIZSrB2cm0Vi+2pP7i1JeHg4zzzzDC+88ALjx48/70l+ypQpAPTo0YPff/8dgJ07d7JihTznMmzYMAoKCigtLb3qeSZNmoSFhQUWFhYMHTqU/fv3Y29vT69evfD39wdg/fr1xMXF1cf/S0pKSEpKYvv27UyfPh21Wo2npyfDhg27qP+9e/cyaNCg+r4cHesWdGrM5QljyQDnpCaXlJRQXFzM4MGyou3MmTPPk9k+99rPjqr69u3Lu+++S0ZGBlOmTGnVowFowBxBXVH6sEu8/gRyhBAeAHU/L1n8VpKkzLqfycjho25AAWAvhDjrjLyAzEZfkcL1o1LJ2TKnNstaO+dg4uZG+KKf6WlqTV5+Lv+8/AwaEzXj5kTg5GXN2i+PcCb52uvAhjqH8nj3x1vWCYCcKVSSLo+KziErqRDJUIq9e/tbeZqenk5kZCSRkZEsXLiQjh07cujQIcLDw3n11Vd5++2369uamclaPWq1+rpj+Ge50OGe/XyuDLYkScyfP5+YmBhiYmJISUlh1KhRjTovJuaAAWquTSH1Utd+xx13sGrVKiwsLBg3bhybN29unG1NTGMni1cBM+vezwT+vLCBEMJBCGFW994Z6A8ck+SUky3ArVc6XqGZCZ8Kkl6WWLgAtZ0dfb/7ic6WdpxIS2bXC89gYq5m/KNdsbI34+/PYinIKr9Ep22AuF9luYGQf/WHak+fJkftDYZSHDzanyPw9vauv9HOmTOHrKwsLC0tueuuu3juuec4dOjQFY8fOHAgv/wiz/ts3boVZ2dnbG1trypH/eeff1JdXU1BQQFbt26lZ8+eF7UZPXo0X3zxBVqtnNF14sQJKioqGDRoEEuXLkWv15Odnc2WLVsuOrZPnz5s376dlJQUAAoL5XRmG3tnyiqqoLLgvPZ2dnY4ODiwY8cOABYtWlQ/OrgcycnJBAQE8PjjjzNp0iTi4uKu2L6laawjeB8YKYRIAkbUfUYIESWE+KauTWcgWggRi3zjf1+SpLM5XC8ATwshTiLPGXzbSHsUGotbF3ALlyeNL4HK3JzRX32Pu5Ut+5MTiX/qCSwsBBOfiERtouKvT2IozW9j+djaKoj/HbpMArN/5wEKtuyixMYDSapplonilmT69On07duX48eP4+XlxbffXvxVPHLkCL169SIyMpK33nqLV1999Yp9vvnmmxw8eJCIiAhefPFFfvzxRwAmTJjAH3/8QWRkZP3N9VwiIiIYOnQoffr04bXXXsPT0/OiNvfddx9dunShe/fuhIWF8eCDD6LT6Zg8eTLBwcF06dKFGTNm0LfvxbUpXFxc+Oqrr5gyZQpdu3atn/CeMHEif6zdRuTAsezYuvW8Y3788Ueee+45IiIiiImJ4fXXX7/itf/222+EhYURGRlJfHx8q88+UmSoFS5mz+ew7iV4eC+4XloeubK0hEWP3YeutIyRjh0I+vRTiool/vjwEJa2ptz2Si/UJm1kveKR5bDiXpj5F/gPqt+8e85/OagPoLZ0ETc98Twh/QZdoZPrp63KUDcFb7755hUnkpuc2krIPw52XmDl0jI2GIlrkaFuI99UhWYlYhqoNHD458s2sbS14+Y330drbsauM6mkzJqFnUUtI+7pQtGZSuK2ZDSjwY0k5hd5ktx3QP0mQ20tmQVmaAzyXImd641XmUzhEphagsYCKhu2+v1GQXEEChdj5Qwdx0DcUtBrL9vMzT+QUQ8/SaG1BbHFuZyefgeeDtX4hjkRvSaFqrKGq5W2GCWZcGoLdL1dniyvo+JANIV2HTE3z8LE3AJXvxasptWOePPNN1tuNHAWS0fQVoK2/SiSKo5A4dJE3gkVeXBy4xWbdRk4lO5jJ5LiaEOqtorU6dOJihRoaw3s+yulmYxtBHG/AhJETj9vc+rGWHQaS2q02Xh3CUOtUUp3tBss6pRyqgqu3O4GQnEECpcmeKQcI71CeOgsg+6ajVeXMOI9HSk1N6X0mQcI7e3EsR2ZFGS24iwiSYKYxeDTDxwDztuVfqIcdIVUlOThG9GthQxUaBHUJmBmC5VF7UaRVHEECpdGbQIRt8GJtVBxZUExtUbD+CdewNzGlkOBHaiqrsLn+O+YWmjYtTypmQy+DjKioeAkRJ5fSlN75gw5qg5YmchaSr7hiiNod1g6gUF7zWsK2iqKI1C4PJF3gEEnSy9cBSt7ByY+/TKVZaUc6xVB1e9LiYw0JT2hiPSEVjrxdvgnee1Al0nnbc5es4NKK3dU5vlYOzrh2MGrhQxUaDHMbeWEicqGq6q2ZRRHoHB53ELBsxsc+rFBQ2SP4E4MvGMWWaVFlHh5YP/nR1g7mLF35alrkqxuFqpL5LTRsFvkL/05nNx6AkkyUFJ8Gt/wbs0iLdGSpKenM3ToULp06UJoaCiffPLJNR3f5mWoL2WXULFwyWp+WvQL6NtA0kMjURyBwpWJmg15iZC2p0HNu44ch6WdPamhHdEePUKoax65p8tIiWllT1Zxv8mZIVHnC5LVJKeQqXPH2iSD2soKfCMiW8a+ZkSj0fDhhx9y7Ngx9u7dy4IFC654w2wsrU6Guo4L7Zrz2JPMmDq+XaSSKqkQClcm7BZY9yoc+BZ8+121ucbUlKjxk9n+y/eERHXD6tcPsB81l72rkvHr6oxK1QqeriUJor8Dj0jo0P28XVnL11Bq2xl3t6MU5IFveGSzmnbmvfeoSTCuDLVZ5xDcX375svs9PDzw8JAVX21sbOjcuTOZmZkX3TRvaBnqb7+9yK5NmzZhLap59sG7iDmZzZyHHrphZaiVEYHClTG1klMrj/15kRDd5eg6cixmVlakBvlCeSmdag5RlF3BiX1nmtjYBpK+X647cMFoQDIYOLlX1j2sqcnAxdcfSzv7FjCw5UhNTeXw4cP07t37on1nZahjY2OJj49nzJgx9fvOylA/9NBDzJsn13M4K0MdFxfHe++9x4wZM/Dz82POnDk89dRTxMTEnOcEzhIXF8fmzZvZs2cPb7/9NllZWYAsQ/3JJ59w4sQJvv3223oZ6gMHDvD111+TkpLCH3/8US9D/dNPP11y5HFWhnrFihXExsaybNmyy9tlagkGLTNm3MX//d//ERcXR3h4OG+99VZ9f2dlqD/++OP67WdlqGNiYoiOjsbLq3XPMykjAoWr0+Me2LcQYn6GAU9dtbmphSXdxkxk74oldLllMpbLvsDp1oXs/yuF4Ci3lpeeiP4OTG3k0c45VO7bR7Z5EPY2NeQkH6f7uImX6aDpuNKTe1NTXl7OLbfcwscff4ytre1F+294GepLobGgpLyK4qKi9i1DraCAa4gsvxD9vazb3wC6j52AiZk5Jx2s0Dg5EZD8J2WF1RzdmdXExsrUJCVRffz4xTsqC2Vl1a63nScwB5D1+zrKbP1wCdZh0OuaPSzUkmi1Wm655RbuvPPO+htbu5OhvrQxcglXSQLdpSeNFRlqhfZDz9lQfBpObbp6W8DCxpaIkWM5vn83Fg89gPXBNbjaa4lek0JtdeNuFFej+sQJUm+fTurt06mKP3r+zpjFoK+RRznnoC+vIPmo/LRq0KWhMTWjQ+fQJrWztSBJEvfeey+dO3fm6aefrt/e7mSoL2OXnYc/DnY27NgoV2ZTZKgV2i8hE8DKFQ58c/W2dUTddDMqlYrEylIse/TA98B3VJVpidvcdIJ0urw80ufMQWVpicbBgfSH5qDNrKt3pNfB/i/Bpy+4h513XMnvv5PjEI6Ts4qMxMP4hEVgYmrWZHa2Jnbt2sWiRYvYvHlz/QhgzZo1F7W74WWoL2eXxowfv/iI5155Q5Ghbk0oMtQtxJb/wrb34dFocG5YzHPDV59xdNtG7n7yZXJn3kPCqDcpEG7c/U5fzK1MjGqeoaqK03fPoObUKXx/XoTKzIzU6XegcXPFb/Fi1OmbYdlMuO1n6Dyh/riakyc5dueD7O72CpEjrNm77G2Gz36IyNE3XeFsxkORof6XFpehvhw15VCQBHbe55Uybc0oMtQKTUPP+0BtBns+a/ghE2/BoDcQfzweh+nT8d75JbVVOg6tO21U0wzV1WQ++RTVR4/S4cMPsQgNxSwoCK/586k9nUbGI49i2DYfHPyg07h/j6upIfPpZ8jwGiJvkNIA8O/Ww6j2KbRxTK3AxEIWYmyDD89Xo1GOQAjhKITYIIRIqvvpcIk2Q4UQMee8qoUQN9ft+0EIkXLOvsjG2KPQxFi7yHLNsb9eVX/oLPbuHnTqN5DY9WuwuW82dqZVdKhNIm5LBhXFNUYxS19SQtq991G8cy/SY++QWOXH35/FsuePk1j06oXnf/9L5cFo0pekYIi8D1Tq+mNz586jILOMdNf+dOnvQfaJWBw7eGPn2v5KU7YGWoUM9aUQQg6N6qpvSP2hxo4IXgQ2SZIUDGyq+3wekiRtkSQpUpKkSGAYUAmsP6fJc2f3S5IU00h7FJqavo/KX4ZrmCvodfNUtDXVxO7ciuszz+BzaBGSTs+BNamNNkd75gyn77qLzLRa9o/4kC1x9uz/O4WinEoOrUtjy6IEbMeNw/OWACrzTUlbuBtdUREVe/dx5r33KPz5Z5IHPI6ppYYeYz3JOHaEgO4XT04qKGBhDyoTqMhtaUuMTmPXEUwChtS9/xHYilyH+HLcCvwjSVJlI8+r0FK4dJSL1uz/Cvo/IQ+Xr3aIjx+BUb05vPYvesz/BsfffsMzfz8JO1V0G+mNnYvldZlSFRND2hNPc8JhEGmhg3BwtGL4lEA8guwxs9Cw/+8UDvydgtBWMFS9G3HPzWT+uJ+kfv3l4b2JCSXDZ5NfY8uQWwPJTUlAr9PhH3lRCFVBAYRKnh8oy5brXDfgf7+t0NgRgZskSdl1788AV6vndztwYVX0d4UQcUKI/wkhLpumIYR4QAgRLYSIzstr2ApXhSai32NQWSCHiBpIr0lTqS4v48jm9bi9/hq+J/5ESDr2rbr24jWSJFG0ZAnxDzzPvoD7SXMfRNjgDkx7KQq/cGfMLOTnm17j/Ym6yY+E6DI2lzyK1b1v4f3llzjOnInX5wvw3byDBMs+uPnb0qW/JymHozG1sKBDiDJxq3AZLJ1lh1Ce09KWGJWrOgIhxEYhRPwlXudp90py+tFlZ1GEEB5AOLDunM0vASFAT8CRK4wmJEn6SpKkKEmSolxc2nZR6TaPb39Zp2f3p3JKZgPw7BiCT1gE0X//gUnHjrhNGYvX6U0kHcghP6PhxWsMlZVkvfgyB77dQXS359A6dmDcwxEMnt4Jjan6ova9BlvSy3YZiVVD+OObHAjtgduLL1Du24NVC49TXa5l8PROICA5Jhrf8G6oNcbNZlK4gVBrZGdQVSSHSG8QruoIJEkaIUlS2CVefwI5dTf4szf6KwXPpgF/SJJUXwRXkqRsSaYG+B7o1bjLUWgWhIDBz0NhMhy5eq2Cs/S6eRoVRYUc3boR1yeewL9kHxqphr1/nmrQ8dUnTnD09vvYlupDUtCt+ES4Mv2NPvhHXD6dT+z+lJ5WSxkz3ZnCrAp+e+8A67+J5/e5B6mp1DJ2TjguPjbkp5+mvCAf/+7tLyxUXV1Nr1696Nq1K6GhobzxxhvXdLy1tfXVG53DypUrm1Td9FyGDBnC2VTzcePGUVxc3GC7Xn/9dTZuvESpVmtXQAVlN86ooLGhoVXAzLr3M4E/r9B2OheEhc5xIgK4GYhvpD0KzUWnceDRFbb93xUL3J+LT1hX3IM6cmDVcoSNDZ5PPoxPyj+cPlJA2tHL14eVJInCJUvZ8din7HK7m3LXTgy5sxPjHo7A0tb0ssdRlgPR30LX2wkcHMGtL0ZhZmnCqUN5dBvlw/Q3euPfVR5dJu7aBkLg37X9pY2amZmxefNmYmNjiYmJYe3atezdu7fJztdYR3C9EhZr1qzB3t7+svsvtOvtt99mxIgRFzdUm4CVE1QVgs44mW8tTWMni98HfhNC3AucRn7qRwgRBcyRJOm+us9+gDew7YLjfxFCuAACiAHmNNIeheZCCBj6CiyeJss29JjZgEMEvW+exp/z3uH47u2E3HILwcv+IKc6l43fH+X21/tcdGOvSjzO0fe+JkEbQqn/FLyDbRh6Tzg2juZXt3HXx7KTGvgMAI4eVtz2Sk+qK3RYO/w7HVVTWUHs+jV07NUPa0ena/o1GJsdv50gP924dZ6dva0ZOK3jZfcLIeqf6rVaLVqt9pLFeLKzs7ntttsoLS1Fp9PxxRdf1AvPvfLKK/z9999YWFjw559/4ubmRmpqKrNnzyY/Px8XFxe+//57MjIyWLVqFdu2beOdd95hxYoVBAYG1p9j1qxZmJubEx0dTWlpKR999BHjx4/nhx9+4Pfff6e8vBy9Xs+aNWt47LHHiI+PR6vV8uabbzJp0iSqqqq45557iI2NJSQkhKqqqvq+/fz8iI6OxtnZmZ9++ol58+YhhCAiIoKHHnroIrv+85//MH78eG699VY2bdrEs88+i06no2fPnnzx2aeYkY9fQCAz77n3Ignubdu28cQTT9T/frdv346NjY1R/p5NQaNGBJIkFUiSNFySpOC6EFJh3fbos06g7nOqJEkdJEkyXHD8MEmSwutCTXdJktSKK50rXETwKOjQA7bPvawg14UE9uiFk5cP+1YuA6DDG68QlvgDNWXVbPompr6Sma60lLjXF7DirW3ssxyH1sWPYTNCmPB0VMOcQNkZWWW06+3g9O+NRmOqPs8JAMSsX0NNZQW9bp56YS/tBr1eT2RkJK6urowcOfKSMtSLFy9m9OjRxMTEEBsbS2RkJCDXKejTpw+xsbEMGjSIr7/+GoDHHnuMmTNnEhcXx5133snjjz9Ov379mDhxInPnziUmJuY8J3CW1NRU9u/fz+rVq5kzZw7V1XIs/tChQyxfvpxt27bx7rvvMmzYMPbv38+WLVt47rnnqKio4IsvvsDS0pKEhATeeustDh48eFH/R48e5Z133qkfBX3yySdXtKu6uppZs2axdOlSjhw5IjvBr76RM4gkA84O9hdJcM+bN48FCxYQExPDjh07sLBo3RlGigy1wvUjBAx9GX6+BQ4vgp73Xv0QlYreN09lzWcfcvLgPoJ79qXzJ29T/ObPHBc3c2hZLKYFacTsKabUsjMWDtX0H+9F2MhANCYXTwafS3lhAdUV5Th18Ebs/FgeDQy68uIkbU01B1evxC+yB24BQddy9U3ClZ7cmxK1Wk1MTAzFxcVMnjyZ+Ph4wsLO12Pq2bMns2fPRqvVcvPNN9c7AlNTU8aPHw/IUswbNmwAYM+ePfWS1HfffTfPP/98g2yZNm0aKpWK4OBgAgICSEyUC/WMHDmyXjJ6/fr1rFq1qv7GW11dTVpaGtu3b+fxxx8HZM2iiIiIi/rfvHkzU6dOxdlZnlu6ogw1cPz4cfz9/enYUf7bzJw5kwULFvDkYw8DMGVk3/prP3u9/fv35+mnn65Xc1XqESjc2AQOB+/e8qgg4mJp50vRqd8gdv32M/tXLiMoqg9WvXvR7wNLCt7bxt7NnQFrLMwM9BtiQ8QtQxpUv+DY9s1s+HoButoazC0t8VRlEN7jZoIcA654XPyWDVSVltC7HY8GzsXe3p6hQ4eydu1aKioqePDBBwE5Xj5x4kS2b9/O6tWrmTVrFk8//TQzZszAxMSkPpTUnDLUK1asoFOnTo06V6NQm4JKjZlUCbUV5137iy++yE033cSaNWvo378/69atIyQkpOVsvQqK1pBC4xACRr0jL7LZMa9Bh6jUanpOvJUzJ0+QFh8LgGV4GGNeGY0XqfTrWs3MLybQ7faeV3UCutpaNnz9Gf8s+Aj3oGBGzXmcIOca8musWbUjn9S4w5c9Vq/TcmDV73QI6YJX57DLtrvRycvLq8+mqaqqYsOGDYSEhNC7d+96GeqJEydy+vRp3NzcuP/++7nvvvuuKkPdr18/fv1VXmvyyy+/1M8nXE2GetmyZRgMBk6dOkVycvIlb/ajR49m/vz59aHEw4flv/OgQYNYvHgxAPHx8ZeUfx42bBjLli2joEBOULiaDHWnTp1ITU3l5MmTwAUy1EINQgMlmedpEJ06dYrw8HBeeOEFevbsWT+qaa0ojkCh8Xj3gq7TYc8CKGhYKmjo4OFYOTiyf+W/6ad2YUFMWjibbg+NQ625chgIoCT3DL++8QJxG9fSc9KtTH31XcI7wGirrcy8byxOXj78/b/3KchMv+TxsRv+v737j46qPBM4/n2STJjITyFQNCEmgpYQGEgjiJtKJEolSsHS2lIWlx8uLlhbs/hj1WzclqPlaBQCtIKULhSxBStUYoBFeyCLi0JVVntURAIChmKBkBpAliaZd/94hySSSSY/ZnIzmedzTs4hN3dungvDPPd933uf5784U36S6+/4fvPOs5M6fvw4Y8eOxePxMHLkSMaNG1c71VNfSUkJw4cPJz09nfXr19cuhjZm6dKlrFq1Co/HwwsvvMDixYsBmDJlCgUFBaSnp3PwYMP3S1JSEqNGjSInJ4fly5fjdjdcE8rPz6eqqgqPx0NaWhr5+fkAzJ07l7Nnz5Kamsrjjz9ORkbDu8DS0tLIy8sjKyuL4cOH1/ZgaCwut9vNqlWruPPOOxk2bBhRUVHMmVPvvpZu/aHqHFyo675WWFjI0KFD8Xg8uFwucnJymvy7cpqWoVbBceZzWHqdbXD/j817tuCdVzfy32v/k6lPPMsV17RsiH/g7bfY9lwhAOPv/VcGjRxtF6yX3WCvzO7dTWXF33gxbx6x7jimPvkscd3r2i8efHcPm555kqShw/nuY/P93iXTXrQMdZ0ZM2bU3qkTNoyBk/vBWw39hkBUx7i+1jLUqv11728fMjuwDT7ZFnh/wDMuB3fXbrV3EDVHdVUVJWt+RdEzT9Kr/5Xc9dRimwTA1j8qL4XxCyAmlh59+zHpwX/nzOlTvPSzRznwpzcxXi9l+z6geNFTfC1lIBPnPepoElCdgAj0TABvVdiWntDFYhU818+BvWtg68O2DEWAheNYdxzpORN56+Xf8tdDpQHv2ikvO8rmJQWcPPIpI26dQNZddxPj8pWDKD8IO34Og8bBtbfWvubKawczcd5jbF/9PEXP/pzeCQM4V3Ga7n378Z1HfkpsXOsK3qnQWL16tdMhtE6X7hB3uU0E7p4QG17vKx0RqOCJiYVvF0LFEdj2WLNe8o3bJtK11+Vse34JNY3cbeKtqWHv1iLWPpLL2YrT3PFwPjfPmlOXBGqq4Q9zbB2Yby9u8PqrvzGSWYue5/afPER0TAxdunble3nzuaxHz9aeqVIN9UyEqBjb29vrDbx/B6IjAhVcyd+05al3FdoHzlIbLjrW5+7ajVv++UdseuYJ3i7awOjJP6j9mfF62f/WG7z5+99ScfwYySMyGD83l669Lul/tKsQyv4Ek1faIbofUdHRDM7MYnBmFsbrRTrIPK7qRKJioFcSnD4IZ49DD//vxY5IE4EKvrF5cGgHFP0YEq+z6wdNGDRyNF+/4UZ2b/gdg0aOpk9iEgff2cObL63l5NHDxA+4ikkP5TMwY1TD+fy/vAclCyBtMgxr3gKjJgEVMu4ecFkfOHsCuvRs1nM1HYEmAhV8MbH26vz5MXbKZtqGr7SH9Cd71hyOfvA+W5Y+Q1R0DH89dIBe/a/gth8/yOB/GOP/w/vL07DhbujaF25/1i7aKeW0Hgm2nWXFYYi/1v5/6OD00kiFRt9rIecpOzLY/EDAht+X9ehJ9sx/4eSRTzl/5gtunXM/MxcuJ/WbN/lPAtUXYP00+NtR+O6v4bKmywSowGpqakhPT/f7DEFTIq4MdSBR0dD7ajA1tlS7t6blx2hnOiJQoZMxHSo+hf9ZZKeHbmrQ0vorBmdm0Scxid4JiU03h/F67UjjyC6bBJIzgxx4ZFq8eDGpqalUVlYG3rkNXnnlFSZMmMCQIUNa9frq6mpiYlr+0bVly5YWxTV//vxWxQfYNpaXp9j1gorDNjF04BGrJgIVWjf/h50vLVlgG3pcN6vJ3fteldL08YyB1/Phw41wy8+avS4QLnasXsGJI4eCesx+V13N2Bn3NLlPWVkZmzdvJi8vj4ULF/rdR8tQ1ytDvWwZXbp0ITk5menTpzdehtpbjXir2bl1I90TU9ueDLw1AadZW0OnhlRoidhbOq/5FhTPs8XpWntrXdV52Dgb3voFjJxt705SQZGbm8vTTz9NVBML6VqGul4Z6mXLan8eHx/feBnqP3/AG6+9Spw517ZpImPg3Ck48VFImuHoiECFXrQLvr8Gin4C25+wd/rcsczeYdFcZz6HdVPh2LuQnW+bzXTgoXZrBbpyD4Xi4mL69etHRkYGJSUlje6nZagvKUOdmwvA5MmTa8+90TLUfS6HLz6DUwfsNFFLFpC9Nfa15yvsg2sS/Ov3Nh1RRO4UkQ9FxOvrStbYfuNFZL+IlIrII/W2p4jIHt/29SLS8ZfXVeu44mDyCrh1AezfCr/Khn2vBh4dVF+APStg+Y1w4mP4wYu2x0AnTAJO2bVrF0VFRSQnJzNlyhS2b9/OtGnT2LNnDyNGjGDEiBEUFRUxZswYdu7cSUJCAjNmzGDNmjUAjpahvlgd9ejRo47Va+rSxTY6urQM9cqVKzl//jyZmZl8/Nkp6D0Qai7AyY/thU2g0YEx9u6jU5/YJNC9vz1GdBPrZ63U1tTyATAZ2NnYDiISDfwSyAGGAD8UkYurRE8Bi4wxg4AKIHBnExW+ROCGe+GfNtm6LOunwXOjYe8LcKq0rvdx1f/ZUcPuZbAkHbY+BPHXwN2vBXxATbXcggULKCsr4/Dhw6xbt47s7GzWrl2rZah9mixD3Qi/ZajdPSD+6xDbzZZtP7HPlqT4+7m6CyJjbPHEL8ttIbvyUlvMrvdA6H5FyC6A2jQ1ZIzZBw0z+CVGAaXGmEO+fdcBk0RkH5ANTPXt9xvgp8AyfwdRnUjKjXDfu/DRK/DGQii6z26XaHvVc+Zze+sd2KY3dzwHKVk6CnBYSUkJBQUFuFwuunXrVjsiaMzSpUuZOXMmBQUFtYvFYMs9z549myVLlvDyyy83WCe4WIa6srKyyTLUubm5eDwevF4vKSkpFBcXM3fuXGbOnElqaiqpqakBy1BHR0eTnp7O6tWrG8R1Uf0y1BcXi79ShtqPwsJCduzYQVRUFGlpaXVlqF1u6HM1XDgLlX+xXwCIfTLZW1V3kBg39BwAcb1DXtE0KGWoRaQEeNAY06A2tIh8Dxhfr5H9XcD12A/93b7RACIyANhqjPHbIURE7gHuAUhKSso4cuRIm+NWHYAxcGwvnNpvC8d98Zl9TP9rQ+1Xn4GdPgFoGeo6YVmGui1qquyIoOpL++dol+18FuOG2K5teu+3pAx1wBGBiPwR8FcjIM8Ys6nVUbaQMWYFsAJsP4L2+r0qxEQgMcN+KRVpol0Q18t+OShgIjDG3NLG33EMGFDv+0TftnKgl4jEGGOq621XSkWosC1DHeba4zmCt4FrfHcIxQJTgCJj56R2ABfHgNOBdhthKNWRhGOnQNVxtfT91NbbR78jImXADcBmEdnm236liGzxBVQN3AdsA/YBLxljPvQd4t+AeSJSCvQBft2WeJQKR263m/Lyck0GKiiMMZSXl/tdZG+M9ixWymFVVVWUlZXVPkGrVFu53W4SExNxub76zEGrF4uVUqHlcrlISQlQY0mpENJaQ0opFeE0ESilVITTRKCUUhEuLBeLReQkEG6PFscDp5wOop3pOUcGPefwcZUxpu+lG8MyEYQjEXnH32p9Z6bnHBn0nMOfTg0ppVSE00SglFIRThNB+1nhdAAO0HOODHrOYU7XCJRSKsLpiEAppSKcJgKllIpwmggcICIPiIgRkXinYwk1ESkQkY9F5M8i8gcR6eV0TKEiIuNFZL+IlIrII07HE2oiMkBEdojIRyLyoYjc73RM7UFEokXkf0Wk2OlYgkUTQTvzteT8FnDU6VjayevAUGOMB/gEeNTheEJCRKKBXwI5wBDghyIyxNmoQq4aeMAYMwQYDfwoAs4Z4H5sSf1OQxNB+1sEPAxExCq9MeY1X08KgN3YTnSd0Sig1BhzyBjzd2AdMMnhmELKGHPcGLPX9+cz2A/HBGejCi0RSQRuB1Y6HUswaSJoRyIyCThmjHnf6VgcMgvY6nQQIZIAfFbv+zI6+YdifSKSDKQDexwOJdQKsRdyXofjCCrtRxBkIvJHoL+fH+UBj2GnhTqVps7ZGLPJt08edirhxfaMTYWeiHQDNgC5xphKp+MJFRGZAJwwxrwrIjc5HE5QaSIIMmPMLf62i8gwIAV4X0TATpHsFZFRxpjP2zHEoGvsnC8SkRnABOBm03kfXDkGDKj3faJvW6cmIi5sEnjRGLPR6XhCLBOYKCK3AW6gh4isNcZMcziuNtMHyhwiIoeB64wx4VjBsNlEZDywEMgyxpx0Op5QEZEY7GL4zdgE8DYwtV5/7k5H7BXNb4DTxphch8NpV74RwYPGmAkOhxIUukagQu0XQHfgdRF5T0SWOx1QKPgWxO8DtmEXTV/qzEnAJxO4C8j2/du+57taVmFGRwRKKRXhdESglFIRThOBUkpFOE0ESikV4TQRKKVUhNNEoJRSEU4TgVJKRThNBEopFeH+HyLbx/dw28tBAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# batch the inference across K=100\n",
    "x = np.linspace(-5,5,100).reshape((100, 1)) # (k, 1)\n",
    "targets = np.sin(x)\n",
    "predictions = jax.vmap(functools.partial(mlp.apply, params))(x)\n",
    "plt.plot(x, predictions, label='pre-update predictions')\n",
    "plt.plot(x, targets, label='target')\n",
    "\n",
    "x1 = np.random.uniform(low=-5., high=5., size=(K,1))\n",
    "y1 = 1. * np.sin(x1 + 0.)\n",
    "\n",
    "net_params = params\n",
    "for i in range(1,5):\n",
    "    net_params = inner_update(net_params, x1, y1)\n",
    "    predictions = jax.vmap(functools.partial(mlp.apply, net_params))(x)\n",
    "    plt.plot(x, predictions, label='{}-shot predictions'.format(i))\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoregressive Flows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_checkerboard, make_moons\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "X_train, X_test = make_moons(5000, noise=0.07)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "_ =  ax.hist2d(*X_train.T, bins=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpyro.distributions.flows import InverseAutoregressiveTransform\n",
    "from numpyro.distributions import Normal\n",
    "import numpyro\n",
    "from typing import Sequence\n",
    "\n",
    "\n",
    "\n",
    "def get_masks(input_dim, hidden_dim=64, num_hidden=1):\n",
    "    masks = []\n",
    "    input_degrees = jnp.arange(input_dim)\n",
    "    degrees = [input_degrees]\n",
    "\n",
    "    for n_h in range(num_hidden + 1):\n",
    "        degrees += [jnp.arange(hidden_dim) % (input_dim - 1)]\n",
    "    degrees += [input_degrees % input_dim - 1]\n",
    "\n",
    "    for (d0, d1) in zip(degrees[:-1], degrees[1:]):\n",
    "        masks += [jnp.transpose(jnp.expand_dims(d1, -1) >= jnp.expand_dims(d0, 0)).astype(jnp.float32)]\n",
    "    return masks\n",
    "\n",
    "def masked_transform(rng, input_dim):\n",
    "    masks = get_masks(input_dim, hidden_dim=64, num_hidden=1)\n",
    "    act = stax.Relu\n",
    "    init_fun, apply_fun = stax.serial(\n",
    "        flows.MaskedDense(masks[0]),\n",
    "        act,\n",
    "        flows.MaskedDense(masks[1]),\n",
    "        act,\n",
    "        flows.MaskedDense(masks[2].tile(2)),\n",
    "    )\n",
    "    _, params = init_fun(rng, (input_dim,))\n",
    "    return params, apply_fun\n",
    "\n",
    "init_fun = flows.Flow(\n",
    "    flows.Serial(*(flows.MADE(masked_transform), flows.Reverse()) * 5),\n",
    "    flows.Normal(),\n",
    ")\n",
    "\n",
    "params, log_pdf, sample = init_fun(flow_rng, input_dim)\n",
    "\n",
    "\n",
    "\n",
    "class Flow(flax.linen.Module):\n",
    "    base_dist: numpyro.distributions.Distribution\n",
    "    transforms: Sequence[flax.nn.Module]\n",
    "    \n",
    "    def __call__(self, key, num_samples):\n",
    "        return self.forward(key, num_samples)\n",
    "\n",
    "    def forward(self, key, num_samples):\n",
    "        logdet = jnp.zeros(num_samples)\n",
    "        x = self.base_dist.sample(key, (1, num_samples,1)).reshape(-1,2)\n",
    "        for transform in self.transforms:\n",
    "            x, t_logdet = transform(x)\n",
    "            logdet+=t_logdet\n",
    "        return x, self.base_dist.expand(x.shape).log_prob(x).sum(-1)+t_logdet[:]\n",
    "\n",
    "    def inverse(self, x):\n",
    "        logdet = jnp.zeros(x.shape[0])\n",
    "        for transform in reversed(self.transforms):\n",
    "            x, t_logdet = transform.inverse(x)\n",
    "            logdet+=t_logdet\n",
    "        return x, self.base_dist.expand(x.shape).log_prob(x).sum(-1) + t_logdet[:, None]\n",
    "    \n",
    "\n",
    "class MADE(flax.linen.Module):\n",
    "    arnn: flax.nn.Module\n",
    "    \n",
    "    def inverse(self, x):\n",
    "        log_weight, bias = self.arnn(x).split(2, axis=1)\n",
    "        outputs = (x - bias) * jnp.exp(-log_weight)\n",
    "        log_det_jacobian = -log_weight.sum(-1)\n",
    "        return outputs, log_det_jacobian\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = jnp.zeros_like(x)\n",
    "        for i_col in range(x.shape[1]):\n",
    "            log_weight, bias = self.arnn(outputs).split(2, axis=1)\n",
    "            outputs = jax.ops.index_update(\n",
    "                outputs, jax.ops.index[:, i_col], x[:, i_col] * jnp.exp(log_weight[:, i_col]) + bias[:, i_col]\n",
    "            )\n",
    "        log_det_jacobian = -log_weight.sum(-1)\n",
    "        return outputs, log_det_jacobian\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return self.forward(x)\n",
    "\n",
    "arnn = MLPCompact(2, 64, 2)\n",
    "made = MADE(arnn)\n",
    "flow = Flow(Normal(jnp.zeros(2), jnp.ones(2)), [made])\n",
    "\n",
    "x = jnp.zeros((10, 2))\n",
    "key, _ = jax.random.split(key)\n",
    "params = flow.init(key, key, 10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import optax\n",
    "\n",
    "def arr_iterator(arr, batch_size):\n",
    "    iters = int(arr.shape[0]/batch_size)\n",
    "    for i in range(0, iters, batch_size):\n",
    "        yield arr[i*batch_size:(i+1)*batch_size]\n",
    "\n",
    "optimizer = optax.adam(1e-3)\n",
    "def construct_training_step(model, optimizer):\n",
    "  # model forward\n",
    "  def model_loss(params, x):\n",
    "    _, logprob = model.apply(params, x, method=model.inverse)\n",
    "    return jnp.mean(-logprob) # NLL\n",
    "  \n",
    "  grad_func = jax.value_and_grad(model_loss, argnums=0)\n",
    "  # this is the function that we call in the end\n",
    "  def update_func(params, opt_state,  x):\n",
    "    loss, grads = grad_func(params, x)\n",
    "    updates, opt_state = optimizer.update(grads, opt_state)\n",
    "    return loss, updates, opt_state\n",
    "\n",
    "  return jax.jit(update_func)\n",
    "\n",
    "\n",
    "#params = None\n",
    "for e in range(100):\n",
    "  #tqdm_iter = tqdm(arr_iterator(X_train, 128))\n",
    "  for i, batch in enumerate(arr_iterator(X_train, 128)):\n",
    "\n",
    "    if params is None:\n",
    "      # initialize params, optimizer\n",
    "      rk1, rk2 = jax.random.split(key)\n",
    "      params = flow.init(rk1, rk2, 10)\n",
    "      opt_state = optimizer.init(params)\n",
    "      training_step = construct_training_step(flow, optimizer)\n",
    "\n",
    "    key, _ = jax.random.split(key)\n",
    "    loss, updates, opt_state = training_step(params, opt_state, x)\n",
    "    params = optax.apply_updates(params, updates)\n",
    "  print(f\"Epoch {e:5d}, batch {i:5d}, loss={loss:10.5f}\")\n",
    "    #tqdm_iter.set_description(f\"Epoch {e:5d}, batch {i:5d}, loss={loss:10.5f}\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, logdet  = flow.apply(params, key, 200)\n",
    "_ = plt.hist2d(*x.T, bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dist = Normal(jnp.zeros(2),jnp.ones(2))\n",
    "base_dist.expand(x.shape).log_prob(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shapea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits, z, mean, logvar = vae.apply(params, key, image)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stein Variational Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SVGD(flax.linen.Module):\n",
    "  base_dist: numpyro.distributions.Distribution\n",
    "  kernel: callable\n",
    "\n",
    "  def __init__(self, P, K, optimizer):\n",
    "    self.P = P\n",
    "    self.K = K\n",
    "    self.optim = optimizer\n",
    "\n",
    "  def phi(self, X):\n",
    "    X = X.detach().requires_grad_(True)\n",
    "\n",
    "    log_prob = self.base_dist.log_prob(X)\n",
    "    score_func = autograd.grad(log_prob.sum(), X)[0]\n",
    "\n",
    "    K_XX = self.K(X, X.detach())\n",
    "    grad_K = -autograd.grad(K_XX.sum(), X)[0]\n",
    "\n",
    "    phi = (K_XX.detach().matmul(score_func) + grad_K) / X.size(0)\n",
    "\n",
    "    return phi\n",
    "\n",
    "  def step(self, X):\n",
    "    self.optim.zero_grad()\n",
    "    X.grad = -self.phi(X)\n",
    "    self.optim.step()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "be97daffee0cb79bf284fb5c73b937ebf75cd7f53ef12b5a2937ebadc10a9be5"
  },
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('.venv': poetry)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
