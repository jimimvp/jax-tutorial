{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JAX Basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "from jax import numpy as jnp\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "x = np.array([1.0, 2.0])[None].T\n",
    "y = jnp.array([1.0, 2.0])[None].T\n",
    "\n",
    "# numpy arrays are transformed to JAX tensors automatically at operation execution (no to_tensor, from_numpy mumbo-jumbo)\n",
    "print(\"Dot product:\", x.T@y)\n",
    "x.T@y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y[0] = 3.0\n",
    "y.at[0].set(3.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we should not do this\n",
    "class Func:\n",
    "    def __init__(self):\n",
    "        self.x = 0\n",
    "    def __call__(self, x):\n",
    "        res = jnp.square(x+self.x)\n",
    "        self.x += x\n",
    "        return res\n",
    "\n",
    "\n",
    "func = Func()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jax.grad(func)(2.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The JIT\n",
    "\n",
    "Stands for \"Just in Time Compilation\". All JAX operations are implemented in terms of operations in XLA â€“ the Accelerated Linear Algebra compiler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "M = np.random.uniform(1.0, 2.0, (2000, 1000))\n",
    "M_jax = jnp.array(M)\n",
    "\n",
    "def numpy_function(M):\n",
    "    return np.log((M @ M.T)**2) - M @ M.T\n",
    "\n",
    "def function(M):\n",
    "    return jnp.log((M @ M.T)**2) - M @ M.T\n",
    "\n",
    "@jax.jit\n",
    "def jit_function(M):\n",
    "    return jnp.log((M @ M.T)**2) - M @ M.T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Numpy function:\")\n",
    "%timeit  numpy_function(M)\n",
    "print(\"JAX function:\")\n",
    "%timeit  function(M_jax)\n",
    "print(\"JAX jit function:\")\n",
    "%timeit  jit_function(M_jax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorization / Parallelization\n",
    "\n",
    "`jax.vmap` and `jax.pmap`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vmap usage\n",
    "# say we have a \"complicated function\" that we want to apply row-wise, ie. over axis=0\n",
    "\n",
    "M = np.random.uniform(1, 10, (200, 2))\n",
    "func = lambda x: x[0]**2 > np.exp(x[1])\n",
    "\n",
    "#@jax.jit\n",
    "def naive(M):\n",
    "    return jnp.stack([func(x) for x in M])\n",
    "\n",
    "#@jax.jit\n",
    "def with_vmap(M):\n",
    "    return jax.vmap(func)(M)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def with_pmap(M): # this is just for the sake of example, it goes across devices\n",
    "    return jax.pmap(func)(M)\n",
    "\n",
    "\n",
    "print(\"Naive:\")\n",
    "%timeit -n 50 naive(M)\n",
    "print(\"With vmap:\")\n",
    "%timeit -n 50 with_vmap(M)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating Gradients\n",
    "\n",
    "Using the `jax.grad` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "from matplotlib import pyplot as plt\n",
    "# n-th order polynomial\n",
    "\n",
    "n = 6\n",
    "roots = np.random.uniform(-3, 3, n).astype(np.float64)\n",
    "def nth_order_polynomial(x, roots):\n",
    "    y = 1\n",
    "    for r in roots:\n",
    "        y = y*(x-r)\n",
    "    return y\n",
    "\n",
    "\n",
    "poly = jax.jit(functools.partial(nth_order_polynomial, roots=roots))\n",
    "\n",
    "x = jnp.linspace(-2, 2, 100)\n",
    "\n",
    "y = poly(x)\n",
    "\n",
    "\n",
    "plt.plot(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can trace the computation of the polynomial\n",
    "jax.make_jaxpr(poly)(2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is how we compute gradients / derivatives\n",
    "jax.grad(poly)(0.)\n",
    "jax.value_and_grad(poly)(0.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n-th order derivatives of this function?\n",
    "grad_func = poly\n",
    "nth_order_grads = []\n",
    "for i in range(n):\n",
    "\n",
    "    grad_func = jax.grad(grad_func)\n",
    "    y = jax.vmap(grad_func)(x)\n",
    "    plt.plot(x, y, label=f'n={i+1}')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JAX Can Differentiate with Respect to (almost) Anything\n",
    "\n",
    "As long as we register the type. But standard Python containers are supported out of the box. The type of datastructures that JAX can handle are called `Pytrees`.\n",
    "\n",
    "`Pytrees` by default include compositions of standard Python containers, `list`, `tuple`, `dict`. But we can also add our custom `Pytree` nodes.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def square(data):\n",
    "    return data['x']**2 + data['y']**2\n",
    "\n",
    "g = jax.grad(square)({\"x\": 1.0, \"y\": 2.0})\n",
    "\n",
    "print(f\"Gradient 1: {g}\")\n",
    "\n",
    "\n",
    "def square(data):\n",
    "    return data[0]**2 + data[1] ** 2\n",
    "\n",
    "g = jax.grad(square)([1.0, 2.0])\n",
    "print(f\"Gradient 2: {g}\")\n",
    "\n",
    "\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class Point:\n",
    "    x: float\n",
    "    y: float\n",
    "\n",
    "\n",
    "# now we register the Point class as a Pytree node, so that JAX knows how to deal with it\n",
    "from jax.tree_util import register_pytree_node\n",
    "register_pytree_node(\n",
    "    Point, \n",
    "    lambda x: ([x.x, x.y], None), #unpacking\n",
    "    lambda d, x: Point(*x) # packing\n",
    ")\n",
    "\n",
    "def square(data):\n",
    "    return data.x**2 + data.y ** 2\n",
    "\n",
    "g = jax.grad(square)(Point(1.0, 2.0))\n",
    "print(f\"Gradient 3: {g}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Useful Pytree Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# further useful operations on Pytrees\n",
    "\n",
    "def mul(x, y):\n",
    "    return x*y\n",
    "\n",
    "\n",
    "tree_a = {\"x\": 1.0, \"y\": 2.0}\n",
    "tree_b = {\"x\": 2.0, \"y\": 4.0}\n",
    "\n",
    "from jax.tree_util import tree_multimap, tree_map\n",
    "\n",
    "\n",
    "# equivalent of python `map` but applied to pytrees\n",
    "\n",
    "res = tree_map(lambda x: x**2, tree_a)\n",
    "print(\"Result of map:\", res)\n",
    "\n",
    "# constructing operations between multiple trees\n",
    "res = tree_multimap(lambda x,y,z: x+y+z, tree_a, tree_b, tree_b)\n",
    "print(\"Result of multimap:\", res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression Using SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset for linear regression\n",
    "def generate_data(d, N):\n",
    "    x = np.random.uniform(-1,1, (N, d))\n",
    "    theta = np.random.uniform(-1,1, (d,1))\n",
    "    y = x @ theta\n",
    "    theta_ = np.random.uniform(-1,1, (d,1))\n",
    "    return x, y, theta, theta_\n",
    "\n",
    "x, y, theta, theta_ = generate_data(20, 500)\n",
    "\n",
    "def vis_lines(theta, theta_):\n",
    "    x = np.linspace(-2, 2, 100)\n",
    "    y = theta[0, 0]*x + theta[1,0]\n",
    "    y_ = theta_[0,0]*x+ theta_[1,0]\n",
    "    plt.plot(x, y, label='gt')\n",
    "    plt.plot(x, y_, label='pred')\n",
    "    plt.legend()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def predict(x, theta):\n",
    "    return x@theta\n",
    "\n",
    "def mse(x, theta, y):\n",
    "    y_ = predict(x, theta)\n",
    "    return ((y-y_)**2).mean()\n",
    "\n",
    "grad_func = jax.grad(mse, argnums=[1]) # returns a function which takes the same arguments as the wrapped one\n",
    "\n",
    "\n",
    "# returns tuple of gradients with respect to arguments of function\n",
    "grad, = grad_func(x, theta, y)\n",
    "\n",
    "# for practicality, this is also available\n",
    "loss, grad = jax.value_and_grad(mse, argnums=1)(x,theta_, y)\n",
    "print(f\"MSE loss: {loss}\")\n",
    "\n",
    "\n",
    "\n",
    "vis_lines(theta, theta_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stupid loop to optimize our model\n",
    "for _ in range(50):\n",
    "    loss, grad = jax.value_and_grad(mse, argnums=1)(x,theta_, y)\n",
    "    theta_ -= 0.01*grad \n",
    "\n",
    "print(f\"MSE: {loss}\")\n",
    "vis_lines(theta, theta_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jacobians, Hessians"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what if we want to get a Jacobian? x is d-dimensional\n",
    "J = jax.jacobian(predict, argnums=0)(x[0], theta)\n",
    "J.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what about the Hessian?\n",
    "H, = jax.jacfwd(jax.jacrev(predict), argnums=0)(x[0], theta)\n",
    "print(\"Hessian shape:\", H.shape)\n",
    "H, = jax.hessian(predict, argnums=0)(x[0], theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2nd Order Optimization!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "d = 10\n",
    "\n",
    "@jax.jit\n",
    "def f(x):\n",
    "    return (x**4).sum(-1)\n",
    "\n",
    "\n",
    "x = np.linspace(-10, 10, 100)[:, None]\n",
    "plt.plot(x, f(x))\n",
    "\n",
    "# what can we say about the hessian of this function?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = jnp.array([10.0]*d)\n",
    "# how does the hessian look like?\n",
    "jax.hessian(f)(x0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets try to minimize it via gradient descent\n",
    "x_ = x0\n",
    "trajx = [x_[0]]\n",
    "trajy = [f(x_)]\n",
    "\n",
    "err = 1e-5\n",
    "for i in range(300):\n",
    "    g = jax.grad(f)(x_)\n",
    "    x_ -= 1/2* g\n",
    "    trajx.append(x_[0])\n",
    "    trajy.append(f(x_))\n",
    "    if trajy[-1] < err:\n",
    "        break\n",
    "print(f\"Converged in {i} steps, {err}, {x_}\")\n",
    "plt.plot(x, f(x))\n",
    "plt.plot(trajx, trajy)\n",
    "plt.scatter(trajx, trajy, color='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets try to minimize it via gradient descent\n",
    "x_ = x0\n",
    "trajx = [x0[0]]\n",
    "trajy = [f(x0)]\n",
    "\n",
    "err = 1e-5\n",
    "for i in range(100):\n",
    "    g = jax.grad(f)(x_)\n",
    "    H = jax.hessian(f)(x_)\n",
    "    x_ -= (jnp.linalg.inv(H) @ g).flatten()\n",
    "    trajx.append(x_[0])\n",
    "    trajy.append(f(x_))\n",
    "    if trajy[-1] < err:\n",
    "        break\n",
    "print(f\"Converged in {i} steps, {err}, {x_}\")\n",
    "plt.plot(x, f(x))\n",
    "plt.plot(trajx, trajy)\n",
    "plt.scatter(trajx, trajy, color='red')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note on Gradient Calculation\n",
    "\n",
    "<p style=\"font-size:20px\">\n",
    "\n",
    "Some options:\n",
    "* finite differences $\\frac{df}{dx} = \\lim_{h \\mapsto 0} \\frac{f(x+h)-f(x)}{h}$\n",
    "* symbolic\n",
    "* automatic differentiation - most of deep learning\n",
    "<p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def square(x):\n",
    "    return x**2\n",
    "\n",
    "def finite_differences(f, h):\n",
    "    def func(x):\n",
    "        return  (f(x+h)-f(x))/h\n",
    "    return func\n",
    "\n",
    "\n",
    "finite_differences(square, 1e-5)(2.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autodiff Forward vs. Reverse Mode\n",
    "\n",
    "<p style=\"font-size:16px\">\n",
    "\n",
    "**Forward Mode**: augments the outputs of the forward pass with their derivatives in a (primal, tangent) tuple $(x, \\dot x)$. This is preferred in the case where the number of inputs is much smaller than the number of outputs, in practice we compute a Jacobian-vector product.\n",
    "\n",
    "\n",
    "**Reverse Mode**: comes in two stages. First we make a forward pass through our computation graph which is followed by computation of partial derivatives with respect to intermediate variables (adjoints). Backpropagation is a special case of reverse mode autodiff. Vector-Jacobian product.\n",
    "\n",
    "**Reverse on Forward**: hybrid, for example computing Hessian.\n",
    "\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# back to our Hessian example, let's time it with different ways of computing the gradient\n",
    "\n",
    "def func(x):\n",
    "    return jnp.sum(jnp.sin(jnp.log(jnp.exp(jnp.sin(x) +jnp.cos(x)) )+ jnp.cos(jnp.exp(jnp.sin(x)))))\n",
    "\n",
    "print(\"Only reverse mode autodiff:\")\n",
    "%timeit jax.jacrev(jax.jacrev(func), argnums=0)(x)\n",
    "print(\"Hybrid autodiff forward then reverse:\")\n",
    "%timeit jax.jacrev(jax.jacfwd(func), argnums=0)(x)\n",
    "print(\"Hybrid autodiff reverse mode then forward mode:\")\n",
    "%timeit jax.jacfwd(jax.jacrev(func), argnums=0)(x)\n",
    "print(\"JAX hessian func\")\n",
    "%timeit jax.hessian(func, argnums=0)(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JAX Random Numbers\n",
    "\n",
    "Random numbers in JAX are annoying. There is no stateful random number generator such as we have in `numpy`, but we need to pass around a `key` that we split with `jax.random.split`. This is also where JAX syntax for distributions and numpy syntax differs considerably."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 123\n",
    "key = jax.random.PRNGKey(random_seed) \n",
    "\n",
    "rngs = jax.random.split(key, 10)\n",
    "\n",
    "\n",
    "print(\"These are 10 random numbers\")\n",
    "print(jnp.array([jax.random.normal(k) for k in rngs]))\n",
    "print(\"These are the same numbers\")\n",
    "print(jnp.array([jax.random.normal(k) for k in rngs]))\n",
    "\n",
    "\n",
    "key, _ = jax.random.split(rngs[-1])\n",
    "\n",
    "normal_sample = jax.random.normal(key)\n",
    "\n",
    "keys = jax.random.split(key, 10)\n",
    "\n",
    "print(\"These are samples from normal distribution\")\n",
    "print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Here come the neural networks..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# say we use an ensemble neural network (this cause a bit of pain for me and Sebastian to implement in PyTorch)\n",
    "from jax.tree_util import tree_flatten, tree_unflatten\n",
    "\n",
    "def tree_stack(trees):\n",
    "    \"\"\"Takes a list of trees and stacks every corresponding leaf.\n",
    "    For example, given two trees ((a, b), c) and ((a', b'), c'), returns\n",
    "    ((stack(a, a'), stack(b, b')), stack(c, c')).\n",
    "    Useful for turning a list of objects into something you can feed to a\n",
    "    vmapped function.\n",
    "    \"\"\"\n",
    "    leaves_list = []\n",
    "    treedef_list = []\n",
    "    for tree in trees:\n",
    "        leaves, treedef = tree_flatten(tree)\n",
    "        leaves_list.append(leaves)\n",
    "        treedef_list.append(treedef)\n",
    "\n",
    "    grouped_leaves = zip(*leaves_list)\n",
    "    result_leaves = [jnp.stack(l) for l in grouped_leaves]\n",
    "    return treedef_list[0].unflatten(result_leaves)\n",
    "\n",
    "\n",
    "def tree_unstack(tree):\n",
    "    \"\"\"Takes a tree and turns it into a list of trees. Inverse of tree_stack.\n",
    "    For example, given a tree ((a, b), c), where a, b, and c all have first\n",
    "    dimension k, will make k trees\n",
    "    [((a[0], b[0]), c[0]), ..., ((a[k], b[k]), c[k])]\n",
    "    Useful for turning the output of a vmapped function into normal objects.\n",
    "    \"\"\"\n",
    "    leaves, treedef = tree_flatten(tree)\n",
    "    n_trees = leaves[0].shape[0]\n",
    "    new_leaves = [[] for _ in range(n_trees)]\n",
    "    for leaf in leaves:\n",
    "        for i in range(n_trees):\n",
    "            new_leaves[i].append(leaf[i])\n",
    "    new_trees = [treedef.unflatten(l) for l in new_leaves]\n",
    "    return new_trees\n",
    "\n",
    "\n",
    "\n",
    "def get_nn_params():\n",
    "        return [\n",
    "            (np.random.uniform(-1,1, (10, 512)), np.random.uniform(-1,1, (512, 1))),\n",
    "            (np.random.uniform(-1,1, (512, 256)), np.random.uniform(-1,1, (256, 1))),\n",
    "            (np.random.uniform(-1,1, (256, 2)),  np.random.uniform(-1,1, (2,1)))\n",
    "        ]\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def forward(x, theta):\n",
    "    for w, b in theta:\n",
    "        x = jax.nn.relu(x@w) + b.T\n",
    "    return x\n",
    "\n",
    "\n",
    "\n",
    "params = get_nn_params()\n",
    "\n",
    "\n",
    "out = forward(x, params)\n",
    "out.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lots_of_params = [get_nn_params() for _ in range(20)]\n",
    "# how do we parallelize this?\n",
    "\n",
    "stacked_tree = tree_stack(lots_of_params)\n",
    "\n",
    "def seq_ensemble_forward(x, trees):\n",
    "    return jnp.stack([forward(x,tree) for tree in trees])\n",
    "\n",
    "\n",
    "seq_ensemble_forward = jax.jit(seq_ensemble_forward)\n",
    "\n",
    "vmap_ensemble_forward = jax.vmap(forward, in_axes=[None,[(0, 0)]*len(lots_of_params[0])])\n",
    "vmap_ensemble_forward = jax.jit(vmap_ensemble_forward)\n",
    "# seems to still not be better than linear speedup when stacking the matrices into tensors, but there is some improvement (possibly limited by hardware)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what about simple stacking?\n",
    "\n",
    "@jax.jit\n",
    "def stacked_forward(x, theta):\n",
    "    for w, b in theta:\n",
    "        x = jax.nn.relu(x@w) + b.transpose(0,2,1)\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x = np.random.uniform(-1,1, (1024,10))\n",
    "\n",
    "vmap_ensemble_forward(x, stacked_tree)\n",
    "stacked_forward(x, stacked_tree)\n",
    "seq_ensemble_forward(x, lots_of_params)\n",
    "\n",
    "%timeit stacked_forward(x, stacked_tree).block_until_ready() \n",
    "%timeit vmap_ensemble_forward(x, stacked_tree).block_until_ready() \n",
    "%timeit seq_ensemble_forward(x, lots_of_params).block_until_ready() \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flax: Making Things More Simple\n",
    "\n",
    "Alternatives: `Haiku`, `Stax`, `Objax`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import flax\n",
    "import jax\n",
    "from jax import numpy as jnp\n",
    "import numpy as np\n",
    "from typing import Optional\n",
    "# for details about this, read about python >=3.7 dataclasses\n",
    "\n",
    "class MLP(flax.linen.Module):\n",
    "    # here, ordering is preserved\n",
    "    num_hidden: int\n",
    "    hidden_size: int\n",
    "    outputs: int\n",
    "    act_function: Optional[str] = 'relu'\n",
    "\n",
    "    def setup(self):\n",
    "        self.layers = [flax.linen.Dense(features=64) for _ in range(self.num_hidden)]\n",
    "        self.last_layer = flax.linen.Dense(self.outputs)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        act_function = getattr(flax.linen, self.act_function)\n",
    "        for layer in self.layers[:-1]:\n",
    "            x = act_function(layer(x))\n",
    "        # don't apply act in last layer\n",
    "        x = self.layers[-1](x)\n",
    "        return x\n",
    "\n",
    "\n",
    "key = jax.random.PRNGKey(0)\n",
    "\n",
    "X = np.random.randn(128, 10)\n",
    "\n",
    "model = MLP(2, 64, 2)\n",
    "\n",
    "# we need to call the init function, takes a batch and key\n",
    "key, _ = jax.random.split(key)\n",
    "params = model.init(key, X)\n",
    "\n",
    "\n",
    "# we need to call the apply function for the forward pass, which also takes the model parameters\n",
    "y_ = model.apply(params, X)\n",
    "y_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import flax\n",
    "from typing import Optional\n",
    "# for details about this, read about python 3.7 datamodule class\n",
    "\n",
    "class MLPCompact(flax.linen.Module):\n",
    "    # here, ordering is preserved\n",
    "    num_hidden: int\n",
    "    hidden_size: int\n",
    "    outputs: int\n",
    "    act_function: Optional[str] = 'relu'\n",
    "    \n",
    "    @flax.linen.compact\n",
    "    def __call__(self, x):\n",
    "        act_function = getattr(flax.linen, self.act_function)\n",
    "        for _ in range(self.num_hidden):\n",
    "            x = flax.linen.Dense(self.hidden_size)(x)\n",
    "            x = act_function(x)\n",
    "        # don't apply act in last layer\n",
    "        x = flax.linen.Dense(self.outputs)(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "key = jax.random.PRNGKey(0)\n",
    "X = np.random.randn(128, 10)\n",
    "\n",
    "model = MLPCompact(2, 64, 2)\n",
    "\n",
    "# we need to call the init function, takes a batch and key to obtain sampled initial parmeters\n",
    "params = model.init(key, X)\n",
    "\n",
    "\n",
    "# we need to call the apply function, which also takes the model parameters\n",
    "y_ = model.apply(params, X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VAE Example\n",
    "\n",
    "This is our loss functions (maximizing ELBO)\n",
    "$$\n",
    "\\mathcal{L}(\\theta, \\phi) = -\\mathbb{E}[p_\\phi(x | z)] +  \\mathbb{KL}[q_\\theta(z | x) || p(z)]  $$\n",
    "\n",
    "\n",
    "First term of the loss we call reconstruction loss, second term you can see as some kind of complexity/regularization term.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optax\n",
    "from numpyro.distributions import Normal\n",
    "\n",
    "# loss functions\n",
    "\n",
    "@jax.vmap\n",
    "def kl_divergence(mean, logvar):\n",
    "  return -0.5 * jnp.sum(1 + logvar - jnp.square(mean) - jnp.exp(logvar))\n",
    "\n",
    "\n",
    "@jax.vmap\n",
    "def binary_cross_entropy_with_logits(logits, labels):\n",
    "  logits = flax.linen.log_sigmoid(logits)\n",
    "  return -jnp.sum(labels * logits + (1. - labels) * jnp.log(-jnp.expm1(logits)))\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def loss(logits, mean, logvar):\n",
    "  reconstruction_loss = binary_cross_entropy_with_logits(logits, image)\n",
    "  kl_div = kl_divergence(mean, logvar)\n",
    "  return jnp.mean(reconstruction_loss + kl_div)\n",
    "\n",
    "from typing import Sequence\n",
    "class Sequential(flax.linen.Module):\n",
    "  layers: Sequence[flax.linen.Module]\n",
    "\n",
    "  def __call__(self, x):\n",
    "    for layer in self.layers:\n",
    "      x = layer(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "class MLPCompact(flax.linen.Module):\n",
    "    # here, ordering is preserved\n",
    "    num_hidden: int\n",
    "    hidden_size: int\n",
    "    outputs: int\n",
    "    act_function: Optional[str] = 'relu'\n",
    "    \n",
    "\n",
    "    def forward(self, x):\n",
    "      act_function = getattr(flax.linen, self.act_function)\n",
    "      for _ in range(self.num_hidden):\n",
    "          x = flax.linen.Dense(self.hidden_size)(x)\n",
    "          x = act_function(x)\n",
    "      # don't apply act in last layer\n",
    "      x = flax.linen.Dense(self.outputs)(x)\n",
    "      return x\n",
    "\n",
    "    @flax.linen.compact\n",
    "    def __call__(self, x):\n",
    "      return self.forward(x)\n",
    "\n",
    "class Decoder(MLPCompact):\n",
    "\n",
    "    def setup(self):\n",
    "        return\n",
    "\n",
    "    @flax.linen.compact\n",
    "    def __call__(self, x):\n",
    "      x = super().forward(x)\n",
    "      return x\n",
    "\n",
    "\n",
    "class VAE(flax.linen.Module):\n",
    "  latents: int\n",
    "  outputs: int\n",
    "  \n",
    "  def setup(self):\n",
    "    self.encoder = MLPCompact(3, 128, self.latents*2)\n",
    "    self.decoder = Decoder(3, 256, outputs=self.outputs)\n",
    "\n",
    "  def __call__(self, key, x, deterministic=False):\n",
    "\n",
    "    gauss_params = self.encoder(x)\n",
    "    mu, logvar = jnp.split(gauss_params, 2, -1)\n",
    "    sigma = jnp.sqrt(jnp.exp(logvar))\n",
    "\n",
    "    if not deterministic:\n",
    "      gauss_dist = Normal(mu, sigma)\n",
    "      z = gauss_dist.rsample(key)\n",
    "    else:\n",
    "      z = mu      \n",
    "    return self.decoder(z), z, mu, logvar\n",
    "\n",
    "  def generate(self, key, samples):\n",
    "    # sample from prior distribution \n",
    "    mu = jnp.zeros((samples, self.latents))\n",
    "    sigma = jnp.ones((samples, self.latents))\n",
    "    gauss_dist = Normal(mu, sigma)\n",
    "    z = gauss_dist.sample(key)\n",
    "    logits =  self.decoder(z)\n",
    "    return jnp.round(flax.linen.log_sigmoid(logits)).reshape(-1, 28, 28)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the model and optimizer\n",
    "\n",
    "# vae template\n",
    "vae = VAE(20, 28*28)\n",
    "# create optimizer\n",
    "optimizer = optax.adam(1e-3)\n",
    "rk1, rk2 = jax.random.split(key)\n",
    "params = vae.init(rk1, rk2, jnp.ones((64, 28*28)))\n",
    "opt_state = optimizer.init(params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Construct a tf.data.Dataset\n",
    "ds = tfds.as_numpy(tfds.load('binarized_mnist', split='train', batch_size=64, shuffle_files=True))\n",
    "\n",
    "# this returns a function!\n",
    "def construct_training_step(model, optimizer):\n",
    "  # model forward\n",
    "  def model_loss(params, key, image):\n",
    "    logits, z, mean, logvar = model.apply(params, key, image)\n",
    "    loss = jnp.mean(binary_cross_entropy_with_logits(logits, image) + kl_divergence(mean, logvar))\n",
    "    return loss\n",
    "  \n",
    "  grad_func = jax.value_and_grad(model_loss, argnums=0)\n",
    "  # this is the function that we call in the end\n",
    "  def update_func(params, opt_state, key,  image):\n",
    "    loss, grads = grad_func(params, key, image)\n",
    "    updates, opt_state = optimizer.update(grads, opt_state)\n",
    "    return loss, updates, opt_state\n",
    "\n",
    "  return jax.jit(update_func)\n",
    "\n",
    "\n",
    "training_step = construct_training_step(vae, optimizer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import optax\n",
    "\n",
    "\n",
    "for e in range(20):\n",
    "  tqdm_iter = tqdm(enumerate(ds))\n",
    "  for i, batch in tqdm_iter:\n",
    "    image  = batch[\"image\"]\n",
    "    \n",
    "    image = image.reshape(-1, 28*28)\n",
    "\n",
    "    key, _ = jax.random.split(key)\n",
    "    logits, z, mean, logvar = vae.apply(params, key, image)\n",
    "\n",
    "    # this is equivalent to  .backward and optimizer.step in PyTorch\n",
    "    loss, updates, opt_state = training_step(params, opt_state, key, image)\n",
    "    params = optax.apply_updates(params, updates)\n",
    "    \n",
    "    tqdm_iter.set_description(f\"Epoch {e:5d}, batch {i:5d}, loss={loss:10.5f}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we sample from p(z)\n",
    "key, _ = jax.random.split(key)\n",
    "imgs = vae.apply(params, key, 2, method=vae.generate)\n",
    "plt.imshow(imgs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Meta Learning: MAML\n",
    "\n",
    "In MAML, we concern ourselves with the multi-task setting. So the following objective has two parts to it. The inner loss is the loss for instances from task 1 and the outer loss is calculated on the shifter paramters in task 2.\n",
    "\n",
    "$$\n",
    "    \\mathcal{L}(\\theta - \\nabla \\mathcal{L}(\\theta, x_1, y_1), x_2, y_2)\n",
    "$$\n",
    "\n",
    "It is clear that here we have an optimization step within the loss calculation. Lucky for us, JAX can help us out here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "from jax.tree_util import tree_multimap\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "def mse(params, inputs, targets):\n",
    "    # Computes average loss for the batch\n",
    "    predictions = mlp.apply(params, inputs)\n",
    "    return jnp.mean((targets - predictions)**2)\n",
    "\n",
    "def inner_update(p, x1, y1, alpha=.1):\n",
    "    \"\"\"\n",
    "        This is the expression with which we obtain \\theta - grad(inner_loss)\n",
    "    \"\"\"\n",
    "    grads = jax.grad(mse)(p, x1, y1)\n",
    "    inner_sgd_fn = lambda g, state: (state - alpha*g)\n",
    "    return tree_multimap(inner_sgd_fn, grads, p)\n",
    "\n",
    "def maml_loss(p, x1, y1, x2, y2):\n",
    "    \"\"\"\n",
    "        This is the outer loss\n",
    "    \"\"\"\n",
    "    p2 = inner_update(p, x1, y1)\n",
    "    return mse(p2, x2, y2)\n",
    "\n",
    "# returns scalar for all tasks.\n",
    "def batch_maml_loss(p, x1_b, y1_b, x2_b, y2_b):\n",
    "    print(x1_b.shape, y1_b.shape, x2_b.shape, y2_b.shape)\n",
    "    task_losses = jax.vmap(functools.partial(maml_loss, p))(x1_b, y1_b, x2_b, y2_b)\n",
    "    return jnp.mean(task_losses)\n",
    "\n",
    "\n",
    "# this returns a function!\n",
    "def construct_training_step(model, optimizer):\n",
    "  # model forward\n",
    "  grad_func = jax.value_and_grad(batch_maml_loss)\n",
    "  # this is the function that we call in the end\n",
    "  def update_func(params, opt_state, x1_b, y1_b, x2_b, y2_b):\n",
    "    l, grads = grad_func(params, x1_b, y1_b, x2_b, y2_b)\n",
    "    updates, opt_state = optimizer.update(grads, opt_state)\n",
    "    return l, updates, opt_state\n",
    "  return jax.jit(update_func)\n",
    "\n",
    "def sample_tasks(outer_batch_size, inner_batch_size):\n",
    "    # Sample random sinusoid functions\n",
    "    As = []\n",
    "    phases = []\n",
    "    for _ in range(outer_batch_size):        \n",
    "        As.append(np.random.uniform(low=0.1, high=.5))\n",
    "        phases.append(np.random.uniform(low=0., high=np.pi))\n",
    "    def get_batch():\n",
    "        xs, ys = [], []\n",
    "        for A, phase in zip(As, phases):\n",
    "            x = np.random.uniform(low=-5., high=5., size=(inner_batch_size, 1))\n",
    "            y = A * np.sin(x/phase)\n",
    "            xs.append(x)\n",
    "            ys.append(y)\n",
    "        return np.stack(xs), np.stack(ys)\n",
    "    x1, y1 = get_batch()\n",
    "    x2, y2 = get_batch()\n",
    "    return x1, y1, x2, y2\n",
    "\n",
    "\n",
    "\n",
    "for a,phase in zip([1.0, 0.2, 0.5], [0.1, np.pi/2, np.pi]):\n",
    "    x = np.linspace(0, 10, 100)\n",
    "    y = a * np.sin(x+phase)\n",
    "    plt.plot(x, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optax\n",
    "mlp = MLPCompact(2, 64, 1)\n",
    "params  = mlp.init(key, jnp.ones((64,1)))\n",
    "optimizer = optax.adam(1e-2)\n",
    "opt_state = optimizer.init(params)\n",
    "training_step = construct_training_step(mlp, optimizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "np_batched_maml_loss = []\n",
    "K=20\n",
    "tqdm_iter = tqdm(range(10000))\n",
    "for i in tqdm_iter:\n",
    "    x1_b, y1_b, x2_b, y2_b = sample_tasks(4, K)\n",
    "    l, updates, opt_state = training_step(params, opt_state, x1_b, y1_b, x2_b, y2_b)\n",
    "    np_batched_maml_loss.append(l)\n",
    "    params = optax.apply_updates(params, updates)\n",
    "    tqdm_iter.set_postfix_str(f\"loss:{l:10.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch the inference across K=100\n",
    "x = np.linspace(-5,5,100).reshape((100, 1)) # (k, 1)\n",
    "targets = np.sin(x)\n",
    "predictions = jax.vmap(functools.partial(mlp.apply, params))(x)\n",
    "plt.plot(x, predictions, label='pre-update predictions')\n",
    "plt.plot(x, targets, label='target')\n",
    "\n",
    "x1 = np.random.uniform(low=-5., high=5., size=(K,1))\n",
    "y1 = 1. * np.sin(x1 + 0.)\n",
    "\n",
    "net_params = params\n",
    "for i in range(1,5):\n",
    "    net_params = inner_update(net_params, x1, y1)\n",
    "    predictions = jax.vmap(functools.partial(mlp.apply, net_params))(x)\n",
    "    plt.plot(x, predictions, label='{}-shot predictions'.format(i))\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flows\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import math\n",
    "from sklearn.datasets import make_blobs, make_circles, make_swiss_roll, make_moons, make_spd_matrix\n",
    "def dataset_transformed_gaussian(size, seed=345):\n",
    "    rng = np.random.RandomState(seed)\n",
    "    rot = make_spd_matrix(2, random_state=seed)\n",
    "    data = rng.randn(size, 2) @ rot\n",
    "    return data\n",
    "\n",
    "def dataset_half_moon(size, seed=345):\n",
    "    \"\"\"Half moon distribution\n",
    "    \n",
    "    from https://blog.evjang.com/2018/01/nf1.html\n",
    "    \"\"\"\n",
    "    rng = np.random.RandomState(seed)\n",
    "    x2 = rng.normal(size=size) * 4\n",
    "    x1 = rng.normal(size=size) + 0.25 * x2**2\n",
    "    data = np.stack((x1, x2), axis=1)\n",
    "    return data\n",
    "\n",
    "def dataset_two_moons(size, seed=345):\n",
    "    data = make_moons(size, noise=0.1, random_state=seed)[0]\n",
    "    return  data\n",
    "\n",
    "def dataset_blobs(size, loc=3, seed=345):\n",
    "    data = make_blobs(size, 2, \n",
    "                                       centers=[[loc, loc], [-loc, -loc], \n",
    "                                                [loc, -loc], [-loc, loc]],\n",
    "                                       random_state=seed)[0]\n",
    "    return data\n",
    "\n",
    "def dataset_circles(size, seed=345):\n",
    "    data = make_circles(size, noise=0.1, factor=0.3, random_state=seed)[0]\n",
    "    return data\n",
    "\n",
    "def dataset_swiss_roll(size, seed=345):\n",
    "    data = make_swiss_roll(size, noise=0.6, random_state=seed)[0][:, [0, 2]]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [dataset_transformed_gaussian, dataset_blobs, dataset_half_moon, \n",
    "            dataset_two_moons, dataset_circles, dataset_swiss_roll]\n",
    "\n",
    "cols = 3\n",
    "rows = int(math.ceil(len(datasets) / cols))\n",
    "fig, axs = plt.subplots(rows, cols, figsize=(4.8 * cols, 3.2 * rows))\n",
    "\n",
    "n_samples = 1000\n",
    "for idx, (ax, dataset) in enumerate(zip(axs.flat, datasets)):\n",
    "    X = dataset(n_samples)\n",
    "    _ = ax.hist2d(X[:, 0], X[:, 1], bins=100);\n",
    "    ax.set_title(dataset.__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpyro.distributions import Normal\n",
    "\n",
    "\n",
    "\n",
    "base_dist = Normal(jnp.array([0,0]), jnp.array([1,1]) )\n",
    "mlp = MLPCompact(2, 2, 2)\n",
    "params = mlp.init(key, np.ones((2,2)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class AffineCoupling(flax.linen.Module):\n",
    "    input_dims: list\n",
    "    output_dims: list\n",
    "    net: flax.linen.Module\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_inp = x[:, self.input_dims]\n",
    "        params = self.net(x_inp)\n",
    "        logscale, shift = jnp.split(params, 2, -1)\n",
    "        x_out = x[:, self.output_dims]*jnp.exp(logscale) + shift\n",
    "        x = x.at[:,self.output_dims].set(x_out)\n",
    "        ldj = jnp.exp(logscale.sum(-1))\n",
    "        return x, ldj\n",
    "\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return self.forward(x)\n",
    "\n",
    "    def inverse(self, x):\n",
    "        x_inp = x[:, self.input_dims]\n",
    "        params = self.net(x_inp)\n",
    "        logscale, shift = jnp.split(params, 2, -1)\n",
    "        \n",
    "        x_out = (x[:, self.output_dims] - shift)/jnp.exp(logscale)\n",
    "        x = x.at[:,self.output_dims].set(x_out)\n",
    "        ldj = -jnp.sum(logscale.sum(-1))\n",
    "        return x, ldj\n",
    "\n",
    "\n",
    "class Flow(flax.linen.Module):\n",
    "    transforms: flax.linen.Module\n",
    "    base_dist: flax.linen.Module\n",
    "    dim: int\n",
    "\n",
    "    def forward(self, key, n):\n",
    "        x = self.base_dist.expand((n, self.dim)).sample(key)\n",
    "        for t in self.transforms:\n",
    "            x, _ =  t.forward(x)\n",
    "        return x\n",
    "    \n",
    "    def __call__(self, key, n):\n",
    "        return self.forward(key, n)\n",
    "\n",
    "    def inverse(self, x):\n",
    "        logprob = 0\n",
    "        for t in reversed(self.transforms):\n",
    "            x, ldj =  t.inverse(x)\n",
    "            logprob+=ldj\n",
    "        logprob += self.base_dist.expand(x.shape).log_prob(x).sum(-1)\n",
    "        return x, logprob\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = jnp.array(np.ones((10, 2)))\n",
    "\n",
    "coupling = AffineCoupling([0], [1], mlp)\n",
    "\n",
    "\n",
    "base_dist = Normal(jnp.array([0,0]), jnp.array([1,1]))\n",
    "flow = Flow([\n",
    "    AffineCoupling([0], [1], MLPCompact(2, 32, 2)),\n",
    "    AffineCoupling([1], [0], MLPCompact(2, 32, 2)),\n",
    "    AffineCoupling([0], [1], MLPCompact(2, 32, 2)),\n",
    "    AffineCoupling([1], [0], MLPCompact(2, 32, 2))],\n",
    "    base_dist=base_dist, dim=2)\n",
    "\n",
    "params =  flow.init(key, key, 2)\n",
    "optimizer = optax.chain(optax.adam(1e-4), optax.clip_by_global_norm(5.0))\n",
    "opt_state = optimizer.init(params)\n",
    "\n",
    "\n",
    "samples = flow.apply(params, key, 500, method=flow.forward)\n",
    "_ = plt.hist2d(samples[:, 0], samples[:, 1], bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z, logprob = flow.apply(params, jnp.array(x), method=flow.inverse)\n",
    "\n",
    "z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this returns a function!\n",
    "def construct_training_step(model, optimizer):\n",
    "  # model forward\n",
    "  def model_loss(params, x):\n",
    "    z, logprob = model.apply(params, x, method=model.inverse)  \n",
    "    return -jnp.mean(logprob)\n",
    "  grad_func = jax.value_and_grad(model_loss)\n",
    "  # this is the function that we call in the end\n",
    "  def update_func(params, opt_state, x):\n",
    "    l, grads = grad_func(params, x)\n",
    "    updates, opt_state = optimizer.update(grads, opt_state)\n",
    "    return l, updates, opt_state\n",
    "  return jax.jit(update_func)\n",
    "\n",
    "\n",
    "training_step = construct_training_step(flow,  optimizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset_transformed_gaussian(5000)\n",
    "def iterate_over_array(X, bs):\n",
    "    for i in range(0, X.shape[0], bs):\n",
    "        yield jnp.array(X[i:i+bs])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(20):\n",
    "    tqdm_iter = tqdm(iterate_over_array(X, 64))\n",
    "    for x in tqdm_iter:\n",
    "        loss, updates, opt_state = training_step(params,opt_state, jnp.array(x))\n",
    "        params = optax.apply_updates(params, updates)\n",
    "        tqdm_iter.set_postfix_str(f\"loss={loss:10.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = flow.apply(params, key, 5000, method=flow.forward)\n",
    "_ = plt.hist2d(samples[:, 0], samples[:, 1], bins=100)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "be97daffee0cb79bf284fb5c73b937ebf75cd7f53ef12b5a2937ebadc10a9be5"
  },
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('.venv': poetry)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
